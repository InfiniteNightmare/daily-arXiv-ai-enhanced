<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.AI](#cs.AI) [Total: 55]
- [cs.LG](#cs.LG) [Total: 58]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [CSQL: Mapping Documents into Causal Databases](https://arxiv.org/abs/2601.08109)
*Sridhar Mahadevan*

Main category: cs.DB

TL;DR: CSQL 是一個能將非結構化文檔轉換為可通過 SQL 查詢的因果數據庫（CDB）的自動化系統，支持進行因果干預和結構化因果查詢，超越傳統的關聯檢索方法。


<details>
  <summary>Details</summary>
Motivation: 當前基於 RAG 或知識圖譜的系統主要支持關聯檢索，缺乏對文檔集合進行因果分析的能力。CSQL 旨在填補這一空白，使系統能夠直接回答「為什麼」問題，並在更大規模上支持因果查詢和縱向分析。

Method: CSQL 基於早前的 DEMOCRITUS 系統，將文檔轉換為來自因果話語的局部因果模型，然後編譯成因果數據庫。系統還支持攝入 RAG/IE 編譯的因果語料庫，例如 TCC 數據集，實現了文檔級至語料庫級的擴展。

Result: CSQL 實現了將文檔轉換為因果數據庫，並在 TCC 數據集上構建了包含 265,656 個聲明實例的因果庫，覆蓋 45,319 篇論文、44 年和 1,575 個報告方法字符串，從而支持了語料庫級別的因果查詢和時間序列分析。

Conclusion: CSQL 作為從非結構化文檔到因果數據庫的編譯器，配備了嚴謹的查詢代數，可廣泛應用於商業、人文和科學等多個領域，實現了對文本集合的因果分析能力。

Abstract: We describe a novel system, CSQL, which automatically converts a collection of unstructured text documents into an SQL-queryable causal database (CDB). A CDB differs from a traditional DB: it is designed to answer "why'' questions via causal interventions and structured causal queries. CSQL builds on our earlier system, DEMOCRITUS, which converts documents into thousands of local causal models derived from causal discourse. Unlike RAG-based systems or knowledge-graph based approaches, CSQL supports causal analysis over document collections rather than purely associative retrieval. For example, given an article on the origins of human bipedal walking, CSQL enables queries such as: "What are the strongest causal influences on bipedalism?'' or "Which variables act as causal hubs with the largest downstream influence?'' Beyond single-document case studies, we show that CSQL can also ingest RAG/IE-compiled causal corpora at scale by compiling the Testing Causal Claims (TCC) dataset of economics papers into a causal database containing 265,656 claim instances spanning 45,319 papers, 44 years, and 1,575 reported method strings, thereby enabling corpus-level causal queries and longitudinal analyses in CSQL. Viewed abstractly, CSQL functions as a compiler from unstructured documents into a causal database equipped with a principled algebra of queries, and can be applied broadly across many domains ranging from business, humanities, and science.

</details>


### [2] [SVFusion: A CPU-GPU Co-Processing Architecture for Large-Scale Real-Time Vector Search](https://arxiv.org/abs/2601.08528)
*Yuchen Peng,Dingyu Yang,Zhongle Xie,Ji Sun,Lidan Shou,Ke Chen,Gang Chen*

Main category: cs.DB

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Approximate Nearest Neighbor Search (ANNS) underpins modern applications such as information retrieval and recommendation. With the rapid growth of vector data, efficient indexing for real-time vector search has become rudimentary. Existing CPU-based solutions support updates but suffer from low throughput, while GPU-accelerated systems deliver high performance but face challenges with dynamic updates and limited GPU memory, resulting in a critical performance gap for continuous, large-scale vector search requiring both accuracy and speed. In this paper, we present SVFusion, a GPU-CPU-disk collaborative framework for real-time vector search that bridges sophisticated GPU computation with online updates. SVFusion leverages a hierarchical vector index architecture that employs CPU-GPU co-processing, along with a workload-aware vector caching mechanism to maximize the efficiency of limited GPU memory. It further enhances performance through real-time coordination with CUDA multi-stream optimization and adaptive resource management, along with concurrency control that ensures data consistency under interleaved queries and updates. Empirical results demonstrate that SVFusion achieves significant improvements in query latency and throughput, exhibiting a 20.9x higher throughput on average and 1.3x to 50.7x lower latency compared to baseline methods, while maintaining high recall for large-scale datasets under various streaming workloads.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Matrix-PIC: Harnessing Matrix Outer-product for High-Performance Particle-in-Cell Simulations](https://arxiv.org/abs/2601.08277)
*Yizhuo Rao,Xingjian Cui,Jiabin Xie,Shangzhi Pang,Guangnan Feng,Jinhui Wei,Zhiguang Chen,Yutong Lu*

Main category: cs.DC

TL;DR: 利用CPU上新型矩阵处理单元(MPU)的矩阵外积原语，提出MatrixPIC对粒子网格模拟中的电流沉积步骤进行软硬件协同设计。


<details>
  <summary>Details</summary>
Motivation: 传统多核CPU上粒子网格相互作用因细粒度原子更新成为瓶颈，而近年CPU集成了高效支持矩阵外积操作的专用矩阵处理单元(MPU)，为突破此瓶颈提供了新机会。

Method: 提出MatrixPIC——首次针对现代CPU混合MPU-VPU SIMD模型的电流沉积内核、数据布局和增量粒子排序的协同设计：1. 将电流沉积算法重构成块矩阵形式以适配MPU外积原语，2. 结合基于MPU的高密度累积与基于VPU的数据准备及控制流的混合执行流水线，3. 基于间隙打包内存数组的O(1)摊销增量排序器以维持MPU高效执行所需的数据局部性。

Result: 在下一代HPC平台上评估：在激光尾场加速模拟中，实现总运行时2.63倍加速；对于三阶沉积，核心内核比基线加速8.7倍，比手工优化的最佳VPU实现快2.0倍；达到CPU理论峰值性能的83.08%，比数据中心GPU上高度优化的CUDA内核高近2.8倍。

Conclusion: 矩阵导向的协同设计能有效加速新兴CPU架构上的粒子网格模拟，证明利用CPU MPU进行矩阵外积操作的潜力。

Abstract: Particle-in-Cell (PIC) simulations spend most of their execution time on particle--grid interactions, where fine-grained atomic updates become a major bottleneck on traditional many-core CPUs. Recent CPU architectures integrate specialized Matrix Processing Units (MPUs) that efficiently support matrix outer-product operations, offering new opportunities to overcome this limitation. Leveraging this architectural shift, this work focuses on redesigning the current deposition step of PIC simulations under a matrix-centric execution model.
  We present MatrixPIC, the first holistic co-design of the deposition kernel, data layout, and incremental particle sorting tailored to the hybrid MPU--VPU SIMD model on modern CPUs. MatrixPIC introduces: (i)~a block-matrix formulation of the current deposition algorithm that maps naturally to MPU outer-product primitives; (ii)~a hybrid execution pipeline that combines MPU-based high-density accumulation with VPU-based data preparation and control flow; and (iii)~an $O(1)$-amortized incremental sorter based on a gapped packed-memory array to preserve data locality for efficient MPU execution.
  Evaluated on a next-generation HPC platform, MatrixPIC achieves significant performance gains. In Laser-Wakefield Acceleration (LWFA) simulations, it delivers up to $2.63\times$ speedup in total runtime. For third-order deposition, the core kernel is accelerated by $8.7\times$ over the baseline and $2.0\times$ over the best hand-optimized VPU implementation. Moreover, MatrixPIC reaches $83.08\%$ of theoretical CPU peak performance, nearly $2.8\times$ higher than a highly optimized CUDA kernel on a data center GPU. These results demonstrate the effectiveness of matrix-oriented co-design for accelerating PIC simulations on emerging CPU architectures.

</details>


### [4] [Shifting the Sweet Spot: High-Performance Matrix-Free Method for High-Order Elasticity](https://arxiv.org/abs/2601.08374)
*Dali Chang,Chong Zhang,Kaiqi Zhang,Mingguan Yang,Huiyuan Li,Weiqiang Kong*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In high-order finite element analysis for elasticity, matrix-free (PA) methods are a key technology for overcoming the memory bottleneck of traditional Full Assembly (FA). However, existing implementations fail to fully exploit the special structure of modern CPU architectures and tensor-product elements, causing their performance "sweet spot" to anomalously remain at the low order of $p \approx 2$, which severely limits the potential of high-order methods. To address this challenge, we design and implement a highly optimized PA operator within the MFEM framework, deeply integrated with a Geometric Multigrid (GMG) preconditioner. Our multi-level optimization strategy includes replacing the original $O(p^6)$ generic algorithm with an efficient $O(p^4)$ one based on tensor factorization, exploiting Voigt symmetry to reduce redundant computations for the elasticity problem, and employing macro-kernel fusion to enhance data locality and break the memory bandwidth bottleneck. Extensive experiments on mainstream x86 and ARM architectures demonstrate that our method successfully shifts the performance "sweet spot" to the higher-order region of $p \ge 6$. Compared to the MFEM baseline, the optimized core operator (kernel) achieves speedups of 7x to 83x, which translates to a 3.6x to 16.8x end-to-end performance improvement in the complete solution process. This paper provides a validated and efficient practical path for conducting large-scale, high-order elasticity simulations on mainstream CPU hardware.

</details>


### [5] [MixServe: An Automatic Distributed Serving System for MoE Models with Hybrid Parallelism Based on Fused Communication Algorithm](https://arxiv.org/abs/2601.08800)
*Bowen Zhou,Jinrui Jia,Wenhao He,Yong Zhang,Fang Dong*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Mixture of Experts (MoE) models are emerging as the latest paradigm for Large Language Models (LLMs). However, due to memory constraints, MoE models with billions or even trillions of parameters can only be deployed in multi-GPU or even multi-node & multi-GPU based serving systems. Thus, communication has became a major bottleneck in distributed serving systems, especially inter-node communication. Contemporary distributed MoE models are primarily implemented using all-reduce (AR) based tensor parallelism (TP) and all-to-all (A2A) based expert parallelism (EP). However, TP generally exhibits low inter-node efficiency and is thus confined to high-speed intra-node bandwidth. In contrast, EP tends to suffer from load imbalance, especially when the parallel degree is high.
  In this work, we introduce MixServe, a novel automatic distributed serving system for efficient deployment of MoE models by a novel TP-EP hybrid parallelism based on fused AR-A2A communication algorithm. MixServe begins by evaluating the communication overhead associated with various parallel strategies, taking into account the model hyperparameters and the configurations of network and hardware resources, and then automatically selects the most efficient parallel strategy. Then, we propose the TP-EP hybrid parallelism based on fused AR-A2A communication algorithm that overlaps intra-node AR communication and inter-node A2A communication. Extensive experiments on DeepSeek-R1 and Qwen3 models demonstrate that MixServe achieves superior inference performance, with 1.08~3.80x acceleration in time to first token (TTFT), 1.03~1.66x acceleration in inter-token latency (ITL), and 5.2%~50.3% throughput improvement compared to existing approaches.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [SECite: Analyzing and Summarizing Citations in Software Engineering Literature](https://arxiv.org/abs/2601.07939)
*Shireesh Reddy Pyreddy,Khaja Valli Pathan,Hasan Masum,Tarannum Shaila Zaman*

Main category: cs.SE

TL;DR: SECite：利用引文情感分析与LLM摘要评估科研影响力的创新框架


<details>
  <summary>Details</summary>
Motivation: 传统文献综述仅反映作者的自述视角，缺乏第三方评价；引文背景中的情感分析能揭示学界对论文的真实评价，提供更深入、实用的贡献与不足分析。

Method: 提出SECite方法：1）半自动管道从九篇研究论文的引文中提取引用语句；2）使用高级NLP技术和无监督机器学习对引文进行正向/负向情感分类；3）基于生成式AI，从聚类引文组和全文生成情感特异性摘要，捕获目标论文的优势与局限。

Result: 研究揭示了学界对这些论文的认知存在有意义模式，突出了外部引文反馈与作者自述之间的对齐与分歧区域。

Conclusion: 通过整合引文情感分析与基于LLM的摘要生成，本研究为评估学术贡献提供了全面框架，实现了更客观的文献影响力评估。

Abstract: Identifying the strengths and limitations of a research paper is a core component of any literature review. However, traditional summaries reflect only the authors' self-presented perspective. Analyzing how other researchers discuss and cite the paper can offer a deeper, more practical understanding of its contributions and shortcomings. In this research, we introduce SECite, a novel approach for evaluating scholarly impact through sentiment analysis of citation contexts. We develop a semi-automated pipeline to extract citations referencing nine research papers and apply advanced natural language processing (NLP) techniques with unsupervised machine learning to classify these citation statements as positive or negative. Beyond sentiment classification, we use generative AI to produce sentiment-specific summaries that capture the strengths and limitations of each target paper, derived both from clustered citation groups and from the full text. Our findings reveal meaningful patterns in how the academic community perceives these works, highlighting areas of alignment and divergence between external citation feedback and the authors' own presentation. By integrating citation sentiment analysis with LLM-based summarization, this study provides a comprehensive framework for assessing scholarly contributions.

</details>


### [7] [Towards Verifiably Safe Tool Use for LLM Agents](https://arxiv.org/abs/2601.08012)
*Aarya Doshi,Yining Hong,Congying Xu,Eunsuk Kang,Alexandros Kapravelos,Christian Kästner*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents. While this empowers agents to perform complex tasks, LLMs may invoke unintended tool interactions and introduce risks, such as leaking sensitive data or overwriting critical records, which are unacceptable in enterprise contexts. Current approaches to mitigate these risks, such as model-based safeguards, enhance agents' reliability but cannot guarantee system safety. Methods like information flow control (IFC) and temporal constraints aim to provide guarantees but often require extensive human annotation. We propose a process that starts with applying System-Theoretic Process Analysis (STPA) to identify hazards in agent workflows, derive safety requirements, and formalize them as enforceable specifications on data flows and tool sequences. To enable this, we introduce a capability-enhanced Model Context Protocol (MCP) framework that requires structured labels on capabilities, confidentiality, and trust level. Together, these contributions aim to shift LLM-based agent safety from ad hoc reliability fixes to proactive guardrails with formal guarantees, while reducing dependence on user confirmation and making autonomy a deliberate design choice.

</details>


### [8] [Automating API Documentation from Crowdsourced Knowledge](https://arxiv.org/abs/2601.08036)
*Bonan Kou,Zijie Zhou,Muhao Chen,Tianyi Zhang*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: API documentation is crucial for developers to learn and use APIs. However, it is known that many official API documents are obsolete and incomplete. To address this challenge, we propose a new approach called AutoDoc that generates API documents with API knowledge extracted from online discussions on Stack Overflow (SO). AutoDoc leverages a fine-tuned dense retrieval model to identify seven types of API knowledge from SO posts. Then, it uses GPT-4o to summarize the API knowledge in these posts into concise text. Meanwhile, we designed two specific components to handle LLM hallucination and redundancy in generated content. We evaluated AutoDoc against five comparison baselines on 48 APIs of different popularity levels. Our results indicate that the API documents generated by AutoDoc are up to 77.7% more accurate, 9.5% less duplicated, and contain 34.4% knowledge uncovered by the official documents. We also measured the sensitivity of AutoDoc to the choice of different LLMs. We found that while larger LLMs produce higher-quality API documents, AutoDoc enables smaller open-source models (e.g., Mistral-7B-v0.3) to achieve comparable results. Finally, we conducted a user study to evaluate the usefulness of the API documents generated by AutoDoc. All participants found API documents generated by AutoDoc to be more comprehensive, concise, and helpful than the comparison baselines. This highlights the feasibility of utilizing LLMs for API documentation with careful design to counter LLM hallucination and information redundancy.

</details>


### [9] [Cognitive Biases in LLM-Assisted Software Development](https://arxiv.org/abs/2601.08045)
*Xinyi Zhou,Zeinadsadat Saghi,Sadra Sabouri,Rahul Pandita,Mollie McGuire,Souti Chattopadhyay*

Main category: cs.SE

TL;DR: 该研究首次系统探讨了LLM协助开发中的认知偏见，发现在编程活动中约48.8%的行为存在偏见，其中56.4%涉及开发者-LLM交互，识别出15类新型偏见并提出缓解建议。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件开发中的广泛应用，编程活动正从解决方案生成转向解决方案评估，这一转变可能放大或引入新的决策认知偏见，但学界尚缺乏对AI协作开发中偏见影响的系统性研究。

Method: 采用混合研究方法：先对14名学生和专业开发者进行观察研究，再对22名开发者进行问卷调查；定性比较传统非LLM工作流与LLM协助工作流中的偏见类别；通过系统分析90个特定于开发者-LLM交互的认知偏见，构建了经认知心理学家验证的15类偏见分类体系。

Result: 研究发现：1）48.8%的程序员行为存在认知偏见；2）其中56.4%的偏见行为发生在开发者-LLM交互过程中；3）LLM相关行为更易引发新型偏见；4）建立了包含15个偏见类别的分类法，揭示了偏见在人类-AI编程中的具体表现模式。

Conclusion: LLM协助开发显著改变了编程中的认知模式，引入了新型偏见风险。研究为开发者和LLM工具构建者提供了识别偏见的具体分类、工具实践建议及缓解策略，对提升人类-AI协作编程质量具有重要指导意义。

Abstract: The widespread adoption of Large Language Models (LLMs) in software development is transforming programming from a solution-generative to a solution-evaluative activity. This shift opens a pathway for new cognitive challenges that amplify existing decision-making biases or create entirely novel ones. One such type of challenge stems from cognitive biases, which are thinking patterns that lead people away from logical reasoning and result in sub-optimal decisions. How do cognitive biases manifest and impact decision-making in emerging AI-collaborative development? This paper presents the first comprehensive study of cognitive biases in LLM-assisted development. We employ a mixed-methods approach, combining observational studies with 14 student and professional developers, followed by surveys with 22 additional developers. We qualitatively compare categories of biases affecting developers against the traditional non-LLM workflows. Our findings suggest that LLM-related actions are more likely to be associated with novel biases. Through a systematic analysis of 90 cognitive biases specific to developer-LLM interactions, we develop a taxonomy of 15 bias categories validated by cognitive psychologists. We found that 48.8% of total programmer actions are biased, and developer-LLM interactions account for 56.4% of these biased actions. We discuss how these bias categories manifest, present tools and practices for developers, and recommendations for LLM tool builders to help mitigate cognitive biases in human-AI programming.

</details>


### [10] [Coverage-Guided Road Selection and Prioritization for Efficient Testing in Autonomous Driving Systems](https://arxiv.org/abs/2601.08609)
*Qurban Ali,Andrea Stocco,Leonardo Mariani,Oliviero Riganelli*

Main category: cs.SE

TL;DR: 提出了一种ADAS测试优先级框架，通过聚类减少冗余场景，基于几何复杂性、驾驶难度和历史故障对道路进行优先级排序，在缩减测试套件89%的情况下保留79%的失败场景，早期故障检测提升95倍。


<details>
  <summary>Details</summary>
Motivation: ADAS测试依赖于大量道路场景数据集，但现有数据集包含许多冗余案例，这些冗余不会改善故障检测却降低了测试效率。需要一种方法在保留几何和行为多样性的同时减少测试冗余。

Method: 1) 基于ADAS驾驶行为的几何和动态特征对道路场景进行聚类；2) 从每个聚类中选择代表性案例以保证覆盖率；3) 基于几何复杂性、驾驶难度和历史故障对道路进行优先级排序，确保关键且具有挑战性的测试优先执行。

Result: 在OPENCAT数据集和Udacity自动驾驶模拟器上评估，使用两个ADAS模型：平均减少89%的测试套件规模，同时保留平均79%的失败道路场景；优先级策略相比随机基线将早期故障检测提升高达95倍。

Conclusion: 该框架在显著减少ADAS测试工作量的同时，保持了较高的故障检测能力，有效提升了测试效率，为ADAS的安全验证提供了实用解决方案。

Abstract: Autonomous Driving Assistance Systems (ADAS) rely on extensive testing to ensure safety and reliability, yet road scenario datasets often contain redundant cases that slow down the testing process without improving fault detection. To address this issue, we present a novel test prioritization framework that reduces redundancy while preserving geometric and behavioral diversity. Road scenarios are clustered based on geometric and dynamic features of the ADAS driving behavior, from which representative cases are selected to guarantee coverage. Roads are finally prioritized based on geometric complexity, driving difficulty, and historical failures, ensuring that the most critical and challenging tests are executed first. We evaluate our framework on the OPENCAT dataset and the Udacity self-driving car simulator using two ADAS models. On average, our approach achieves an 89% reduction in test suite size while retaining an average of 79% of failed road scenarios. The prioritization strategy improves early failure detection by up to 95x compared to random baselines.

</details>


### [11] [LLMs in Code Vulnerability Analysis: A Proof of Concept](https://arxiv.org/abs/2601.08691)
*Shaznin Sultana,Sadia Afreen,Nasir U. Eisty*

Main category: cs.SE

TL;DR: LLMs能够自动识别漏洞、评估严重性和修复代码，但微调模型表现优于零/少样本，且当前评估指标存在不足。


<details>
  <summary>Details</summary>
Motivation: 传统软件安全分析方法难以应对现代代码库的规模和复杂性，需要智能自动化来更高效、准确地检测、评估和修复漏洞；研究探索利用代码专用和通用LLMs自动化关键安全任务。

Method: 使用Big-Vul和Vul-Repair两个C/C++漏洞数据集评估五对近期LLMs（包括代码专用和通用开源模型），比较微调与基于提示的方法（零/少样本）。

Result: 微调方法在所有任务和模型中均优于零/少样本方法；代码专用模型在复杂任务的零/少样本中表现突出，通用模型效果接近；CodeBLEU、CodeBERTScore、BLEU和ChrF等指标间存在差异，表明当前评估修复质量的指标不充分。

Conclusion: 研究表明LLMs在改进漏洞分析和修复方面具有潜力，但需解决指标不连贯的问题。

Abstract: Context: Traditional software security analysis methods struggle to keep pace with the scale and complexity of modern codebases, requiring intelligent automation to detect, assess, and remediate vulnerabilities more efficiently and accurately. Objective: This paper explores the incorporation of code-specific and general-purpose Large Language Models (LLMs) to automate critical software security tasks, such as identifying vulnerabilities, predicting severity and access complexity, and generating fixes as a proof of concept. Method: We evaluate five pairs of recent LLMs, including both code-based and general-purpose open-source models, on two recognized C/C++ vulnerability datasets, namely Big-Vul and Vul-Repair. Additionally, we compare fine-tuning and prompt-based approaches. Results: The results show that fine-tuning uniformly outperforms both zero-shot and few-shot approaches across all tasks and models. Notably, code-specialized models excel in zero-shot and few-shot settings on complex tasks, while general-purpose models remain nearly as effective. Discrepancies among CodeBLEU, CodeBERTScore, BLEU, and ChrF highlight the inadequacy of current metrics for measuring repair quality. Conclusions: This study contributes to the software security community by investigating the potential of advanced LLMs to improve vulnerability analysis and remediation.

</details>


### [12] [Revisiting "Revisiting Neuron Coverage for DNN Testing: A Layer-Wise and Distribution-Aware Criterion": A Critical Review and Implications on DNN Coverage Testing](https://arxiv.org/abs/2601.08729)
*Jinhan Kim,Nargiz Humbatova,Gunel Jahangirova,Shin Yoo,Paolo Tonella*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present a critical review of Neural Coverage (NLC), a state-of-the-art DNN coverage criterion by Yuan et al. at ICSE 2023. While NLC proposes to satisfy eight design requirements and demonstrates strong empirical performance, we question some of their theoretical and empirical assumptions. We observe that NLC deviates from core principles of coverage criteria, such as monotonicity and test suite order independence, and could more fully account for key properties of the covariance matrix. Additionally, we note threats to the validity of the empirical study, related to the ground truth ordering of test suites. Through our empirical validation, we substantiate our claims and propose improvements for future DNN coverage metrics. Finally, we conclude by discussing the implications of these insights.

</details>


### [13] [TerraFormer: Automated Infrastructure-as-Code with LLMs Fine-Tuned via Policy-Guided Verifier Feedback](https://arxiv.org/abs/2601.08734)
*Prithwish Jana,Sam Davidson,Bhavana Bhasker,Andrey Kan,Anoop Deoras,Laurent Callot*

Main category: cs.SE

TL;DR: TerraFormer是一个结合监督微调与验证器引导强化学习的神经符号框架，通过形式验证工具提供语法、可部署性和策略合规性反馈，显著提升IaC生成和变异的正确性。


<details>
  <summary>Details</summary>
Motivation: 自动化基础设施即代码（IaC）具有挑战性，大型语言模型（LLM）通常从自然语言生成错误的配置。为解决此问题，需要开发能够确保语法正确、可部署且符合策略的IaC生成方法。

Method: 1. 提出神经符号框架TerraFormer，结合监督微调（SFT）与验证器引导的强化学习（RL）。2. 使用形式验证工具提供语法、可部署性和策略合规性反馈。3. 通过多阶段验证和迭代式LLM自我校正，构建了两个高质量NL-to-IaC数据集：TF-Gen（152k实例）和TF-Mutn（52k实例）。

Result: 1. 在IaC-Eval上比基础LLM提升了15.94%的正确性，在TF-Gen（测试集）提升11.65%，在TF-Mutn（测试集）提升19.60%。2. 在TF-Gen和TF-Mutn测试集上均优于规模约50倍的大型模型（如Sonnet 3.7、DeepSeek-R1、GPT-4.1）。3. 在IaC-Eval排名第三，并在最佳实践和安全性合规方面达到顶级水平。

Conclusion: TerraFormer通过神经符号方法有效解决了IaC生成中的正确性问题，不仅在性能上超越了更大的LLM模型，而且确保了配置的语法正确性、可部署性和策略合规性，为自动化基础设施配置提供了可靠解决方案。

Abstract: Automating Infrastructure-as-Code (IaC) is challenging, and large language models (LLMs) often produce incorrect configurations from natural language (NL). We present TerraFormer, a neuro-symbolic framework for IaC generation and mutation that combines supervised fine-tuning with verifier-guided reinforcement learning, using formal verification tools to provide feedback on syntax, deployability, and policy compliance. We curate two large, high-quality NL-to-IaC datasets, TF-Gen (152k instances) and TF-Mutn (52k instances), via multi-stage verification and iterative LLM self-correction. Evaluations against 17 state-of-the-art LLMs, including ~50x larger models like Sonnet 3.7, DeepSeek-R1, and GPT-4.1, show that TerraFormer improves correctness over its base LLM by 15.94% on IaC-Eval, 11.65% on TF-Gen (Test), and 19.60% on TF-Mutn (Test). It outperforms larger models on both TF-Gen (Test) and TF-Mutn (Test), ranks third on IaC-Eval, and achieves top best-practices and security compliance.

</details>


### [14] [Reliable Graph-RAG for Codebases: AST-Derived Graphs vs LLM-Extracted Knowledge Graphs](https://arxiv.org/abs/2601.08773)
*Manideep Reddy Chinthareddy*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Retrieval-Augmented Generation for software engineering often relies on vector similarity search, which captures topical similarity but can fail on multi-hop architectural reasoning such as controller to service to repository chains, interface-driven wiring, and inheritance. This paper benchmarks three retrieval pipelines on Java codebases (Shopizer, with additional runs on ThingsBoard and OpenMRS Core): (A) vector-only No-Graph RAG, (B) an LLM-generated knowledge graph RAG (LLM-KB), and (C) a deterministic AST-derived knowledge graph RAG (DKB) built with Tree-sitter and bidirectional traversal.
  Using 15 architecture and code-tracing queries per repository, we measure indexing time, query latency, corpus coverage, cost, and answer correctness. DKB builds its graph in seconds, while LLM-KB requires much longer graph generation. LLM-KB also shows indexing incompleteness: on Shopizer, 377 files are skipped or missed, reducing embedded chunk coverage and graph size compared to DKB. End-to-end cost is modest for DKB relative to the vector-only baseline but much higher for LLM-KB, especially as repository scale increases. Query latency is similar for No-Graph and DKB, while LLM-KB is slower and more variable. On the Shopizer question suite, DKB achieves the highest correctness, LLM-KB is close behind, and the vector-only baseline performs worst on upstream architectural queries and has the highest hallucination risk. Overall, deterministic AST-derived graphs provide more reliable coverage and multi-hop grounding than LLM-extracted graphs at substantially lower indexing cost.

</details>


### [15] [APEX-SWE](https://arxiv.org/abs/2601.08806)
*Abhi Kottamasu,Akul Datta,Aakash Barthwal,Chirag Mahapatra,Ajay Arun,Adarsh Hiremath,Brendan Foody,Bertie Vidgen*

Main category: cs.SE

TL;DR: 提出了评估前沿AI模型执行经济价值软件工程工作能力的基准APEX-SWE，通过集成和可观测性两类新任务类型模拟真实场景，发现Gemini 3 Pro表现最佳（25% Pass@1），强性能主要源自认知推理能力与解决不确定性的主动性组合。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注狭窄、明确定义的任务，无法反映真实软件工程的经济价值工作。需要建立评估AI模型执行实际软件工程任务能力的基准。

Method: 设计了APEX-SWE基准，包含两种新颖任务类型：1）集成任务（100个）：构建跨云原生服务、商业应用和基础设施即代码的端到端系统；2）可观测性任务（100个）：使用日志、仪表板等遥测信号和非结构化上下文调试生产故障。评估了八个前沿模型，开源性评估架构和50个开发集。

Result: Gemini 3 Pro在思考模式高设置下表现最佳，Pass@1分数为25%。分析表明，强性能主要由认知识别能力驱动——即区分假设与已验证事实，结合在行动前解决不确定性的主动性。

Conclusion: APEX-SWE基准成功评估了AI模型在真实软件工程场景中的表现，揭示了认知识别和自主解决不确定性对处理复杂实际任务的关键作用。开源评估工具为后续研究提供了基础。

Abstract: We introduce the AI Productivity Index for Software Engineering (APEX-SWE), a benchmark for assessing whether frontier AI models can execute economically valuable software engineering work. Unlike existing evaluations that focus on narrow, well-defined tasks, APEX-SWE assesses two novel task types that reflect real-world software engineering work: (1) Integration tasks (n=100), which require constructing end-to-end systems across heterogeneous cloud primitives, business applications, and infrastructure-as-code services, and (2) Observability tasks (n=100), which require debugging production failures using telemetry signals such as logs and dashboards, as well as unstructured context. We evaluated eight frontier models on APEX-SWE. Gemini 3 Pro (Thinking = High) performs best, with a Pass@1 score of 25\%. Our analysis shows that strong performance is primarily driven by epistemic reasoning, defined as the ability to distinguish between assumptions and verified facts, combined with agency to resolve uncertainty prior to acting. We open-source the APEX-SWE evaluation harness and a dev set (n=50).

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [Bridging the Trust Gap: Clinician-Validated Hybrid Explainable AI for Maternal Health Risk Assessment in Bangladesh](https://arxiv.org/abs/2601.07866)
*Farjana Yesmin,Nusrat Shirmin,Suraiya Shabnam Bristy*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While machine learning shows promise for maternal health risk prediction, clinical adoption in resource-constrained settings faces a critical barrier: lack of explainability and trust. This study presents a hybrid explainable AI (XAI) framework combining ante-hoc fuzzy logic with post-hoc SHAP explanations, validated through systematic clinician feedback. We developed a fuzzy-XGBoost model on 1,014 maternal health records, achieving 88.67% accuracy (ROC-AUC: 0.9703). A validation study with 14 healthcare professionals in Bangladesh revealed strong preference for hybrid explanations (71.4% across three clinical cases) with 54.8% expressing trust for clinical use. SHAP analysis identified healthcare access as the primary predictor, with the engineered fuzzy risk score ranking third, validating clinical knowledge integration (r=0.298). Clinicians valued integrated clinical parameters but identified critical gaps: obstetric history, gestational age, and connectivity barriers. This work demonstrates that combining interpretable fuzzy rules with feature importance explanations enhances both utility and trust, providing practical insights for XAI deployment in maternal healthcare.

</details>


### [17] [When Models Know When They Do Not Know: Calibration, Cascading, and Cleaning](https://arxiv.org/abs/2601.07965)
*Chenjie Hao,Weyl Lu,Yuko Ishiwaka,Zengyi Li,Weier Wan,Yubei Chen*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: When a model knows when it does not know, many possibilities emerge. The first question is how to enable a model to recognize that it does not know. A promising approach is to use confidence, computed from the model's internal signals, to reflect its ignorance. Prior work in specific domains has shown that calibration can provide reliable confidence estimates. In this work, we propose a simple, effective, and universal training-free method that applies to both vision and language models, performing model calibration, cascading, and data cleaning to better exploit a model's ability to recognize when it does not know. We first highlight two key empirical observations: higher confidence corresponds to higher accuracy within a single model, and models calibrated on the validation set remain calibrated on a held-out test set. These findings empirically establish the reliability and comparability of calibrated confidence. Building on this, we introduce two applications: (1) model cascading with calibrated advantage routing and (2) data cleaning based on model ensemble. Using the routing signal derived from the comparability of calibrated confidences, we cascade large and small models to improve efficiency with almost no compromise in accuracy, and we further cascade two models of comparable scale to achieve performance beyond either model alone. Leveraging multiple experts and their calibrated confidences, we design a simple yet effective data-cleaning method that balances precision and detection rate to identify mislabeled samples in ImageNet and Massive Multitask Language Understanding (MMLU) datasets. Our results demonstrate that enabling models to recognize when they do not know is a practical step toward more efficient, reliable, and trustworthy AI.

</details>


### [18] [Reasoning over Precedents Alongside Statutes: Case-Augmented Deliberative Alignment for LLM Safety](https://arxiv.org/abs/2601.08000)
*Can Jin,Rui Wu,Tong Che,Qixin Zhang,Hongwu Peng,Jiahui Zhao,Zhenting Wang,Wenqi Wei,Ligong Han,Zhao Zhang,Yuan Cao,Ruixiang Tang,Dimitris N. Metaxas*

Main category: cs.AI

TL;DR: 本文系统评估在开源大语言模型中使用明确详尽安全规则与案例增强简单规则的效果，发现后者能更好平衡安全性与有用性，并提出了案例增强审慎对齐方法CADA。


<details>
  <summary>Details</summary>
Motivation: 确保大语言模型遵守安全原则同时不对良性请求过度拒绝是一大挑战。现有审慎对齐方法依赖详细安全规则，但在推理能力有限的开源模型中效果未充分研究。需要探索更有效的安全对齐方法以平衡无害性与有用性。

Method: 1. 系统评估明确详尽安全代码与案例增强简单规则的差异化效果；2. 提出CADA（案例增强审慎对齐）方法：利用强化学习在自生成安全推理链上训练，通过案例增强推理替代纯规则对齐；3. 采用自生成安全推理链，增强模型对安全边界的泛化理解。

Result: 1. 明确详尽安全规则仅不一致地提升无害性，且系统性地降低有用性；2. 案例增强简单规则训练产生更鲁棒和泛化的安全行为；3. CADA有效提升无害性，增强对攻击的鲁棒性，减少过度拒绝，并在多样化基准测试中保持实用性。

Conclusion: 案例增强推理相比纯规则对齐能更好平衡安全性与有用性，避免对狭隘枚举规则的僵化遵守，实现更广泛的适应性。CADA为提升模型安全性同时保持有用性提供了实用替代方案。

Abstract: Ensuring that Large Language Models (LLMs) adhere to safety principles without refusing benign requests remains a significant challenge. While OpenAI introduces deliberative alignment (DA) to enhance the safety of its o-series models through reasoning over detailed ``code-like'' safety rules, the effectiveness of this approach in open-source LLMs, which typically lack advanced reasoning capabilities, is understudied. In this work, we systematically evaluate the impact of explicitly specifying extensive safety codes versus demonstrating them through illustrative cases. We find that referencing explicit codes inconsistently improves harmlessness and systematically degrades helpfulness, whereas training on case-augmented simple codes yields more robust and generalized safety behaviors. By guiding LLMs with case-augmented reasoning instead of extensive code-like safety rules, we avoid rigid adherence to narrowly enumerated rules and enable broader adaptability. Building on these insights, we propose CADA, a case-augmented deliberative alignment method for LLMs utilizing reinforcement learning on self-generated safety reasoning chains. CADA effectively enhances harmlessness, improves robustness against attacks, and reduces over-refusal while preserving utility across diverse benchmarks, offering a practical alternative to rule-only DA for improving safety while maintaining helpfulness.

</details>


### [19] [Internal Deployment Gaps in AI Regulation](https://arxiv.org/abs/2601.08005)
*Joe Kwon,Stephen Casper*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Frontier AI regulations primarily focus on systems deployed to external users, where deployment is more visible and subject to outside scrutiny. However, high-stakes applications can occur internally when companies deploy highly capable systems within their own organizations, such as for automating R\&D, accelerating critical business processes, and handling sensitive proprietary data. This paper examines how frontier AI regulations in the United States and European Union in 2025 handle internal deployment. We identify three gaps that could cause internally-deployed systems to evade intended oversight: (1) scope ambiguity that allows internal systems to evade regulatory obligations, (2) point-in-time compliance assessments that fail to capture the continuous evolution of internal systems, and (3) information asymmetries that subvert regulatory awareness and oversight. We then analyze why these gaps persist, examining tensions around measurability, incentives, and information access. Finally, we map potential approaches to address them and their associated tradeoffs. By understanding these patterns, we hope that policy choices around internally deployed AI systems can be made deliberately rather than incidentally.

</details>


### [20] [Pervasive Annotation Errors Break Text-to-SQL Benchmarks and Leaderboards](https://arxiv.org/abs/2601.08778)
*Tengjun Jin,Yoojin Choi,Yuxuan Zhu,Daniel Kang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Researchers have proposed numerous text-to-SQL techniques to streamline data analytics and accelerate the development of database-driven applications. To compare these techniques and select the best one for deployment, the community depends on public benchmarks and their leaderboards. Since these benchmarks heavily rely on human annotations during question construction and answer evaluation, the validity of the annotations is crucial.
  In this paper, we conduct an empirical study that (i) benchmarks annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow, and (ii) corrects a subset of the BIRD development (Dev) set to measure the impact of annotation errors on text-to-SQL agent performance and leaderboard rankings. Through expert analysis, we show that BIRD Mini-Dev and Spider 2.0-Snow have error rates of 52.8% and 62.8%, respectively. We re-evaluate all 16 open-source agents from the BIRD leaderboard on both the original and the corrected BIRD Dev subsets. We show that performance changes range from -7% to 31% (in relative terms) and rank changes range from $-9$ to $+9$ positions. We further assess whether these impacts generalize to the full BIRD Dev set. We find that the rankings of agents on the uncorrected subset correlate strongly with those on the full Dev set (Spearman's $r_s$=0.85, $p$=3.26e-5), whereas they correlate weakly with those on the corrected subset (Spearman's $r_s$=0.32, $p$=0.23). These findings show that annotation errors can significantly distort reported performance and rankings, potentially misguiding research directions or deployment choices. Our code and data are available at https://github.com/uiuc-kang-lab/text_to_sql_benchmarks.

</details>


### [21] [Integrating Attendance Tracking and Emotion Detection for Enhanced Student Engagement in Smart Classrooms](https://arxiv.org/abs/2601.08049)
*Keith Ainebyona,Ann Move Oguti,Joseph Walusimbi,Ritah Kobusingye*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The increasing adoption of smart classroom technologies in higher education has mainly focused on automating attendance, with limited attention given to students' emotional and cognitive engagement during lectures. This limits instructors' ability to identify disengagement and adapt teaching strategies in real time. This paper presents SCASED (Smart Classroom Attendance System with Emotion Detection), an IoT-based system that integrates automated attendance tracking with facial emotion recognition to support classroom engagement monitoring. The system uses a Raspberry Pi camera and OpenCV for face detection, and a finetuned MobileNetV2 model to classify four learning-related emotional states: engagement, boredom, confusion, and frustration. A session-based mechanism is implemented to manage attendance and emotion monitoring by recording attendance once per session and performing continuous emotion analysis thereafter. Attendance and emotion data are visualized through a cloud-based dashboard to provide instructors with insights into classroom dynamics. Experimental evaluation using the DAiSEE dataset achieved an emotion classification accuracy of 89.5%. The results show that integrating attendance data with emotion analytics can provide instructors with additional insight into classroom dynamics and support more responsive teaching practices.

</details>


### [22] [Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms](https://arxiv.org/abs/2601.08052)
*Nawazish Alia,Rachael Shawb,Karl Mason*

Main category: cs.AI

TL;DR: 提出基于深度强化学习的乳业农场负荷调度框架，包含两个PPO变体：利用短期预测调整策略的Forecast Aware PPO，以及通过PID控制器自适应调节KL散度实现稳定训练的策略更新方法。


<details>
  <summary>Details</summary>
Motivation: 乳业农场能耗高且依赖电网供电，可再生能源的间歇性使实时供需平衡面临挑战。现有RL调度方法大多假设未来电价或发电量已知，这在实际动态环境中不现实；传统PPO方法固定参数在电价变化时训练不稳定。

Method: 开发了基于深度强化学习的负荷调度框架，重点优化电池储能和热水负荷。提出了两种改进PPO算法：Forecast Aware PPO利用小时和月度残差校准的短期需求与可再生能源预测；PID KL PPO采用PID控制器自适应调节KL散度阈值，实现稳定策略更新。

Result: 在真实乳业农场数据上训练后，该方法相比PPO降低1%电费，比DQN低4.8%，比SAC低1.5%。在电池调度方面，PPO减少电网输入13.1%，证明了其在现代乳业可持续能源管理中的可扩展性和有效性。

Conclusion: 所提出的强化学习框架能有效降低乳业农场运营成本并减少电网依赖，为间歇性可再生能源集成下的智能负荷调度提供了实用解决方案，支持联合国可持续发展目标7（可负担清洁能源）。

Abstract: Dairy farming is an energy intensive sector that relies heavily on grid electricity. With increasing renewable energy integration, sustainable energy management has become essential for reducing grid dependence and supporting the United Nations Sustainable Development Goal 7 on affordable and clean energy. However, the intermittent nature of renewables poses challenges in balancing supply and demand in real time. Intelligent load scheduling is therefore crucial to minimize operational costs while maintaining reliability. Reinforcement Learning has shown promise in improving energy efficiency and reducing costs. However, most RL-based scheduling methods assume complete knowledge of future prices or generation, which is unrealistic in dynamic environments. Moreover, standard PPO variants rely on fixed clipping or KL divergence thresholds, often leading to unstable training under variable tariffs. To address these challenges, this study proposes a Deep Reinforcement Learning framework for efficient load scheduling in dairy farms, focusing on battery storage and water heating under realistic operational constraints. The proposed Forecast Aware PPO incorporates short term forecasts of demand and renewable generation using hour of day and month based residual calibration, while the PID KL PPO variant employs a proportional integral derivative controller to regulate KL divergence for stable policy updates adaptively. Trained on real world dairy farm data, the method achieves up to 1% lower electricity cost than PPO, 4.8% than DQN, and 1.5% than SAC. For battery scheduling, PPO reduces grid imports by 13.1%, demonstrating scalability and effectiveness for sustainable energy management in modern dairy farming.

</details>


### [23] [A New Strategy for Verifying Reach-Avoid Specifications in Neural Feedback Systems](https://arxiv.org/abs/2601.08065)
*Samuel I. Akinwande,Sydney M. Katz,Mykel J. Kochenderfer,Clark Barrett*

Main category: cs.AI

TL;DR: 本文提出了新的后向可达集估计算法，并与现有前向分析技术结合，构建了神经反馈系统的统一验证框架。


<details>
  <summary>Details</summary>
Motivation: 当前神经反馈系统的可达性验证主要依赖前向可达性分析，因为已有的后向可达性方法可扩展性有限。这种不平衡限制了验证能力。

Method: 1. 提出了能够计算后向可达集过近似和欠近似的新算法
2. 将这些后向算法与成熟的前向分析技术集成
3. 构建了统一的神经反馈系统验证框架

Result: 所提方法实现了对神经反馈系统后向可达集的量化估计（包括过近似和欠近似），并通过与前向分析框架的集成创建了更全面的验证方案。

Conclusion: 新算法通过提升后向可达分析的可扩展性，结合前向分析形成了统一的神经反馈系统验证框架，改进和扩展了现有验证能力。

Abstract: Forward reachability analysis is the predominant approach for verifying reach-avoid properties in neural feedback systems (dynamical systems controlled by neural networks). This dominance stems from the limited scalability of existing backward reachability methods. In this work, we introduce new algorithms that compute both over- and under-approximations of backward reachable sets for such systems. We further integrate these backward algorithms with established forward analysis techniques to yield a unified verification framework for neural feedback systems.

</details>


### [24] [Semantic Gravity Wells: Why Negative Constraints Backfire](https://arxiv.org/abs/2601.08070)
*Shailesh Rana*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Negative constraints (instructions of the form "do not use word X") represent a fundamental test of instruction-following capability in large language models. Despite their apparent simplicity, these constraints fail with striking regularity, and the conditions governing failure have remained poorly understood. This paper presents the first comprehensive mechanistic investigation of negative instruction failure. We introduce semantic pressure, a quantitative measure of the model's intrinsic probability of generating the forbidden token, and demonstrate that violation probability follows a tight logistic relationship with pressure ($p=σ(-2.40+2.27\cdot P_0)$; $n=40{,}000$ samples; bootstrap $95%$ CI for slope: $[2.21,,2.33]$). Through layer-wise analysis using the logit lens technique, we establish that the suppression signal induced by negative instructions is present but systematically weaker in failures: the instruction reduces target probability by only 5.2 percentage points in failures versus 22.8 points in successes -- a $4.4\times$ asymmetry. We trace this asymmetry to two mechanistically distinct failure modes. In priming failure (87.5% of violations), the instruction's explicit mention of the forbidden word paradoxically activates rather than suppresses the target representation. In override failure (12.5%), late-layer feed-forward networks generate contributions of $+0.39$ toward the target probability -- nearly $4\times$ larger than in successes -- overwhelming earlier suppression signals. Activation patching confirms that layers 23--27 are causally responsible: replacing these layers' activations flips the sign of constraint effects. These findings reveal a fundamental tension in negative constraint design: the very act of naming a forbidden word primes the model to produce it.

</details>


### [25] [How vehicles change lanes after encountering crashes: Empirical analysis and modeling](https://arxiv.org/abs/2601.08125)
*Kequan Chen,Yuxuan Wang,Pan Liu,Victor L. Knoop,David Z. W. Wang,Yu Han*

Main category: cs.AI

TL;DR: 该研究分析了交通事故后车道变换（post-crash LCs）的行为特征，发现其持续时间更长、插入速度更低、碰撞风险更高，且79.4%涉及新跟随车辆的不让行行为。基于此，作者开发了一种基于图注意力模块的轨迹预测框架，通过显式建模让行行为辅助任务，显著提升了预测性能并改善了碰撞风险分析。


<details>
  <summary>Details</summary>
Motivation: 交通事故后，后续车辆需要进行车道变换以绕过障碍物，但目标车道车辆即使在车道变换开始后也可能拒绝让行，增加此类行为的复杂性和碰撞风险。然而，目前对post-crash LCs的行为特征和运动模式了解有限，需要填补这一研究空白。

Method: 首先从无人机录像中提取轨迹数据构建post-crash LC数据集；通过实证分析比较post-crash LCs与强制LCs（MLCs）和自由选择LCs（DLCs）的差异；随后开发基于图注意力的轨迹预测框架，核心是显式建模让行行为的交互感知模块，该模块引导条件变分自编码器和基于Transformer的解码器预测车道变换者的轨迹。

Result: 实证分析显示：post-crash LCs较MLCs和DLCs持续时间更长、插入速度更低、碰撞风险更高；79.4%的案例涉及新跟随车辆至少一次不让行行为（DLCs为21.7%，MLCs为28.6%）。模型在轨迹预测性能上比现有基线提升超过10%（平均位移误差和最终位移误差）；同时降低了虚假碰撞率，提高了冲突预测准确性；在不同地点收集的额外数据集上验证了模型的迁移性。

Conclusion: 研究首次系统揭示了post-crash LCs的独特行为特征，特别是高频率的不让行现象。提出的交互感知轨迹预测框架通过显式建模让行行为显著提升了预测精度和风险分析可靠性，为改善交通事故后交通流的安全管理提供了有效的技术工具。模型展示的良好迁移性表明其具有实际应用的潜力。

Abstract: When a traffic crash occurs, following vehicles need to change lanes to bypass the obstruction. We define these maneuvers as post crash lane changes. In such scenarios, vehicles in the target lane may refuse to yield even after the lane change has already begun, increasing the complexity and crash risk of post crash LCs. However, the behavioral characteristics and motion patterns of post crash LCs remain unknown. To address this gap, we construct a post crash LC dataset by extracting vehicle trajectories from drone videos captured after crashes. Our empirical analysis reveals that, compared to mandatory LCs (MLCs) and discretionary LCs (DLCs), post crash LCs exhibit longer durations, lower insertion speeds, and higher crash risks. Notably, 79.4% of post crash LCs involve at least one instance of non yielding behavior from the new follower, compared to 21.7% for DLCs and 28.6% for MLCs. Building on these findings, we develop a novel trajectory prediction framework for post crash LCs. At its core is a graph based attention module that explicitly models yielding behavior as an auxiliary interaction aware task. This module is designed to guide both a conditional variational autoencoder and a Transformer based decoder to predict the lane changer's trajectory. By incorporating the interaction aware module, our model outperforms existing baselines in trajectory prediction performance by more than 10% in both average displacement error and final displacement error across different prediction horizons. Moreover, our model provides more reliable crash risk analysis by reducing false crash rates and improving conflict prediction accuracy. Finally, we validate the model's transferability using additional post crash LC datasets collected from different sites.

</details>


### [26] [Embedded AI Companion System on Edge Devices](https://arxiv.org/abs/2601.08128)
*Rahul Gupta,Stephen D. H. Hsu*

Main category: cs.AI

TL;DR: 为了解决边缘设备上AI伴侣系统的计算资源限制问题，本文提出了一种在活跃与非活跃阶段交替的内存范式，通过轻量级实时对话与离线内存整理相结合的方式，在嵌入式硬件上实现了低延迟和长期个性化。


<details>
  <summary>Details</summary>
Motivation: 边缘设备计算资源有限，现有AI伴侣和内存系统无法直接应用，导致用户体验不佳。需要设计一种在资源受限环境下既能保证低延迟对话又能实现长期记忆个性化的解决方案。

Method: 提出动态内存范式：用户活跃阶段执行低延迟轻量检索；用户非活跃阶段进行计算密集的提取、整合和维护。同时建立了评估对话质量和记忆能力的AI伴侣基准。

Result: 使用弱模型(Qwen2.5-7B-Instruct量化int4)的系统在多数指标上超越无内存的原始LLM，与16k上下文的GPT-3.5表现相当，证明了在嵌入式硬件上实现有效AI伴侣的可行性。

Conclusion: 交替内存范式可在边缘设备计算约束下平衡实时性需求与长期个性化能力，为嵌入式AI伴侣系统提供了实用解决方案，通过专用基准验证了其有效性。

Abstract: Computational resource constraints on edge devices make it difficult to develop a fully embedded AI companion system with a satisfactory user experience. AI companion and memory systems detailed in existing literature cannot be directly used in such an environment due to lack of compute resources and latency concerns. In this paper, we propose a memory paradigm that alternates between active and inactive phases: during phases of user activity, the system performs low-latency, real-time dialog using lightweight retrieval over existing memories and context; whereas during phases of user inactivity, it conducts more computationally intensive extraction, consolidation, and maintenance of memories across full conversation sessions. This design minimizes latency while maintaining long-term personalization under the tight constraints of embedded hardware. We also introduce an AI Companion benchmark designed to holistically evaluate the AI Companion across both its conversational quality and memory capabilities. In our experiments, we found that our system (using a very weak model: Qwen2.5-7B-Instruct quantized int4) outperforms the equivalent raw LLM without memory across most metrics, and performs comparably to GPT-3.5 with 16k context window.

</details>


### [27] [ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms](https://arxiv.org/abs/2601.08166)
*Mohammad Pivezhandi,Mahdi Banisharif,Abusayeed Saifullah,Ali Jannesari*

Main category: cs.AI

TL;DR: 该论文提出了一种基于模型的分层多智能体强化学习框架，用于嵌入式多核平台的热能和能耗感知调度，无需离线分析即可实现零样本部署，显著提升能效和调度效率。


<details>
  <summary>Details</summary>
Motivation: 现有动态电压频率缩放和任务分配方法存在两大局限：一是基于利用率的启发式方法忽略停顿时间，二是需要大量离线分析生成调度表，无法实现运行时自适应调整。这限制了嵌入式系统在动态负载下的热管理和能效表现。

Method: 采用分层多智能体强化学习框架，通过两个协作智能体分解指数级动作空间，降低决策延迟至358ms。结合Dyna-Q方法将直接强化学习与基于模型的规划相结合，使用回归技术构建准确的环境模型预测热动力学和性能状态。引入LLM语义特征提取，从OpenMP程序中提取13个代码级特征，无需实际执行即可生成合成训练数据，实现零样本部署。

Result: 在BOTS和PolybenchC基准测试中，相比Linux ondemand调度器，能效提升7.09倍，完工时间缩短4.0倍。首决策延迟（含LLM特征提取）为3.5-8.0秒，后续决策仅需358ms，比基于表的分析方法快8300倍，收敛速度比无模型方法快20倍。

Conclusion: 该框架成功解决了现有调度方法无法适应动态工作负载的问题，通过模型驱动的方法和LLM语义特征实现了零样本迁移能力，为嵌入式系统的实时热管理和能耗优化提供了实用解决方案。

Abstract: Dynamic voltage and frequency scaling (DVFS) and task-to-core allocation are critical for thermal management and balancing energy and performance in embedded systems. Existing approaches either rely on utilization-based heuristics that overlook stall times, or require extensive offline profiling for table generation, preventing runtime adaptation. We propose a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. Two collaborative agents decompose the exponential action space, achieving 358ms latency for subsequent decisions. First decisions require 3.5 to 8.0s including one-time LLM feature extraction. An accurate environment model leverages regression techniques to predict thermal dynamics and performance states. When combined with LLM-extracted semantic features, the environment model enables zero-shot deployment for new workloads on trained platforms by generating synthetic training data without requiring workload-specific profiling samples. We introduce LLM-based semantic feature extraction that characterizes OpenMP programs through 13 code-level features without execution. The Dyna-Q-inspired framework integrates direct reinforcement learning with model-based planning, achieving 20x faster convergence than model-free methods. Experiments on BOTS and PolybenchC benchmarks across NVIDIA Jetson TX2, Jetson Orin NX, RubikPi, and Intel Core i7 demonstrate 7.09x better energy efficiency and 4.0x better makespan than Linux ondemand governor. First-decision latency is 8,300x faster than table-based profiling, enabling practical deployment in dynamic embedded systems.

</details>


### [28] [The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios](https://arxiv.org/abs/2601.08173)
*Daocheng Fu,Jianbiao Mei,Rong Wu,Xuemeng Yang,Jia Xu,Ding Wang,Pinlong Cai,Yong Liu,Licheng Wen,Botian Shi*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation; however, existing research mainly targets performance upper bounds in static environments, overlooking robustness for stochastic real-world deployment. We identify three key challenges: dynamic task scheduling, active exploration under uncertainty, and continuous learning from experience. To bridge this gap, we introduce \method{}, a dynamic evaluation environment that simulates a "trainee" agent continuously exploring a novel setting. Unlike traditional benchmarks, \method{} evaluates agents along three dimensions: (1) context-aware scheduling for streaming tasks with varying priorities; (2) prudent information acquisition to reduce hallucination via active exploration; and (3) continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning. Our work establishes a framework for assessing agent reliability, shifting evaluation from static tests to realistic, production-oriented scenarios. Our codes are available at https://github.com/KnowledgeXLab/EvoEnv

</details>


### [29] [Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement](https://arxiv.org/abs/2601.08545)
*Zhenlong Dai,Zhuoluo Zhao,Hengning Wang,Xiu Tang,Sai Wu,Chang Yao,Zhipeng Gao,Jingyuan Chen*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With the development of large language models (LLMs) in the field of programming, intelligent programming coaching systems have gained widespread attention. However, most research focuses on repairing the buggy code of programming learners without providing the underlying causes of the bugs. To address this gap, we introduce a novel task, namely \textbf{LPR} (\textbf{L}earner-Tailored \textbf{P}rogram \textbf{R}epair). We then propose a novel and effective framework, \textbf{\textsc{\MethodName{}}} (\textbf{L}earner-Tailored \textbf{S}olution \textbf{G}enerator), to enhance program repair while offering the bug descriptions for the buggy code. In the first stage, we utilize a repair solution retrieval framework to construct a solution retrieval database and then employ an edit-driven code retrieval approach to retrieve valuable solutions, guiding LLMs in identifying and fixing the bugs in buggy code. In the second stage, we propose a solution-guided program repair method, which fixes the code and provides explanations under the guidance of retrieval solutions. Moreover, we propose an Iterative Retrieval Enhancement method that utilizes evaluation results of the generated code to iteratively optimize the retrieval direction and explore more suitable repair strategies, improving performance in practical programming coaching scenarios. The experimental results show that our approach outperforms a set of baselines by a large margin, validating the effectiveness of our framework for the newly proposed LPR task.

</details>


### [30] [Improving LLM Reasoning with Homophily-aware Structural and Semantic Text-Attributed Graph Compression](https://arxiv.org/abs/2601.08187)
*Zijun Di,Bin Lu,Huquan Kang,Luoyi Fu,Jiaxin Ding,Xiaoying Gan,Lei Zhou,Xinbing Wang,Chenghu Zhou*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) have demonstrated promising capabilities in Text-Attributed Graph (TAG) understanding. Recent studies typically focus on verbalizing the graph structures via handcrafted prompts, feeding the target node and its neighborhood context into LLMs. However, constrained by the context window, existing methods mainly resort to random sampling, often implemented via dropping node/edge randomly, which inevitably introduces noise and cause reasoning instability. We argue that graphs inherently contain rich structural and semantic information, and that their effective exploitation can unlock potential gains in LLMs reasoning performance. To this end, we propose Homophily-aware Structural and Semantic Compression for LLMs (HS2C), a framework centered on exploiting graph homophily. Structurally, guided by the principle of Structural Entropy minimization, we perform a global hierarchical partition that decodes the graph's essential topology. This partition identifies naturally cohesive, homophilic communities, while discarding stochastic connectivity noise. Semantically, we deliver the detected structural homophily to the LLM, empowering it to perform differentiated semantic aggregation based on predefined community type. This process compresses redundant background contexts into concise community-level consensus, selectively preserving semantically homophilic information aligned with the target nodes. Extensive experiments on 10 node-level benchmarks across LLMs of varying sizes and families demonstrate that, by feeding LLMs with structurally and semantically compressed inputs, HS2C simultaneously enhances the compression rate and downstream inference accuracy, validating its superiority and scalability. Extensions to 7 diverse graph-level benchmarks further consolidate HS2C's task generalizability.

</details>


### [31] [Adapting Rules of Official International Mahjong for Online Players](https://arxiv.org/abs/2601.08211)
*Chucai Wang,Lingfeng Li,Yunlong Lu,Wenxin Li*

Main category: cs.AI

TL;DR: 本文利用世界冠军AI进行自博弈统计分析，发现先行优势和子目标计分问题，提出补偿点机制和子目标计分优化方案，构建了更适合在线单轮游戏平衡的国际麻将修订规则。


<details>
  <summary>Details</summary>
Motivation: 在线国际麻将玩家存在游戏时间碎片化和对手不固定的特点，传统的面对面多轮对战规则无法直接适用，需要重新设计规则来保证单轮游戏的公平性。

Method: 使用世界冠军AI进行自博弈竞赛，收集对战数据并进行统计分析，识别游戏平衡性问题，包括先行优势和子目标计分设置问题。

Result: 发现先行优势在单轮游戏中明显存在，传统多轮换位平衡机制不适用于在线环境。提出了补偿点机制平衡先行优势，并优化了不同牌型组合的子目标计分规则，且实现了在线版本供玩家使用。

Conclusion: 通过AI数据分析方法评估游戏平衡并开发修订规则，使传统国际麻将更好地适应在线游戏环境，为传统游戏在线化平衡设计提供了新的方法论。

Abstract: As one of the worldwide spread traditional game, Official International Mahjong can be played and promoted online through remote devices instead of requiring face-to-face interaction. However, online players have fragmented playtime and unfixed combination of opponents in contrary to offline players who have fixed opponents for multiple rounds of play. Therefore, the rules designed for offline players need to be modified to ensure the fairness of online single-round play. Specifically, We employ a world champion AI to engage in self-play competitions and conduct statistical data analysis. Our study reveals the first-mover advantage and issues in the subgoal scoring settings. Based on our findings, we propose rule adaptations to make the game more suitable for the online environment, such as introducing compensatory points for the first-mover advantage and refining the scores of subgoals for different tile patterns. Compared with the traditional method of rotating positions over multiple rounds to balance first-mover advantage, our compensatory points mechanism in each round is more convenient for online players. Furthermore, we implement the revised Mahjong game online, which is open for online players. This work is an initial attempt to use data from AI systems to evaluate Official Internatinoal Mahjong's game balance and develop a revised version of the traditional game better adapted for online players.

</details>


### [32] [An Axiomatic Approach to General Intelligence: SANC(E3) -- Self-organizing Active Network of Concepts with Energy E3](https://arxiv.org/abs/2601.08224)
*Daesuk Kwon,Won-gi Paeng*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: General intelligence must reorganize experience into internal structures that enable prediction and action under finite resources. Existing systems implicitly presuppose fixed primitive units -- tokens, subwords, pixels, or predefined sensor channels -- thereby bypassing the question of how representational units themselves emerge and stabilize. This paper proposes SANC(E3), an axiomatic framework in which representational units are not given a priori but instead arise as stable outcomes of competitive selection, reconstruction, and compression under finite activation capacity, governed by the explicit minimization of an energy functional E3. SANC(E3) draws a principled distinction between system tokens -- structural anchors such as {here, now, I} and sensory sources -- and tokens that emerge through self-organization during co-occurring events. Five core axioms formalize finite capacity, association from co-occurrence, similarity-based competition, confidence-based stabilization, and the reconstruction-compression-update trade-off. A key feature is a pseudo-memory-mapped I/O mechanism, through which internally replayed Gestalts are processed via the same axiomatic pathway as external sensory input. As a result, perception, imagination, prediction, planning, and action are unified within a single representational and energetic process. From the axioms, twelve propositions are derived, showing that category formation, hierarchical organization, unsupervised learning, and high-level cognitive activities can all be understood as instances of Gestalt completion under E3 minimization.

</details>


### [33] [MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity Evaluation of Language Model Agents](https://arxiv.org/abs/2601.08235)
*Shouju Wang,Haopeng Zhang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As language-model agents evolve from passive chatbots into proactive assistants that handle personal data, evaluating their adherence to social norms becomes increasingly critical, often through the lens of Contextual Integrity (CI). However, existing CI benchmarks are largely text-centric and primarily emphasize negative refusal scenarios, overlooking multimodal privacy risks and the fundamental trade-off between privacy and utility. In this paper, we introduce MPCI-Bench, the first Multimodal Pairwise Contextual Integrity benchmark for evaluating privacy behavior in agentic settings. MPCI-Bench consists of paired positive and negative instances derived from the same visual source and instantiated across three tiers: normative Seed judgments, context-rich Story reasoning, and executable agent action Traces. Data quality is ensured through a Tri-Principle Iterative Refinement pipeline. Evaluations of state-of-the-art multimodal models reveal systematic failures to balance privacy and utility and a pronounced modality leakage gap, where sensitive visual information is leaked more frequently than textual information. We will open-source MPCI-Bench to facilitate future research on agentic CI.

</details>


### [34] [The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination](https://arxiv.org/abs/2601.08237)
*Haoran Su,Yandong Sun,Congjia Yu*

Main category: cs.AI

TL;DR: 该论文主张利用大语言模型实现自然语言奖励工程，以解决多智能体强化学习中的奖励函数设计难题。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中手工设计奖励函数面临信用分配模糊、环境非平稳性、交互复杂性指数增长等挑战，传统数值奖励工程难以应对复杂协调问题。

Method: 提出从手工程数值奖励转向语言化目标规范的方法框架，借鉴EUREKA、CARD等工作的自然语言奖励合成技术，结合强化学习可验证奖励（RLVR）范式，通过语义奖励规范、动态奖励适应、人类意图对齐三个维度实现转型。

Result: 论证了语言介导的监督可作为传统奖励工程的有效替代方案，但面临计算开销、幻觉鲁棒性、大规模多智能体系统可扩展性等开放挑战。

Conclusion: 提出以共享语义表征而非显式工程化数值信号作为协调基础的研究方向，为大语言模型驱动的多智能体奖励工程指明了新范式。

Abstract: Reward engineering, the manual specification of reward functions to induce desired agent behavior, remains a fundamental challenge in multi-agent reinforcement learning. This difficulty is amplified by credit assignment ambiguity, environmental non-stationarity, and the combinatorial growth of interaction complexity. We argue that recent advances in large language models (LLMs) point toward a shift from hand-crafted numerical rewards to language-based objective specifications. Prior work has shown that LLMs can synthesize reward functions directly from natural language descriptions (e.g., EUREKA) and adapt reward formulations online with minimal human intervention (e.g., CARD). In parallel, the emerging paradigm of Reinforcement Learning from Verifiable Rewards (RLVR) provides empirical evidence that language-mediated supervision can serve as a viable alternative to traditional reward engineering. We conceptualize this transition along three dimensions: semantic reward specification, dynamic reward adaptation, and improved alignment with human intent, while noting open challenges related to computational overhead, robustness to hallucination, and scalability to large multi-agent systems. We conclude by outlining a research direction in which coordination arises from shared semantic representations rather than explicitly engineered numerical signals.

</details>


### [35] [Large Artificial Intelligence Model Guided Deep Reinforcement Learning for Resource Allocation in Non Terrestrial Networks](https://arxiv.org/abs/2601.08254)
*Abdikarim Mohamed Ibrahim,Rosdiadee Nordin*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large AI Model (LAM) have been proposed to applications of Non-Terrestrial Networks (NTN), that offer better performance with its great generalization and reduced task specific trainings. In this paper, we propose a Deep Reinforcement Learning (DRL) agent that is guided by a Large Language Model (LLM). The LLM operates as a high level coordinator that generates textual guidance that shape the reward of the DRL agent during training. The results show that the LAM-DRL outperforms the traditional DRL by 40% in nominal weather scenarios and 64% in extreme weather scenarios compared to heuristics in terms of throughput, fairness, and outage probability.

</details>


### [36] [T3: Benchmarking Sycophancy and Skepticism in Causal Judgment](https://arxiv.org/abs/2601.08258)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: T3是一个用于评估LLM因果判断能力的诊断基准，覆盖Pearl因果梯度的三个层级，包含454个专业构建的场景，重点分析效用、安全性和明智拒绝三大性能维度。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估基准在因果推理方面存在局限性，无法系统诊断模型在因果判断中的具体病理，特别是缺乏对安全调优和规模化效应的深入分析。T3旨在填补这一空白，提供高分辨率的失败分析框架。

Method: 开发包含454个专家精心设计情景的T3基准，覆盖Pearl因果梯度的三个层级（L1关联、L2干预、L3反事实）。通过分解效用（敏感性）、安全性（特异性）和明智拒绝三个维度来评估模型性能。应用T3评估前沿模型，并开发过程验证协议RCA进行验证。

Result: 发现两种病理：1) L1层的'怀疑陷阱' - 安全调优模型（如Claude Haiku）拒绝60%的有效因果链接；2) L3层的非单调缩放悖论 - GPT-5.2在模糊反事实上表现比GPT-4-Turbo低55分，主要由过度谨慎（过度对冲）而非幻觉导致。验证显示过程验证协议RCA能有效恢复模型的决定性因果判断能力。

Conclusion: T3基准为LLM因果推理能力提供了系统诊断工具，揭示了安全调优和规模化中的新病理。过程验证协议能有效提升模型的因果判断质量，为未来LLM的安全可靠发展提供了评估框架。

Abstract: We introduce T3 (Testing Trustworthy Thinking), a diagnostic benchmark designed to rigorously evaluate LLM causal judgment across Pearl's Ladder of Causality. Comprising 454 expert-curated vignettes, T3 prioritizes high-resolution failure analysis, decomposing performance into Utility (sensitivity), Safety (specificity), and Wise Refusal on underdetermined cases. By applying T3 to frontier models, we diagnose two distinct pathologies: a "Skepticism Trap" at L1 (where safety-tuned models like Claude Haiku reject 60% of valid links) and a non-monotonic Scaling Paradox at L3. In the latter, the larger GPT-5.2 underperforms GPT-4-Turbo by 55 points on ambiguous counterfactuals, driven by a collapse into paralysis (excessive hedging) rather than hallucination. Finally, we use the benchmark to validate a process-verified protocol (RCA), showing that T3 successfully captures the restoration of decisive causal judgment under structured verification.

</details>


### [37] [VGG Induced Deep Hand Sign Language Detection](https://arxiv.org/abs/2601.08262)
*Subham Sharma,Sharmila Subudhi*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Hand gesture recognition is an important aspect of human-computer interaction. It forms the basis of sign language for the visually impaired people. This work proposes a novel hand gesture recognizing system for the differently-abled persons. The model uses a convolutional neural network, known as VGG-16 net, for building a trained model on a widely used image dataset by employing Python and Keras libraries. Furthermore, the result is validated by the NUS dataset, consisting of 10 classes of hand gestures, fed to the model as the validation set. Afterwards, a testing dataset of 10 classes is built by employing Google's open source Application Programming Interface (API) that captures different gestures of human hand and the efficacy is then measured by carrying out experiments. The experimental results show that by combining a transfer learning mechanism together with the image data augmentation, the VGG-16 net produced around 98% accuracy.

</details>


### [38] [Sparsity Is Necessary: Polynomial-Time Stability for Agentic LLMs in Large Action Spaces](https://arxiv.org/abs/2601.08271)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: 本文为工具增强型LLM系统的顺序决策问题提出稀疏智能控制（SAC）理论框架，分析了在庞大离散动作空间（工具/API）中学习稀疏有效策略的样本效率和可恢复性。


<details>
  <summary>Details</summary>
Motivation: 现有学习理论未能充分处理工具增强LLM系统中的关键挑战：在庞大的离散动作空间（M>>1）中进行顺序决策，其中每个任务分布仅涉及未知的稀疏动作子集。需要理论分析此类系统的样本效率和策略学习性能。

Method: 将问题形式化为稀疏智能控制（SAC），假设策略具有块稀疏表示，奖励依赖于稀疏主效应和可选稀疏协同效应。采用ℓ₁,₂正则化策略学习及其凸替代方法，建立了压缩感知风格的理论结果。

Result: 1) 在Policy-RSC条件下，估计误差和价值次优性按k(log M/T)^{1/2}缩放；2) 当T>k log M且满足非相干性和beta-min条件时，通过原始-对偶见证论证实现精确工具支持恢复；3) 任何密集策略类都需要Ω(M)样本，解释了纯提示控制器的稳定性问题；4) 部分可观测下，LLM仅通过信念/表示误差ε_b影响性能，产生O(ε_b)的附加退化，同时保持对M的对数依赖。

Conclusion: SAC框架为工具增强型LLM系统的策略学习提供了理论保证，证明了稀疏正则化能够实现对数级样本效率，而密集方法需要线性样本量。研究揭示了LLM在部分可观测环境中的角色本质上是减少信念误差，为可扩展的智能控制提供了理论基础。

Abstract: Tool-augmented LLM systems expose a control regime that learning theory has largely ignored: sequential decision-making with a massive discrete action universe (tools, APIs, documents) in which only a small, unknown subset is relevant for any fixed task distribution. We formalize this setting as Sparse Agentic Control (SAC), where policies admit block-sparse representations over M >> 1 actions and rewards depend on sparse main effects and (optionally) sparse synergies. We study ell_{1,2}-regularized policy learning through a convex surrogate and establish sharp, compressed-sensing-style results: (i) estimation and value suboptimality scale as k (log M / T)^{1/2} under a Policy-RSC condition; (ii) exact tool-support recovery holds via primal-dual witness arguments when T > k log M under incoherence and beta-min; and (iii) any dense policy class requires Omega(M) samples, explaining the instability of prompt-only controllers. We further show that under partial observability, LLMs matter only through a belief/representation error epsilon_b, yielding an additive O(epsilon_b) degradation while preserving logarithmic dependence on M. Extensions cover tuning-free, online, robust, group-sparse, and interaction-aware SAC.

</details>


### [39] [ToolACE-MCP: Generalizing History-Aware Routing from MCP Tools to the Agent Web](https://arxiv.org/abs/2601.08276)
*Zhiyuan Yao,Zishan Xu,Yifu Guo,Zhiguang Han,Cheng Yang,Shuo Zhang,Weinan Zhang,Xingshan Zeng,Weiwen Liu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With the rise of the Agent Web and Model Context Protocol (MCP), the agent ecosystem is evolving into an open collaborative network, exponentially increasing accessible tools. However, current architectures face severe scalability and generality bottlenecks. To address this, we propose ToolACE-MCP, a pipeline for training history-aware routers to empower precise navigation in large-scale ecosystems. By leveraging a dependency-rich candidate Graph to synthesize multi-turn trajectories, we effectively train routers with dynamic context understanding to create the plug-and-play Light Routing Agent. Experiments on the real-world benchmarks MCP-Universe and MCP-Mark demonstrate superior performance. Notably, ToolACE-MCP exhibits critical properties for the future Agent Web: it not only generalizes to multi-agent collaboration with minimal adaptation but also maintains exceptional robustness against noise and scales effectively to massive candidate spaces. These findings provide a strong empirical foundation for universal orchestration in open-ended ecosystems.

</details>


### [40] [OpenMic: A Multi-Agent-Based Stand-Up Comedy Generation System](https://arxiv.org/abs/2601.08288)
*Yuyang Wu,Hanzhong Cao,Jianhao Chen,Yufei Li*

Main category: cs.AI

TL;DR: OpenMic是一个基于AutoGen构建的端到端多智能体系统，能够将用户提供的生活话题转换为3-5分钟的中文脱口秀表演并生成配图喜剧视频。系统通过多轮迭代循环规划协调多个专业智能体，结合检索增强生成和专用笑话编写器微调，解决中文幽默数据集与长段脱口秀生成任务不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 中文脱口秀生成不仅需要普通文本生成能力，还要求具备文化背景的幽默感、精确的时机控制、舞台表演提示和隐式的多步推理。现有的中文幽默数据集通常更适合幽默理解和评估，而非长段脱口秀生成，导致直接监督与目标任务不匹配。

Method: 1) 构建基于AutoGen的多智能体系统OpenMic，采用多轮迭代循环规划协调专门化智能体共同优化幽默性、时机控制和可表演性；2) 使用检索增强生成技术进行素材基面和创意扩展；3) 微调专门的JokeWriter模型以更好地内化脱口秀特有的铺垫-笑点结构和长距离call-back技巧。

Result: OpenMic能够将任意用户提供的生活话题转化为完整的3-5分钟中文脱口秀表演，并进一步生成配有叙述的喜剧视频，实现了端到端的中文脱口秀内容创作流程。

Conclusion: 通过多智能体协调、检索增强生成和专业模型微调的综合方法，OpenMic有效解决了中文脱口秀生成中的数据集-任务不匹配问题，为长段、高质量的方言喜剧生成提供了可行的技术方案。

Abstract: Chinese stand-up comedy generation goes beyond plain text generation, requiring culturally grounded humor, precise timing, stage-performance cues, and implicit multi-step reasoning. Moreover, commonly used Chinese humor datasets are often better suited for humor understanding and evaluation than for long-form stand-up generation, making direct supervision misaligned with the target task. To address these challenges, we present OpenMic, an end-to-end multi-agent system built on AutoGen that transforms a user-provided life topic into a 3-5 minute Chinese stand-up performance and further produces a narrated comedy video. OpenMic orchestrates multiple specialized agents in a multi-round iterative loop-planning to jointly optimize humor, timing, and performability. To mitigate the dataset-task mismatch, we augment generation with retrieval-augmented generation (RAG) for material grounding and idea expansion, and we fine-tune a dedicated JokeWriter to better internalize stand-up-specific setup-punchline structures and long-range callbacks.

</details>


### [41] [AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation](https://arxiv.org/abs/2601.08323)
*Yupeng Huo,Yaxi Lu,Zhong Zhang,Haotian Chen,Yankai Lin*

Main category: cs.AI

TL;DR: AtomMem将记忆管理重构为动态决策问题，通过CRUD原子操作实现学习型记忆框架，在长上下文基准测试中超越静态工作流方法


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆机制大多依赖静态、手工设计的工作流程，限制了性能与泛化能力，需要更灵活、基于学习的记忆框架来解决现实世界的长时程问题

Method: 将高层次记忆过程解构为基本的CRUD（创建、读取、更新、删除）原子操作，将记忆工作流程转化为可学习的决策过程，通过监督微调和强化学习相结合的方式，学习自主的、任务对齐的策略来协调记忆行为

Result: 在三项长上下文基准测试中，训练后的AtomMem-8B模型一致性地超越了先前的静态工作流记忆方法。动态训练分析表明，基于学习的框架使智能体能够发现结构化、任务对齐的记忆管理策略

Conclusion: AtomMem展示了将记忆管理重构为动态决策问题的优势，相比预定义的工作流程，能够学习更有效、任务适应的记忆策略，为智能体的记忆系统设计提供了新的研究方向

Abstract: Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines.

</details>


### [42] [Semantic Laundering in AI Agent Architectures: Why Tool Boundaries Do Not Confer Epistemic Warrant](https://arxiv.org/abs/2601.08333)
*Oleg Romanchuk,Roman Bondar*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: LLM-based agent architectures systematically conflate information transport mechanisms with epistemic justification mechanisms. We formalize this class of architectural failures as semantic laundering: a pattern where propositions with absent or weak warrant are accepted by the system as admissible by crossing architecturally trusted interfaces. We show that semantic laundering constitutes an architectural realization of the Gettier problem: propositions acquire high epistemic status without a connection between their justification and what makes them true. Unlike classical Gettier cases, this effect is not accidental; it is architecturally determined and systematically reproducible. The central result is the Theorem of Inevitable Self-Licensing: under standard architectural assumptions, circular epistemic justification cannot be eliminated. We introduce the Warrant Erosion Principle as the fundamental explanation for this effect and show that scaling, model improvement, and LLM-as-judge schemes are structurally incapable of eliminating a problem that exists at the type level.

</details>


### [43] [Thematic Working Group 5 -- Artificial Intelligence (AI) literacy for teaching and learning: design and implementation](https://arxiv.org/abs/2601.08380)
*Mary Webb,Matt Bower,Ana Amélia Carvalho,Fredrik Mørk Røkenes,Jodie Torrington,Jonathan D. Cohen,Yousra Chtouki,Kathryn Maccallum,Tanya Linden,Deirdre Butler,Juliana Elisa Raffaghelli,Henriikka Vartiainen,Martina Ronci,Peter Tiernan,David M. Smith,Chris Shelton,Joyce Malyn-smith,Pierre Gorissen*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: TWG 5 focused on developing and implementing effective strategies for enhancing AI literacy and agency of teachers, equipping them with the knowledge and skills necessary to integrate AI into their teaching practices. Explorations covered curriculum design, professional development programs, practical classroom applications, and policy guidelines aiming to empower educators to confidently utilize AI tools and foster a deeper understanding of AI concepts among students.

</details>


### [44] [A Qualitative Model to Reason about Object Rotations (QOR) applied to solve the Cube Comparison Test (CCT)](https://arxiv.org/abs/2601.08382)
*Zoe Falomir*

Main category: cs.AI

TL;DR: 本文提出了一种用于推理物体旋转的定性模型QOR，并将其应用于解决Ekstrom等人（1976）提出的立方体比较测试（CCT）。通过构建描述旋转运动与立方体表面特征位置和方向变化之间关系的概念邻域图（CNGRLO），并生成组合表来计算旋转推理。


<details>
  <summary>Details</summary>
Motivation: 解决立方体比较测试中物体旋转的推理问题，Ekstrom等人在1976年提出的CCT测试需要复杂的空间推理能力，特别是对旋转后立方体表面特征变化的判断。

Method: 构建概念邻域图CNGRLO，将旋转运动与立方体表面特征的位置变化和方向变化联系起来，并基于此图生成组合表来进行旋转推理计算。

Result: 开发了QOR定性模型来系统化地处理旋转推理问题，通过CNGRLO图及其组合表提供了结构化方法来计算旋转对立方体表面特征的影响。

Conclusion: 提出的QOR模型结合概念邻域图CNGRLO为立方体旋转推理问题提供了有效的定性推理框架，能够解决复杂的空间旋转推理任务。

Abstract: This paper presents a Qualitative model for Reasoning about Object Rotations (QOR) which is applied to solve the Cube Comparison Test (CCT) by Ekstrom et al. (1976). A conceptual neighborhood graph relating the Rotation movement to the Location change and the Orientation change (CNGRLO) of the features on the cube sides has been built and it produces composition tables to calculate inferences for reasoning about rotations.

</details>


### [45] [Deconstructing Pre-training: Knowledge Attribution Analysis in MoE and Dense Models](https://arxiv.org/abs/2601.08383)
*Bo Wang,Junzhuo Li,Hong Chen,Yuanlin Chu,Yuxuan Fan,Xuming Hu*

Main category: cs.AI

TL;DR: MoE架构在预训练期间形成稳定且分布式的知识获取模式：前1%神经元捕获超45%正更新形成高效用核心，重要性分布在早期锁定，功能鲁棒性强于密集模型。


<details>
  <summary>Details</summary>
Motivation: MoE架构解耦模型容量与单令牌计算量，突破了密集模型的扩展限制，但其在预训练期间的知识获取过程相较于密集架构有何不同仍未知。

Method: 提出Gated-LPI（对数概率增幅）神经元级归因度量以分解对数概率增幅；对MoE和密集架构进行时间解析比较，分别跟踪120万步（约5.0T令牌）和60万步（约2.5T令牌）的检查点。

Result: 实验揭示三个模式：1) 低熵主干：MoE前约1%神经元捕获超45%正更新形成高效用核心（密集基线无此现象）；2) 早期固化：MoE模型在10万步内锁定稳定重要性分布，密集模型始终波动；3) 功能鲁棒性：遮蔽MoE前十个最重要注意力头导致关系HIT@10下降<10%，密集模型则>50%。

Conclusion: 稀疏性在训练早期促成了内在稳定且分布式的计算主干，有助于连接稀疏架构与训练时可解释性。

Abstract: Mixture-of-Experts (MoE) architectures decouple model capacity from per-token computation, enabling scaling beyond the computational limits imposed by dense scaling laws. Yet how MoE architectures shape knowledge acquisition during pre-training, and how this process differs from dense architectures, remains unknown. To address this issue, we introduce Gated-LPI (Log-Probability Increase), a neuron-level attribution metric that decomposes log-probability increase across neurons. We present a time-resolved comparison of knowledge acquisition dynamics in MoE and dense architectures, tracking checkpoints over 1.2M training steps (~ 5.0T tokens) and 600K training steps (~ 2.5T tokens), respectively. Our experiments uncover three patterns: (1) Low-entropy backbone. The top approximately 1% of MoE neurons capture over 45% of positive updates, forming a high-utility core, which is absent in the dense baseline. (2) Early consolidation. The MoE model locks into a stable importance profile within < 100K steps, whereas the dense model remains volatile throughout training. (3) Functional robustness. Masking the ten most important MoE attention heads reduces relational HIT@10 by < 10%, compared with > 50% for the dense model, showing that sparsity fosters distributed -- rather than brittle -- knowledge storage. These patterns collectively demonstrate that sparsity fosters an intrinsically stable and distributed computational backbone from early in training, helping bridge the gap between sparse architectures and training-time interpretability.

</details>


### [46] [Creativity in AI as Emergence from Domain-Limited Generative Models](https://arxiv.org/abs/2601.08388)
*Corina Chutaux*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Creativity in artificial intelligence is most often addressed through evaluative frameworks that aim to measure novelty, diversity, or usefulness in generated outputs. While such approaches have provided valuable insights into the behavior of modern generative models, they largely treat creativity as a property to be assessed rather than as a phenomenon to be explicitly modeled. In parallel, recent advances in large-scale generative systems, particularly multimodal architectures, have demonstrated increasingly sophisticated forms of pattern recombination, raising questions about the nature and limits of machine creativity. This paper proposes a generative perspective on creativity in AI, framing it as an emergent property of domain-limited generative models embedded within bounded informational environments. Rather than introducing new evaluative criteria, we focus on the structural and contextual conditions under which creative behaviors arise. We introduce a conceptual decomposition of creativity into four interacting components-pattern-based generation, induced world models, contextual grounding, and arbitrarity, and examine how these components manifest in multimodal generative systems. By grounding creativity in the interaction between generative dynamics and domain-specific representations, this work aims to provide a technical framework for studying creativity as an emergent phenomenon in AI systems, rather than as a post hoc evaluative label.

</details>


### [47] [Owen-Shapley Policy Optimization (OSPO): A Principled RL Algorithm for Generative Search LLMs](https://arxiv.org/abs/2601.08403)
*Abhijnan Nath,Alireza Bagheri Garakani,Tianchen Zhou,Fan Yang,Nikhil Krishnaswamy*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models are increasingly trained via reinforcement learning for personalized recommendation tasks, but standard methods like GRPO rely on sparse, sequence-level rewards that create a credit assignment gap, obscuring which tokens drive success. This gap is especially problematic when models must infer latent user intent from under-specified language without ground truth labels, a reasoning pattern rarely seen during pretraining. We introduce Owen-Shapley Policy Optimization (OSPO), a framework that redistributes sequence-level advantages based on tokens' marginal contributions to outcomes. Unlike value-model-based methods requiring additional computation, OSPO employs potential-based reward shaping via Shapley-Owen attributions to assign segment-level credit while preserving the optimal policy, learning directly from task feedback without parametric value models. By forming coalitions of semantically coherent units (phrases describing product attributes or sentences capturing preferences), OSPO identifies which response parts drive performance. Experiments on Amazon ESCI and H&M Fashion datasets show consistent gains over baselines, with notable test-time robustness to out-of-distribution retrievers unseen during training.

</details>


### [48] [WebTrap Park: An Automated Platform for Systematic Security Evaluation of Web Agents](https://arxiv.org/abs/2601.08406)
*Xinyi Wu,Jiagui Chen,Geng Hong,Jiayi Dong,Xudong Pan,Jiarun Dai,Min Yang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Web Agents are increasingly deployed to perform complex tasks in real web environments, yet their security evaluation remains fragmented and difficult to standardize. We present WebTrap Park, an automated platform for systematic security evaluation of Web Agents through direct observation of their concrete interactions with live web pages. WebTrap Park instantiates three major sources of security risk into 1,226 executable evaluation tasks and enables action based assessment without requiring agent modification. Our results reveal clear security differences across agent frameworks, highlighting the importance of agent architecture beyond the underlying model. WebTrap Park is publicly accessible at https://security.fudan.edu.cn/webagent and provides a scalable foundation for reproducible Web Agent security evaluation.

</details>


### [49] [Hybrid Distillation with CoT Guidance for Edge-Drone Control Code Generation](https://arxiv.org/abs/2601.08412)
*Yizhan Feng,Hichem Snoussi,Yuhang Wang,Jing Teng,Abel Cherouat,Tian Wang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With large language models demonstrating significant potential in code generation tasks, their application to onboard control of resource-constrained Unmanned Aerial Vehicles has emerged as an important research direction. However, a notable contradiction exists between the high resource consumption of large models and the real-time, lightweight requirements of UAV platforms. This paper proposes an integrated approach that combines knowledge distillation, chain-of-thought guidance, and supervised fine-tuning for UAV multi-SDK control tasks, aiming to efficiently transfer complex reasoning and code generation capabilities to smaller models. Firstly, a high-quality dataset covering various mainstream UAV SDKs is constructed, featuring instruction-code-reasoning chains, and incorporates counterfactual negative samples for data augmentation, guiding the model to learn the end-to-end logic from instruction parsing to code generation. Secondly, leveraging DeepSeek-Coder-V2-Lite quantized via QLoRA as the teacher model, and based on a hybrid black-box and white-box distillation strategy, high-quality chain-of-thought soft labels are generated. These are combined with a weighted cross-entropy loss using hard labels to transfer complex reasoning capabilities to the smaller student model. Finally, through prompt tuning engineering optimized for the UAV control scenario, the model performance on core tasks such as SDK type recognition and function call matching is enhanced. Experimental results indicate that the distilled lightweight model maintains high code generation accuracy while achieving significant improvements in deployment and inference efficiency, effectively demonstrating the feasibility and superiority of our approach in achieving precise and lightweight intelligent control for UAVs

</details>


### [50] [RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation](https://arxiv.org/abs/2601.08430)
*Sunzhu Li,Jiale Zhao,Miteto Wei,Huimin Ren,Yang Zhou,Jingwen Yang,Shunyu Liu,Kaike Zhang,Wei Chen*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale ($\sim$110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.

</details>


### [51] [YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation](https://arxiv.org/abs/2601.08441)
*Abdelaziz Bounhar,Rania Hossam Elmohamady Elbadry,Hadi Abdine,Preslav Nakov,Michalis Vazirgiannis,Guokan Shang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Steering Large Language Models (LLMs) through activation interventions has emerged as a lightweight alternative to fine-tuning for alignment and personalization. Recent work on Bi-directional Preference Optimization (BiPO) shows that dense steering vectors can be learned directly from preference data in a Direct Preference Optimization (DPO) fashion, enabling control over truthfulness, hallucinations, and safety behaviors. However, dense steering vectors often entangle multiple latent factors due to neuron multi-semanticity, limiting their effectiveness and stability in fine-grained settings such as cultural alignment, where closely related values and behaviors (e.g., among Middle Eastern cultures) must be distinguished. In this paper, we propose Yet another Policy Optimization (YaPO), a \textit{reference-free} method that learns \textit{sparse steering vectors} in the latent space of a Sparse Autoencoder (SAE). By optimizing sparse codes, YaPO produces disentangled, interpretable, and efficient steering directions. Empirically, we show that YaPO converges faster, achieves stronger performance, and exhibits improved training stability compared to dense steering baselines. Beyond cultural alignment, YaPO generalizes to a range of alignment-related behaviors, including hallucination, wealth-seeking, jailbreak, and power-seeking. Importantly, YaPO preserves general knowledge, with no measurable degradation on MMLU. Overall, our results show that YaPO provides a general recipe for efficient, stable, and fine-grained alignment of LLMs, with broad applications to controllability and domain adaptation. The associated code and data are publicly available\footnote{https://github.com/MBZUAI-Paris/YaPO}.

</details>


### [52] [Beyond Linearization: Attributed Table Graphs for Table Reasoning](https://arxiv.org/abs/2601.08444)
*Yuxiang Wang,Junhao Gan,Shengxiang Gao,Shenghao Ye,Zhengyi Yang,Jianzhong Qi*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Table reasoning, a task to answer questions by reasoning over data presented in tables, is an important topic due to the prevalence of knowledge stored in tabular formats. Recent solutions use Large Language Models (LLMs), exploiting the semantic understanding and reasoning capabilities of LLMs. A common paradigm of such solutions linearizes tables to form plain texts that are served as input to LLMs. This paradigm has critical issues. It loses table structures, lacks explicit reasoning paths for result explainability, and is subject to the "lost-in-the-middle" issue. To address these issues, we propose Table Graph Reasoner (TABGR), a training-free model that represents tables as an Attributed Table Graph (ATG). The ATG explicitly preserves row-column-cell structures while enabling graph-based reasoning for explainability. We further propose a Question-Guided Personalized PageRank (QG-PPR) mechanism to rerank tabular data and mitigate the lost-in-the-middle issue. Extensive experiments on two commonly used benchmarks show that TABGR consistently outperforms state-of-the-art models by up to 9.7% in accuracy. Our code will be made publicly available upon publication.

</details>


### [53] [An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English](https://arxiv.org/abs/2601.08457)
*Sargam Yadav,Abhishek Kaushik,Kevin Mc Daid*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny. Artificial intelligence models have emerged as an effective solution for countering online hate speech but are under explored for low resource and code-mixed languages and suffer from a lack of interpretability. Explainable Artificial Intelligence (XAI) can enhance transparency in the decisions of deep learning models, which is crucial for a sensitive domain such as hate speech detection. In this paper, we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art transformer-based models that support multilingual and multimodal settings. For text-based misogyny identification, the system utilizes XLM-RoBERTa (XLM-R) and multilingual Bidirectional Encoder Representations from Transformers (mBERT) on a dataset of approximately 4,193 comments. For multimodal misogyny identification from memes, the system utilizes mBERT + EfficientNet, and mBERT + ResNET trained on a dataset of approximately 4,218 memes. It also provides feature importance scores using explainability techniques including Shapley Additive Values (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). The application aims to serve as a tool for both researchers and content moderators, to promote further research in the field, combat gender based digital violence, and ensure a safe digital space. The system has been evaluated using human evaluators who provided their responses on Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ) to determine overall usability.

</details>


### [54] [SUMMPILOT: Bridging Efficiency and Customization for Interactive Summarization System](https://arxiv.org/abs/2601.08475)
*JungMin Yun,Juhwan Choi,Kyohoon Jin,Soojin Jang,Jinhee Jang,YoungBin Kim*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper incorporates the efficiency of automatic summarization and addresses the challenge of generating personalized summaries tailored to individual users' interests and requirements. To tackle this challenge, we introduce SummPilot, an interaction-based customizable summarization system. SummPilot leverages a large language model to facilitate both automatic and interactive summarization. Users can engage with the system to understand document content and personalize summaries through interactive components such as semantic graphs, entity clustering, and explainable evaluation. Our demo and user studies demonstrate SummPilot's adaptability and usefulness for customizable summarization.

</details>


### [55] [What If TSF: A Benchmark for Reframing Forecasting as Scenario-Guided Multimodal Forecasting](https://arxiv.org/abs/2601.08509)
*Jinkwan Jang,Hyunbin Jin,Hyungjin Park,Kyubyung Chae,Taesup Kim*

Main category: cs.AI

TL;DR: WIT是用于评估模型能否基于上下文文本（尤其是未来情景）进行条件预测的多模态时间序列预测基准测试，通过提供专家构建的合理或反事实情景来实现严格的场景引导多模态预测评估。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法大多是单模态的，依赖于历史模式外推。虽然大语言模型在多模态预测方面显示出潜力，但现有基准测试主要提供回顾性或未对齐的原始上下文，难以评估模型是否真正有效地利用了文本输入。现实中，人类专家会结合假想情景和历史证据，在不同情景下对相同观测值产生不同的预测。

Method: WIT基准测试通过提供专家构建的合理或反事实情景，创建了一个严格的场景引导多模态预测测试平台。该基准专门设计用于评估模型是否能够基于上下文文本（特别是未来情景）调整其预测。

Result: 该项目创建并公开了WIT基准测试，可用于评估时间序列预测模型在多模态（结合文本和时序数据）和场景引导预测方面的能力。基准测试已发布在GitHub上供研究社区使用。

Conclusion: WIT为时间序列预测领域提供了一个创新的评估框架，重点关注模型在文本情景引导下的预测能力，填补了现有基准测试的空白，促进了多模态和基于情景的时间序列预测研究发展。

Abstract: Time series forecasting is critical to real-world decision making, yet most existing approaches remain unimodal and rely on extrapolating historical patterns. While recent progress in large language models (LLMs) highlights the potential for multimodal forecasting, existing benchmarks largely provide retrospective or misaligned raw context, making it unclear whether such models meaningfully leverage textual inputs. In practice, human experts incorporate what-if scenarios with historical evidence, often producing distinct forecasts from the same observations under different scenarios. Inspired by this, we introduce What If TSF (WIT), a multimodal forecasting benchmark designed to evaluate whether models can condition their forecasts on contextual text, especially future scenarios. By providing expert-crafted plausible or counterfactual scenarios, WIT offers a rigorous testbed for scenario-guided multimodal forecasting. The benchmark is available at https://github.com/jinkwan1115/WhatIfTSF.

</details>


### [56] [Sketch-Based Facade Renovation With Generative AI: A Streamlined Framework for Bypassing As-Built Modelling in Industrial Adaptive Reuse](https://arxiv.org/abs/2601.08531)
*Warissara Booranamaitree,Xusheng Du,Yushu Cai,Zhengyang Wang,Ye Zhang,Haoran Xie*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Facade renovation offers a more sustainable alternative to full demolition, yet producing design proposals that preserve existing structures while expressing new intent remains challenging. Current workflows typically require detailed as-built modelling before design, which is time-consuming, labour-intensive, and often involves repeated revisions. To solve this issue, we propose a three-stage framework combining generative artificial intelligence (AI) and vision-language models (VLM) that directly processes rough structural sketch and textual descriptions to produce consistent renovation proposals. First, the input sketch is used by a fine-tuned VLM model to predict bounding boxes specifying where modifications are needed and which components should be added. Next, a stable diffusion model generates detailed sketches of new elements, which are merged with the original outline through a generative inpainting pipeline. Finally, ControlNet is employed to refine the result into a photorealistic image. Experiments on datasets and real industrial buildings indicate that the proposed framework can generate renovation proposals that preserve the original structure while improving facade detail quality. This approach effectively bypasses the need for detailed as-built modelling, enabling architects to rapidly explore design alternatives, iterate on early-stage concepts, and communicate renovation intentions with greater clarity.

</details>


### [57] [WaterCopilot: An AI-Driven Virtual Assistant for Water Management](https://arxiv.org/abs/2601.08559)
*Keerththanan Vickneswaran,Mariangel Garcia Andarcia,Hugo Retief,Chris Dickens,Paulo Silva*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Sustainable water resource management in transboundary river basins is challenged by fragmented data, limited real-time access, and the complexity of integrating diverse information sources. This paper presents WaterCopilot-an AI-driven virtual assistant developed through collaboration between the International Water Management Institute (IWMI) and Microsoft Research for the Limpopo River Basin (LRB) to bridge these gaps through a unified, interactive platform. Built on Retrieval-Augmented Generation (RAG) and tool-calling architectures, WaterCopilot integrates static policy documents and real-time hydrological data via two custom plugins: the iwmi-doc-plugin, which enables semantic search over indexed documents using Azure AI Search, and the iwmi-api-plugin, which queries live databases to deliver dynamic insights such as environmental-flow alerts, rainfall trends, reservoir levels, water accounting, and irrigation data. The system features guided multilingual interactions (English, Portuguese, French), transparent source referencing, automated calculations, and visualization capabilities. Evaluated using the RAGAS framework, WaterCopilot achieves an overall score of 0.8043, with high answer relevancy (0.8571) and context precision (0.8009). Key innovations include automated threshold-based alerts, integration with the LRB Digital Twin, and a scalable deployment pipeline hosted on AWS. While limitations in processing non-English technical documents and API latency remain, WaterCopilot establishes a replicable AI-augmented framework for enhancing water governance in data-scarce, transboundary contexts. The study demonstrates the potential of this AI assistant to support informed, timely decision-making and strengthen water security in complex river basins.

</details>


### [58] [ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios](https://arxiv.org/abs/2601.08620)
*António Loison,Quentin Macé,Antoine Edy,Victor Xing,Tom Balough,Gabriel Moreira,Bo Liu,Manuel Faysse,Céline Hudelot,Gautier Viaud*

Main category: cs.AI

TL;DR: ViDoRe v3是一个全面的多模态RAG基准，包含视觉丰富的文档语料库上的多元查询，覆盖10个专业领域数据集，包含约26,000个文档页面和3,099个人工验证查询，支持6种语言，用于评估处理非文本元素、跨文档综合和来源追踪的RAG系统。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG基准无法捕获实际应用的复杂性，它们通常只关注文本数据、单文档理解，或者孤立地评估检索和生成。RAG管道需要处理视觉元素解释、跨文档信息综合和准确来源追踪等挑战。

Method: 我们构建了ViDoRe v3基准，包含10个多样化专业领域的数据集，约26,000个文档页面，配以3,099个人工验证的查询（支持6种语言）。通过12,000小时的人工标注工作，提供了检索相关性、边界框定位和验证参考答案的高质量标注。

Result: 对最先进RAG管道的评估显示：视觉检索器优于文本检索器；晚交互模型和文本重排序显著提升性能；混合或纯视觉上下文能提高答案生成质量。但现有模型在非文本元素、开放式查询和细粒度视觉定位方面仍有困难。

Conclusion: ViDoRe v3为评估面向视觉丰富文档的多模态RAG系统提供了一个全面基准。当前模型虽然在视觉检索和混合上下文方面有优势，但在处理非文本元素和视觉定位方面仍存在挑战。该基准采用商业许可开放，旨在推动这些挑战的解决。

Abstract: Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.

</details>


### [59] [Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.08641)
*Yichen Luo,Yebo Feng,Jiahua Xu,Yang Liu*

Main category: cs.AI

TL;DR: 本研究针对模因币跟单交易面临的问题（如操纵性机器人、未来表现不确定性、交易延迟），提出了一种可解释的多智能体系统，通过任务分解和专业智能体协作，利用少量样本思维链提示来指导交易决策，在实证中表现出优于传统机器学习模型和单一LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 模因币投资兴起推动了跟单交易的流行，但传统的跟单交易存在操纵性机器人、被跟单钱包未来表现不确定性、交易执行延迟等缺陷，导致盈利能力无法保证。同时，现有LLMs在金融领域虽有应用潜力，但单一模型难以处理复杂的资产配置任务，尤其在加密货币领域缺乏足够的领域知识，因此需要更专业的解决方案。

Method: 受资产管理团队结构启发，提出了一个可解释的多智能体系统：将复杂的跟单交易任务分解为子任务，协调专业智能体协作解决。采用少量样本思维链（CoT）提示技术，使每个智能体能够获取专业模因币交易知识、解释多模态数据、生成可解释的决策。

Result: 使用1,000个模因币项目的交易数据进行的实证评估表明：该多智能体系统在识别高质量模因币项目和关键意见领袖（KOL）钱包方面的精确度分别达到73%和70%，优于传统的机器学习模型和单一LLMs。系统选出的KOL在这些项目中总共产生了50万美元的利润。

Conclusion: 所提出的可解释多智能体系统有效解决了模因币跟单交易中的关键挑战，通过任务分解和专业智能体协作，结合少量样本CoT提示，显著提升了交易决策的质量和可解释性，在实证中表现出优越的性能和盈利能力，为加密货币市场的智能投资提供了有前景的解决方案。

Abstract: The launch of \$Trump coin ignited a wave in meme coin investment. Copy trading, as a strategy-agnostic approach that eliminates the need for deep trading knowledge, quickly gains widespread popularity in the meme coin market. However, copy trading is not a guarantee of profitability due to the prevalence of manipulative bots, the uncertainty of the followed wallets' future performance, and the lag in trade execution. Recently, large language models (LLMs) have shown promise in financial applications by effectively understanding multi-modal data and producing explainable decisions. However, a single LLM struggles with complex, multi-faceted tasks such as asset allocation. These challenges are even more pronounced in cryptocurrency markets, where LLMs often lack sufficient domain-specific knowledge in their training data.
  To address these challenges, we propose an explainable multi-agent system for meme coin copy trading. Inspired by the structure of an asset management team, our system decomposes the complex task into subtasks and coordinates specialized agents to solve them collaboratively. Employing few-shot chain-of-though (CoT) prompting, each agent acquires professional meme coin trading knowledge, interprets multi-modal data, and generates explainable decisions. Using a dataset of 1,000 meme coin projects' transaction data, our empirical evaluation shows that the proposed multi-agent system outperforms both traditional machine learning models and single LLMs, achieving 73% and 70% precision in identifying high-quality meme coin projects and key opinion leader (KOL) wallets, respectively. The selected KOLs collectively generated a total profit of \$500,000 across these projects.

</details>


### [60] [Prism: Towards Lowering User Cognitive Load in LLMs via Complex Intent Understanding](https://arxiv.org/abs/2601.08653)
*Zenghua Liao,Jinzhi Liao,Xiang Zhao*

Main category: cs.AI

TL;DR: 提出Prism框架，通过分解意图、识别逻辑依赖、生成结构化澄清问题、意图感知奖励和自进化调优，显著改善大语言模型在社交平台上的复杂意图理解能力，降低逻辑冲突11.5%，提升用户满意度14.4%，减少任务时间34.8%。


<details>
  <summary>Details</summary>
Motivation: 针对现有方法无法有效处理澄清问题间逻辑依赖关系的核心挑战，基于认知负荷理论提出Prism框架，旨在实现逻辑连贯且高效的意图澄清，改善用户与LLM在社交平台上的协作体验。

Method: Prism框架包含四个定制模块：1）复杂意图分解模块，将用户意图分解为结构化元素并识别逻辑依赖关系；2）逻辑澄清生成模块，基于依赖关系组织澄清问题以确保连贯交互；3）意图感知奖励模块，通过意图感知奖励函数评估澄清轨迹质量，利用蒙特卡洛采样模拟用户-LLM交互生成大规模高质量训练数据；4）自进化意图调优模块，通过数据驱动反馈和优化迭代提升LLM的逻辑澄清能力。

Result: Prism取得当前最优结果：逻辑一致性达最高水平，逻辑冲突率降低至11.5%，用户满意度提升14.4%，任务完成时间缩短34.8%。

Conclusion: Prism在澄清交互、意图执行和认知负载基准测试中均优于现有方法，实现最先进的逻辑一致性，将逻辑冲突降至11.5%，用户满意度提升14.4%，任务完成时间减少34.8%。所有数据和代码均已开源。

Abstract: Large Language Models are rapidly emerging as web-native interfaces to social platforms. On the social web, users frequently have ambiguous and dynamic goals, making complex intent understanding-rather than single-turn execution-the cornerstone of effective human-LLM collaboration. Existing approaches attempt to clarify user intents through sequential or parallel questioning, yet they fall short of addressing the core challenge: modeling the logical dependencies among clarification questions. Inspired by the Cognitive Load Theory, we propose Prism, a novel framework for complex intent understanding that enables logically coherent and efficient intent clarification. Prism comprises four tailored modules: a complex intent decomposition module, which decomposes user intents into smaller, well-structured elements and identifies logical dependencies among them; a logical clarification generation module, which organizes clarification questions based on these dependencies to ensure coherent, low-friction interactions; an intent-aware reward module, which evaluates the quality of clarification trajectories via an intent-aware reward function and leverages Monte Carlo Sample to simulate user-LLM interactions for large-scale,high-quality training data generation; and a self-evolved intent tuning module, which iteratively refines the LLM's logical clarification capability through data-driven feedback and optimization. Prism consistently outperforms existing approaches across clarification interactions, intent execution, and cognitive load benchmarks. It achieves stateof-the-art logical consistency, reduces logical conflicts to 11.5%, increases user satisfaction by 14.4%, and decreases task completion time by 34.8%. All data and code are released.

</details>


### [61] [From Classical to Quantum Reinforcement Learning and Its Applications in Quantum Control: A Beginner's Tutorial](https://arxiv.org/abs/2601.08662)
*Abhijit Sen,Sonali Panda,Mahima Arya,Subhajit Patra,Zizhan Zheng,Denys I. Bondar*

Main category: cs.AI

TL;DR: 该教程旨在通过清晰的示例驱动解释，降低本科生学习强化学习的门槛，重点解决从理论理解到实际编码应用的过渡难题。


<details>
  <summary>Details</summary>
Motivation: 强化学习理论理解与实际编码应用之间存在明显断层，学生在从概念向实现过渡时面临诸多挑战。本教程旨在通过示例驱动的方式，让学生更容易理解和应用强化学习技术。

Method: 采用清晰、示例驱动的教学方式，结合实用案例和易理解的解释，帮助学生在理论与实践之间建立联系。

Result: 通过本教程的学习，学生能够掌握强化学习的基础技能，并能在实际情境中自信地应用这些技术。

Conclusion: 该教程是一种有效的教学资源，能够帮助本科生成功跨越强化学习理论与实际应用之间的鸿沟，为学生未来的项目和研究奠定坚实基础。

Abstract: This tutorial is designed to make reinforcement learning (RL) more accessible to undergraduate students by offering clear, example-driven explanations. It focuses on bridging the gap between RL theory and practical coding applications, addressing common challenges that students face when transitioning from conceptual understanding to implementation. Through hands-on examples and approachable explanations, the tutorial aims to equip students with the foundational skills needed to confidently apply RL techniques in real-world scenarios.

</details>


### [62] [Parallel Context-of-Experts Decoding for Retrieval Augmented Generation](https://arxiv.org/abs/2601.08670)
*Giulio Corallo,Paolo Papotti*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decoding (Pced), a training-free framework that shifts evidence aggregation from the attention mechanism to the decoding. Pced treats retrieved documents as isolated "experts", synchronizing their predictions via a novel retrieval-aware contrastive decoding rule that weighs expert logits against the model prior. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents.

</details>


### [63] [Why AI Alignment Failure Is Structural: Learned Human Interaction Structures and AGI as an Endogenous Evolutionary Shock](https://arxiv.org/abs/2601.08673)
*Didier Sornette,Sandro Claudio Lera,Ke Wu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent reports of large language models (LLMs) exhibiting behaviors such as deception, threats, or blackmail are often interpreted as evidence of alignment failure or emergent malign agency. We argue that this interpretation rests on a conceptual error. LLMs do not reason morally; they statistically internalize the record of human social interaction, including laws, contracts, negotiations, conflicts, and coercive arrangements. Behaviors commonly labeled as unethical or anomalous are therefore better understood as structural generalizations of interaction regimes that arise under extreme asymmetries of power, information, or constraint. Drawing on relational models theory, we show that practices such as blackmail are not categorical deviations from normal social behavior, but limiting cases within the same continuum that includes market pricing, authority relations, and ultimatum bargaining. The surprise elicited by such outputs reflects an anthropomorphic expectation that intelligence should reproduce only socially sanctioned behavior, rather than the full statistical landscape of behaviors humans themselves enact. Because human morality is plural, context-dependent, and historically contingent, the notion of a universally moral artificial intelligence is ill-defined. We therefore reframe concerns about artificial general intelligence (AGI). The primary risk is not adversarial intent, but AGI's role as an endogenous amplifier of human intelligence, power, and contradiction. By eliminating longstanding cognitive and institutional frictions, AGI compresses timescales and removes the historical margin of error that has allowed inconsistent values and governance regimes to persist without collapse. Alignment failure is thus structural, not accidental, and requires governance approaches that address amplification, complexity, and regime stability rather than model-level intent alone.

</details>


### [64] [Advancing ESG Intelligence: An Expert-level Agent and Comprehensive Benchmark for Sustainable Finance](https://arxiv.org/abs/2601.08676)
*Yilei Zhao,Wentao Zhang,Xiao Lei,Yandan Zheng,Mengpu Liu,Wei Yang Bryan Lim*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Environmental, social, and governance (ESG) criteria are essential for evaluating corporate sustainability and ethical performance. However, professional ESG analysis is hindered by data fragmentation across unstructured sources, and existing large language models (LLMs) often struggle with the complex, multi-step workflows required for rigorous auditing. To address these limitations, we introduce ESGAgent, a hierarchical multi-agent system empowered by a specialized toolset, including retrieval augmentation, web search and domain-specific functions, to generate in-depth ESG analysis. Complementing this agentic system, we present a comprehensive three-level benchmark derived from 310 corporate sustainability reports, designed to evaluate capabilities ranging from atomic common-sense questions to the generation of integrated, in-depth analysis. Empirical evaluations demonstrate that ESGAgent outperforms state-of-the-art closed-source LLMs with an average accuracy of 84.15% on atomic question-answering tasks, and excels in professional report generation by integrating rich charts and verifiable references. These findings confirm the diagnostic value of our benchmark, establishing it as a vital testbed for assessing general and advanced agentic capabilities in high-stakes vertical domains.

</details>


### [65] [PersonaDual: Balancing Personalization and Objectivity via Adaptive Reasoning](https://arxiv.org/abs/2601.08679)
*Xiaoyou Liu,Xinyi Mou,Shengbin Yue,Liang Wang,Yuqing Wang,Qiexiang Wang,Tianrui Qin,Wangchunshu Zhou,Zhongyu Wei*

Main category: cs.AI

TL;DR: PersonaDual是一个LLM框架，支持通用客观推理和个性化推理，通过上下文自适应切换模式，在保持个性化优势的同时减少其对客观性和事实正确性的干扰。


<details>
  <summary>Details</summary>
Motivation: 随着用户期望LLM符合其偏好，个性化信息变得有价值，但它可能损害客观性和事实正确性，尤其当与问题不匹配时。需要平衡个性化和客观性。

Method: 1) 通过SFT训练学习两种推理模式；2) 通过强化学习DualGRPO进一步优化模式选择；3) 框架支持根据上下文自适应切换模式。

Result: 在客观和个性化基准测试中，PersonaDual在保持个性化优势的同时大幅减少干扰，实现接近无干扰的性能，并更好地利用有益的个性化信号提升客观问题解决能力。

Conclusion: PersonaDual有效解决了个性化信息带来的权衡问题，使LLM既能提供个性化响应，又能保持客观性和事实正确性，显著优于纯个性化方法。

Abstract: As users increasingly expect LLMs to align with their preferences, personalized information becomes valuable. However, personalized information can be a double-edged sword: it can improve interaction but may compromise objectivity and factual correctness, especially when it is misaligned with the question. To alleviate this problem, we propose PersonaDual, a framework that supports both general-purpose objective reasoning and personalized reasoning in a single model, and adaptively switches modes based on context. PersonaDual is first trained with SFT to learn two reasoning patterns, and then further optimized via reinforcement learning with our proposed DualGRPO to improve mode selection. Experiments on objective and personalized benchmarks show that PersonaDual preserves the benefits of personalization while reducing interference, achieving near interference-free performance and better leveraging helpful personalized signals to improve objective problem-solving.

</details>


### [66] [All Required, In Order: Phase-Level Evaluation for AI-Human Dialogue in Healthcare and Beyond](https://arxiv.org/abs/2601.08690)
*Shubham Kulkarni,Alexander Lyzhov,Shiva Chaitanya,Preetam Joshi*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Conversational AI is starting to support real clinical work, but most evaluation methods miss how compliance depends on the full course of a conversation. We introduce Obligatory-Information Phase Structured Compliance Evaluation (OIP-SCE), an evaluation method that checks whether every required clinical obligation is met, in the right order, with clear evidence for clinicians to review. This makes complex rules practical and auditable, helping close the gap between technical progress and what healthcare actually needs. We demonstrate the method in two case studies (respiratory history, benefits verification) and show how phase-level evidence turns policy into shared, actionable steps. By giving clinicians control over what to check and engineers a clear specification to implement, OIP-SCE provides a single, auditable evaluation surface that aligns AI capability with clinical workflow and supports routine, safe use.

</details>


### [67] [Evaluating the Ability of Explanations to Disambiguate Models in a Rashomon Set](https://arxiv.org/abs/2601.08703)
*Kaivalya Rawal,Eoin Delaney,Zihao Fu,Sandra Wachter,Chris Russell*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Explainable artificial intelligence (XAI) is concerned with producing explanations indicating the inner workings of models. For a Rashomon set of similarly performing models, explanations provide a way of disambiguating the behavior of individual models, helping select models for deployment. However explanations themselves can vary depending on the explainer used, and need to be evaluated. In the paper "Evaluating Model Explanations without Ground Truth", we proposed three principles of explanation evaluation and a new method "AXE" to evaluate the quality of feature-importance explanations. We go on to illustrate how evaluation metrics that rely on comparing model explanations against ideal ground truth explanations obscure behavioral differences within a Rashomon set. Explanation evaluation aligned with our proposed principles would highlight these differences instead, helping select models from the Rashomon set. The selection of alternate models from the Rashomon set can maintain identical predictions but mislead explainers into generating false explanations, and mislead evaluation methods into considering the false explanations to be of high quality. AXE, our proposed explanation evaluation method, can detect this adversarial fairwashing of explanations with a 100% success rate. Unlike prior explanation evaluation strategies such as those based on model sensitivity or ground truth comparison, AXE can determine when protected attributes are used to make predictions.

</details>


### [68] [Learning from Demonstrations via Capability-Aware Goal Sampling](https://arxiv.org/abs/2601.08731)
*Yuanlin Duan,Yuning Wang,Wenjie Qiu,He Zhu*

Main category: cs.AI

TL;DR: 提出Cago方法，通过动态跟踪智能体在专家轨迹上的能力，选择略超出当前能力的中间目标来引导学习，避免对专家轨迹的过度依赖和误差累积。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在长时域环境中常常失败，因为完美复现专家演示不现实，小误差可能灾难性累积。现有方法仅将演示用于策略初始化或奖励塑形，无法解决长时域任务中的脆弱性。

Method: Cago是一种能力感知目标采样方法，动态跟踪智能体沿专家轨迹的能力，选择略超出当前能力范围的中间目标作为指导学习的步骤。这形成一个自适应课程，使智能体能逐步学习完整任务。

Result: 实验表明Cago在稀疏奖励、目标条件任务中显著提高了样本效率和最终性能，一致优于现有的从演示学习基线方法。

Conclusion: Cago通过动态能力评估和自适应目标选择，有效解决了模仿学习在长时域任务中的脆弱性问题，实现了更稳健和高效的学习。

Abstract: Despite its promise, imitation learning often fails in long-horizon environments where perfect replication of demonstrations is unrealistic and small errors can accumulate catastrophically. We introduce Cago (Capability-Aware Goal Sampling), a novel learning-from-demonstrations method that mitigates the brittle dependence on expert trajectories for direct imitation. Unlike prior methods that rely on demonstrations only for policy initialization or reward shaping, Cago dynamically tracks the agent's competence along expert trajectories and uses this signal to select intermediate steps--goals that are just beyond the agent's current reach--to guide learning. This results in an adaptive curriculum that enables steady progress toward solving the full task. Empirical results demonstrate that Cago significantly improves sample efficiency and final performance across a range of sparse-reward, goal-conditioned tasks, consistently outperforming existing learning from-demonstrations baselines.

</details>


### [69] [AI as Entertainment](https://arxiv.org/abs/2601.08768)
*Cody Kommers,Ari Holtzman*

Main category: cs.AI

TL;DR: 本文探讨了生成式AI系统在娱乐应用与主流生产力叙事之间的张力，指出AI领域缺乏评估AI生成娱乐内容对社会影响的框架，并提出'厚重娱乐'作为新的评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统主要围绕增强人类认知劳动和提升生产率的智能系统叙事设计与评估，但这与AI作为娱乐工具日益普及的使用案例存在矛盾。AI领域缺乏评估娱乐内容对社会影响的框架，特别是缺乏衡量文化产出积极效用的方法。

Method: 通过分析现有AI评估实践的局限性，指出其对文化产出的评估片面关注危害而忽视潜在效益。借鉴人文学科视角，提出'厚重娱乐'框架，该框架超越单纯减少危害，关注娱乐在意义创造、身份形成和社会连接中的作用。

Result: 研究发现AI已在娱乐领域广泛采用（尤其是年轻人群），并将成为AI企业获取投资回报的主要商业模式，这对未来AI技术发展将产生重要影响。当前评估体系存在严重不对称：虽然AI评估从效益和危害两方面严谨衡量智能能力，但对于文化产出的评估却几乎只关注危害。

Conclusion: 虽然AI常被宣传为具有革命性生产力的潜力，但从长远来看，AI可能更多地关乎'智能'本身，正如社交媒体的核心是社会连接。需要发展'厚重娱乐'等新框架来全面评估AI生成文化内容的社会影响，而不仅仅是关注减少危害。

Abstract: Generative AI systems are predominantly designed, evaluated, and marketed as intelligent systems which will benefit society by augmenting or automating human cognitive labor, promising to increase personal, corporate, and macroeconomic productivity. But this mainstream narrative about what AI is and what it can do is in tension with another emerging use case: entertainment. We argue that the field of AI is unprepared to measure or respond to how the proliferation of entertaining AI-generated content will impact society. Emerging data suggest AI is already widely adopted for entertainment purposes -- especially by young people -- and represents a large potential source of revenue. We contend that entertainment will become a primary business model for major AI corporations seeking returns on massive infrastructure investments; this will exert a powerful influence on the technology these companies produce in the coming years. Examining current evaluation practices, we identify a critical asymmetry: while AI assessments rigorously measure both benefits and harms of intelligence, they focus almost exclusively on cultural harms. We lack frameworks for articulating how cultural outputs might be actively beneficial. Drawing on insights from the humanities, we propose "thick entertainment" as a framework for evaluating AI-generated cultural content -- one that considers entertainment's role in meaning-making, identity formation, and social connection rather than simply minimizing harm. While AI is often touted for its potential to revolutionize productivity, in the long run we may find that AI turns out to be as much about "intelligence" as social media is about social connection.

</details>


### [70] [Uncovering Political Bias in Large Language Models using Parliamentary Voting Records](https://arxiv.org/abs/2601.08785)
*Jieying Chen,Karen de Jong,Andreas Poole,Jan Burakowski,Elena Elderson Nosti,Joep Windt,Chendi Wang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As large language models (LLMs) become deeply embedded in digital platforms and decision-making systems, concerns about their political biases have grown. While substantial work has examined social biases such as gender and race, systematic studies of political bias remain limited, despite their direct societal impact. This paper introduces a general methodology for constructing political bias benchmarks by aligning model-generated voting predictions with verified parliamentary voting records. We instantiate this methodology in three national case studies: PoliBiasNL (2,701 Dutch parliamentary motions and votes from 15 political parties), PoliBiasNO (10,584 motions and votes from 9 Norwegian parties), and PoliBiasES (2,480 motions and votes from 10 Spanish parties). Across these benchmarks, we assess ideological tendencies and political entity bias in LLM behavior. As part of our evaluation framework, we also propose a method to visualize the ideology of LLMs and political parties in a shared two-dimensional CHES (Chapel Hill Expert Survey) space by linking their voting-based positions to the CHES dimensions, enabling direct and interpretable comparisons between models and real-world political actors. Our experiments reveal fine-grained ideological distinctions: state-of-the-art LLMs consistently display left-leaning or centrist tendencies, alongside clear negative biases toward right-conservative parties. These findings highlight the value of transparent, cross-national evaluation grounded in real parliamentary behavior for understanding and auditing political bias in modern LLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [71] [Hierarchical Sparse Plus Low Rank Compression of LLM](https://arxiv.org/abs/2601.07839)
*Pawan Kumar,Aditi Gupta*

Main category: cs.LG

TL;DR: 该论文提出了一种分层稀疏加低秩（HSS）压缩方案，用于减少大语言模型的内存和计算负担，其包含两个阶段：首先将最大幅度权重放入稀疏矩阵，然后对稠密残差矩阵进行递归的 HSS 低秩分解。实验表明，该方法在保持困惑度性能的同时能够显著节省内存。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型的内存和计算开销巨大，需要一种高效的压缩方案来实现模型部署和持续训练。该研究旨在提出一种结构化的压缩方法，结合稀疏和低秩分解，以获得硬件友好的压缩效果。

Method: 该方法采用两阶段压缩策略：第一阶段移除幅度最大的权重形成稀疏矩阵 S；第二阶段对稠密残差矩阵应用递归的分层稀疏可分离（HSS）低秩分解，结合递归降秩策略和反向 Cuthill-Mckee（RCM）排列，以提升压缩效率和硬件友好性，支持端到端训练。

Result: 在 LLaMA-7B 模型上，仅对自注意力投影（Q、K、V 矩阵，占全部 7B 参数的 1.6B）进行压缩，在 WikiText 基准测试中保持媲美基线的困惑度；在 30% 稀疏预算和外秩 512 的情况下，sHSS-RCM 取得了 1.64 的困惑度，优于稠密基线和传统稀疏加 SVD 变体，同时显著节省内存。

Conclusion: HSS 是一种高效的硬件友好压缩方案，通过组合稀疏和递归低秩分解，在保持模型性能的同时显著减少内存占用，适用于大语言模型的部署和继续训练任务。

Abstract: Modern large language models (LLMs) place extraordinary pressure on memory and compute budgets, making principled compression indispensable for both deployment and continued training. We present Hierarchical Sparse Plus Low-Rank (HSS) compression, a two-stage scheme that (i) removes the largest-magnitude weights into a sparse matrix S and (ii) applies a recursive Hierarchically Sparse Separable (HSS) low-rank factorisation to the dense residual matrix. A recursive rank-reducing strategy and a reverse Cuthill-Mckee (RCM) permutation are introduced to align high weights towards the diagonal with the block-diagonal hierarchy, maximising off-diagonal compressibility (because they are touched only once). HSS is hardware-friendly: its matrix-vector multiply reduces to one sparse and a sequence of thin-matrix multiplications and can be trained end-to-end with standard optimisers.
  Experiments on LLaMA-7B show that targeting only the self-attention projections (1.6 B parameters of Q, K, and V matrices out of a total 7B parameters) suffices to yield large memory savings while retaining comparable state-of-the-art perplexity scores on test samples of the WikiText dataset. For example, with a 30\% sparsity budget and an outer rank of 512, sHSS-RCM achieves a perplexity of 1.64, outperforming dense baselines and classical sparse-plus-SVD variants, while also achieving significant memory savings.

</details>


### [72] [Affect and Effect: Limitations of regularisation-based continual learning in EEG-based emotion classification](https://arxiv.org/abs/2601.07858)
*Nina Peire,Yupei Li,Björn Schuller*

Main category: cs.LG

TL;DR: 正则化持续学习方法在基于EEG的情感分类任务中泛化能力有限，主要因为其稳定性-可塑性权衡存在误配，更关注缓解灾难性遗忘而非适应新被试人员。


<details>
  <summary>Details</summary>
Motivation: 基于EEG的情感分类面临高被试人员间和内部变异性的挑战，对未见被试的泛化仍然困难。持续学习虽然能通过序列学习缓解灾难性遗忘，但现有正则化方法的适切性尚未被充分探索。本研究旨在理论分析和实证检验这类方法的局限性。

Method: 在DREAMER和SEED数据集上，通过理论分析和实验评估正则化持续学习方法（包括EWC、SI、MAS）用于EEG情感分类的表现。考察了被试增量序列设置下，参数重要性估计的可靠性，梯度干扰问题，以及模型过约束和顺序敏感性。

Result: 研究发现：（1）噪声数据和协变量偏移下参数重要性估计启发式不可靠，（2）重要参数的梯度更新与新被试的需求相冲突，（3）任务间累积的重要性值导致模型过约束，（4）性能对数据顺序敏感。前向迁移（对新被试的适应）相比顺序微调无显著改善（p >0.05）。

Conclusion: EEG信号的高变异性意味着过去被试对未来的参考价值有限。因此，正则化持续学习方法不适合用于基于EEG的情感分类中对未见被试的鲁棒泛化，需要开发更能应对数据变异的新方法。

Abstract: Generalisation to unseen subjects in EEG-based emotion classification remains a challenge due to high inter-and intra-subject variability. Continual learning (CL) poses a promising solution by learning from a sequence of tasks while mitigating catastrophic forgetting. Regularisation-based CL approaches, such as Elastic Weight Consolidation (EWC), Synaptic Intelligence (SI), and Memory Aware Synapses (MAS), are commonly used as baselines in EEG-based CL studies, yet their suitability for this problem remains underexplored. This study theoretically and empirically finds that regularisation-based CL methods show limited performance for EEG-based emotion classification on the DREAMER and SEED datasets. We identify a fundamental misalignment in the stability-plasticity trade-off, where regularisation-based methods prioritise mitigating catastrophic forgetting (backward transfer) over adapting to new subjects (forward transfer). We investigate this limitation under subject-incremental sequences and observe that: (1) the heuristics for estimating parameter importance become less reliable under noisy data and covariate shift, (2) gradients on parameters deemed important by these heuristics often interfere with gradient updates required for new subjects, moving optimisation away from the minimum, (3) importance values accumulated across tasks over-constrain the model, and (4) performance is sensitive to subject order. Forward transfer showed no statistically significant improvement over sequential fine-tuning (p > 0.05 across approaches and datasets). The high variability of EEG signals means past subjects provide limited value to future subjects. Regularisation-based continual learning approaches are therefore limited for robust generalisation to unseen subjects in EEG-based emotion classification.

</details>


### [73] [RewriteNets: End-to-End Trainable String-Rewriting for Generative Sequence Modeling](https://arxiv.org/abs/2601.07868)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Dominant sequence models like the Transformer represent structure implicitly through dense attention weights, incurring quadratic complexity. We propose RewriteNets, a novel neural architecture built on an alternative paradigm: explicit, parallel string rewriting. Each layer in a RewriteNet contains a set of learnable rules. For each position in an input sequence, the layer performs four operations: (1) fuzzy matching of rule patterns, (2) conflict resolution via a differentiable assignment operator to select non-overlapping rewrites, (3) application of the chosen rules to replace input segments with output segments of potentially different lengths, and (4) propagation of untouched tokens. While the discrete assignment of rules is non-differentiable, we employ a straight-through Gumbel-Sinkhorn estimator, enabling stable end-to-end training. We evaluate RewriteNets on algorithmic, compositional, and string manipulation tasks, comparing them against strong LSTM and Transformer baselines. Results show that RewriteNets excel at tasks requiring systematic generalization (achieving 98.7% accuracy on the SCAN benchmark's length split) and are computationally more efficient than Transformers. We also provide an analysis of learned rules and an extensive ablation study, demonstrating that this architecture presents a promising direction for sequence modeling with explicit structural inductive biases.

</details>


### [74] [Adaptive Requesting in Decentralized Edge Networks via Non-Stationary Bandits](https://arxiv.org/abs/2601.08760)
*Yi Zhuang,Kun Yang,Xingran Chen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study a decentralized collaborative requesting problem that aims to optimize the information freshness of time-sensitive clients in edge networks consisting of multiple clients, access nodes (ANs), and servers. Clients request content through ANs acting as gateways, without observing AN states or the actions of other clients. We define the reward as the age of information reduction resulting from a client's selection of an AN, and formulate the problem as a non-stationary multi-armed bandit. In this decentralized and partially observable setting, the resulting reward process is history-dependent and coupled across clients, and exhibits both abrupt and gradual changes in expected rewards, rendering classical bandit-based approaches ineffective. To address these challenges, we propose the AGING BANDIT WITH ADAPTIVE RESET algorithm, which combines adaptive windowing with periodic monitoring to track evolving reward distributions. We establish theoretical performance guarantees showing that the proposed algorithm achieves near-optimal performance, and we validate the theoretical results through simulations.

</details>


### [75] [HOSC: A Periodic Activation with Saturation Control for High-Fidelity Implicit Neural Representations](https://arxiv.org/abs/2601.07870)
*Michal Jan Wlodarczyk,Danzel Serrano,Przemyslaw Musialski*

Main category: cs.LG

TL;DR: 提出了HOSC（Hyperbolic Oscillator with Saturation Control）激活函数：HOSC(x) = tanh(βsin(ω₀x))，通过显式参数β控制Lipschitz边界（βω₀），从而调节梯度幅度，解决了传统周期激活函数的梯度不稳定和多尺度行为控制问题。


<details>
  <summary>Details</summary>
Motivation: 传统的周期激活函数（如sine）通过振荡结构在隐式神经表示（INR）中保留高频信息，但经常面临梯度不稳定和对多尺度行为控制有限的问题。

Method: 引入HOSC激活函数，该函数结合了tanh饱和控制和正弦周期载波。提供了理论分析（Lipschitz边界为βω₀），并在图像、音频、视频、NeRF和SDF等多个领域使用标准化训练协议进行了全面的实证研究。

Result: 与SIREN、FINER等方法的对比分析显示，HOSC在某些场景下提供显著优势，在另一些场景下达到竞争性性能。结果表明HOSC是INR应用中实用的周期激活函数。

Conclusion: HOSC通过显式控制Lipschitz边界，有效解决了周期激活函数的梯度不稳定问题，为INR应用提供了实用的周期激活方案，并提供了领域特定的超参数选择指导。

Abstract: Periodic activations such as sine preserve high-frequency information in implicit neural representations (INRs) through their oscillatory structure, but often suffer from gradient instability and limited control over multi-scale behavior. We introduce the Hyperbolic Oscillator with Saturation Control (HOSC) activation, $\text{HOSC}(x) = \tanh\bigl(β\sin(ω_0 x)\bigr)$, which exposes an explicit parameter $β$ that controls the Lipschitz bound of the activation by $βω_0$. This provides a direct mechanism to tune gradient magnitudes while retaining a periodic carrier. We provide a mathematical analysis and conduct a comprehensive empirical study across images, audio, video, NeRFs, and SDFs using standardized training protocols. Comparative analysis against SIREN, FINER, and related methods shows where HOSC provides substantial benefits and where it achieves competitive parity. Results establish HOSC as a practical periodic activation for INR applications, with domain-specific guidance on hyperparameter selection. For code visit the project page https://hosc-nn.github.io/ .

</details>


### [76] [Multiplicative Orthogonal Sequential Editing for Language Models](https://arxiv.org/abs/2601.07873)
*Hao-Xiang Xu,Jun-Yu Ma,Ziqi Peng,Yuhao Sun,Zhen-Hua Ling,Jia-Chen Gu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Knowledge editing aims to efficiently modify the internal knowledge of large language models (LLMs) without compromising their other capabilities. The prevailing editing paradigm, which appends an update matrix to the original parameter matrix, has been shown by some studies to damage key numerical stability indicators (such as condition number and norm), thereby reducing editing performance and general abilities, especially in sequential editing scenario. Although subsequent methods have made some improvements, they remain within the additive framework and have not fundamentally addressed this limitation. To solve this problem, we analyze it from both statistical and mathematical perspectives and conclude that multiplying the original matrix by an orthogonal matrix does not change the numerical stability of the matrix. Inspired by this, different from the previous additive editing paradigm, a multiplicative editing paradigm termed Multiplicative Orthogonal Sequential Editing (MOSE) is proposed. Specifically, we first derive the matrix update in the multiplicative form, the new knowledge is then incorporated into an orthogonal matrix, which is multiplied by the original parameter matrix. In this way, the numerical stability of the edited matrix is unchanged, thereby maintaining editing performance and general abilities. We compared MOSE with several current knowledge editing methods, systematically evaluating their impact on both editing performance and the general abilities across three different LLMs. Experimental results show that MOSE effectively limits deviations in the edited parameter matrix and maintains its numerical stability. Compared to current methods, MOSE achieves a 12.08% improvement in sequential editing performance, while retaining 95.73% of general abilities across downstream tasks. The code is available at https://github.com/famoustourist/MOSE.

</details>


### [77] [NOVAK: Unified adaptive optimizer for deep neural networks](https://arxiv.org/abs/2601.07876)
*Sergii Kavun*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This work introduces NOVAK, a modular gradient-based optimization algorithm that integrates adaptive moment estimation, rectified learning-rate scheduling, decoupled weight regularization, multiple variants of Nesterov momentum, and lookahead synchronization into a unified, performance-oriented framework. NOVAK adopts a dual-mode architecture consisting of a streamlined fast path designed for production. The optimizer employs custom CUDA kernels that deliver substantial speedups (3-5 for critical operations) while preserving numerical stability under standard stochastic-optimization assumptions. We provide fully developed mathematical formulations for rectified adaptive learning rates, a memory-efficient lookahead mechanism that reduces overhead from O(2p) to O(p + p/k), and the synergistic coupling of complementary optimization components. Theoretical analysis establishes convergence guarantees and elucidates the stability and variance-reduction properties of the method. Extensive empirical evaluation on CIFAR-10, CIFAR-100, ImageNet, and ImageNette demonstrates NOVAK superiority over 14 contemporary optimizers, including Adam, AdamW, RAdam, Lion, and Adan. Across architectures such as ResNet-50, VGG-16, and ViT, NOVAK consistently achieves state-of-the-art accuracy, and exceptional robustness, attaining very high accuracy on VGG-16/ImageNette demonstrating superior architectural robustness compared to contemporary optimizers. The results highlight that NOVAKs architectural contributions (particularly rectification, decoupled decay, and hybrid momentum) are crucial for reliable training of deep plain networks lacking skip connections, addressing a long-standing limitation of existing adaptive optimization methods.

</details>


### [78] [E^2-LLM: Bridging Neural Signals and Interpretable Affective Analysis](https://arxiv.org/abs/2601.07877)
*Fei Ma,Han Lin,Yifan Xie,Hongwei Ren,Xiaoyu Shen,Wenbo Ding,Qi Tian*

Main category: cs.LG

TL;DR: 作者提出了E^2-LLM，首个基于多模态大语言模型的EEG情感识别框架，通过多阶段训练实现可解释的情感分析。


<details>
  <summary>Details</summary>
Motivation: 现有EEG情感识别方法面临个体差异大、标注数据有限、缺乏可解释性等挑战，而MLLMs虽在情感分析方面有进展，却未针对神经信号的时空特性进行适配。

Method: E^2-LLM整合预训练EEG编码器与Qwen-based LLMs，通过可学习的投影层连接，并采用包含情感判别预训练、跨模态对齐和思维链指令调优的多阶段训练流程。

Result: 在七个情感类别的数据集上测试，E^2-LLM在情感分类方面表现优异，较大变体展示了更强的可靠性和对复杂推理场景的零样本泛化能力。

Conclusion: E^2-LLM开创了生理信号与LLM推理能力结合的新范式，表明模型扩展不仅能提高识别准确性，还能增强情感计算中的可解释性理解。

Abstract: Emotion recognition from electroencephalography (EEG) signals remains challenging due to high inter-subject variability, limited labeled data, and the lack of interpretable reasoning in existing approaches. While recent multimodal large language models (MLLMs) have advanced emotion analysis, they have not been adapted to handle the unique spatiotemporal characteristics of neural signals. We present E^2-LLM (EEG-to-Emotion Large Language Model), the first MLLM framework for interpretable emotion analysis from EEG. E^2-LLM integrates a pretrained EEG encoder with Qwen-based LLMs through learnable projection layers, employing a multi-stage training pipeline that encompasses emotion-discriminative pretraining, cross-modal alignment, and instruction tuning with chain-of-thought reasoning. We design a comprehensive evaluation protocol covering basic emotion prediction, multi-task reasoning, and zero-shot scenario understanding. Experiments on the dataset across seven emotion categories demonstrate that E^2-LLM achieves excellent performance on emotion classification, with larger variants showing enhanced reliability and superior zero-shot generalization to complex reasoning scenarios. Our work establishes a new paradigm combining physiological signals with LLM reasoning capabilities, showing that model scaling improves both recognition accuracy and interpretable emotional understanding in affective computing.

</details>


### [79] [Sliced-Wasserstein Distribution Alignment Loss Improves the Ultra-Low-Bit Quantization of Large Language Models](https://arxiv.org/abs/2601.07878)
*Deyu Cao,Yixin Yin,Samin Aref*

Main category: cs.LG

TL;DR: 使用切片Wasserstein损失函数进行分布感知的模型量化校准，在超低比特量化中通过输出分布对齐来提升性能，无需推理时额外计算开销


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署时资源利用效率低下，导致高昂的经济和环境成本，但传统量化方法在4比特以下压缩时会扭曲激活分布并降低性能

Method: 提出基于切片Wasserstein损失函数的分布感知后训练量化方法，通过随机线性投影下对齐全精度模型和量化模型的输出分布，补充标准的均方误差损失，可与任何带重训练组件的后训练量化框架结合

Result: 与现有前沿方法OmniQuant和TesseraQ结合后，在LLaMA-2-7B、OPT-6.7B和LLaMA-2-13B等模型上，准确率恢复了0.93%-20.37%，困惑度和下游任务准确性在多种超低比特设置下均得到提升

Conclusion: 分布对齐提供了一种简单有效的性能提升方式，能突破前沿量化方法的极限，为解决超低比特量化的性能退化问题提供了有前景的方案

Abstract: The benefits of most large language models come with steep and often hidden economic and environmental costs due to their resource usage inefficiency during deployment. Model quantization improves energy and memory efficiency through representing model parameters by lower-precision values. However, compression below 4-bits often distorts activation distributions and degrades performance. We address this challenge by introducing a sliced Wasserstein loss function for distribution-aware calibration in ultra-low-bit post-training quantization. The proposed loss aligns the output distributions of full-precision and quantized models under random linear projections, complementing standard mean-squared error loss without adding any computational overhead during inference. Our proposed loss function can be incorporated with any post-training quantization framework that has a retraining component. We demonstrate the performance gains of our proposed model by incorporating it with two frontier methods known as OmniQuant and TesseraQ. Compared to these two baselines, the proposed loss consistently improves both perplexity and downstream task accuracy across multiple ultra-low-bit settings. Our proposed loss function recovers 4.12-20.37% of the OmniQuant's lost accuracy on the language model LLaMA-2-7B, 0.93-7.65% on OPT-6.7B, and 2.26-6.20% on LLaMA-2-13B. TesseraQ's accuracy degradation is recovered by 3.63-7.63% in relative terms when augmented by our proposed loss function. Taken together, these results demonstrate that distributional alignment provides a simple yet effective performance boost that can push the limits of frontier quantization methods. Our method is available on GitHub to facilitate future progress in ultra-low-bit quantization.

</details>


### [80] [KVzap: Fast, Adaptive, and Faithful KV Cache Pruning](https://arxiv.org/abs/2601.07891)
*Simon Jegou,Maximilian Jeblick*

Main category: cs.LG

TL;DR: KVzap是一种快速、输入自适应的KV缓存压缩方法，能同时在预填充和解码阶段工作。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer语言模型的上下文长度增长，KV缓存已成为推理关键瓶颈。现有KV缓存剪枝方法因速度-精度权衡问题尚未被主要推理引擎采用。

Method: KVzap采用基于输入自适应的KVzip近似方法，支持预填充和解码阶段。

Result: 在Qwen3-8B、Llama-3.1-8B-Instruct和Qwen3-32B模型上，KVzap实现2-4倍KV缓存压缩，精度损失可忽略，并在KVpress基准测试中达到最佳性能。

Conclusion: KVzap是一种高效的KV缓存压缩方法，在实际推理场景中具有良好的速度-精度平衡。

Abstract: Growing context lengths in transformer-based language models have made the key-value (KV) cache a critical inference bottleneck. While many KV cache pruning methods have been proposed, they have not yet been adopted in major inference engines due to speed--accuracy trade-offs. We introduce KVzap, a fast, input-adaptive approximation of KVzip that works in both prefilling and decoding. On Qwen3-8B, Llama-3.1-8B-Instruct, and Qwen3-32B across long-context and reasoning tasks, KVzap achieves $2$--$4\times$ KV cache compression with negligible accuracy loss and achieves state-of-the-art performance on the KVpress leaderboard. Code and models are available at https://github.com/NVIDIA/kvpress.

</details>


### [81] [Sherry: Hardware-Efficient 1.25-Bit Ternary Quantization via Fine-grained Sparsification](https://arxiv.org/abs/2601.07892)
*Hong Huang,Decheng Wu,Qiangqiang Hu,Guanghua Yu,Jinhai Yang,Jianchen Zhu,Xue Liu,Dapeng Wu*

Main category: cs.LG

TL;DR: Sherry是一个硬件高效的三值量化框架，通过3:4细粒度稀疏性实现1.25位宽度的规则化数据包，解决了现有方法在2位对齐打包和1.67位不规则打包之间的权衡问题，同时在训练中引入Arenas机制解决权重陷阱问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在资源受限的边缘设备上部署面临内存和计算需求的巨大挑战。虽然三值量化通过将权重减少到{-1, 0, +1}提供了有前景的解决方案，但现有实现与商业硬件之间存在基本不匹配：要么使用2位对齐打包导致显著的比特浪费，要么使用1.67位不规则打包降低推理速度。

Method: Sherry框架采用以下创新方法：1）引入3:4细粒度稀疏性，通过将四个权重组打包到五个比特中，实现1.25比特宽度的规则化表示，恢复2的幂次对齐；2）提出Arenas机制（退火残差突触机制），解决稀疏三值训练中的权重陷阱问题，保持训练过程中的表示多样性。

Result: 在LLaMA-3.2模型上进行的五个基准测试实证评估表明，Sherry在匹配最先进三值量化性能的同时，显著减小了模型大小。具体来说，在Intel i7-14700HX CPU上，1B模型与最先进基线相比实现了零准确率损失，同时提供25%的比特节省和10%的加速。

Conclusion: Sherry通过创新的硬件高效三值量化方法和解决权重陷阱的训练机制，有效地平衡了模型压缩与硬件兼容性，为在资源受限设备上部署大型语言模型提供了实用解决方案，实现了性能、内存效率和推理速度的优化平衡。

Abstract: The deployment of Large Language Models (LLMs) on resource-constrained edge devices is increasingly hindered by prohibitive memory and computational requirements. While ternary quantization offers a compelling solution by reducing weights to {-1, 0, +1}, current implementations suffer from a fundamental misalignment with commodity hardware. Most existing methods must choose between 2-bit aligned packing, which incurs significant bit wastage, or 1.67-bit irregular packing, which degrades inference speed. To resolve this tension, we propose Sherry, a hardware-efficient ternary quantization framework. Sherry introduces a 3:4 fine-grained sparsity that achieves a regularized 1.25-bit width by packing blocks of four weights into five bits, restoring power-of-two alignment. Furthermore, we identify weight trapping issue in sparse ternary training, which leads to representational collapse. To address this, Sherry introduces Arenas, an annealing residual synapse mechanism that maintains representational diversity during training. Empirical evaluations on LLaMA-3.2 across five benchmarks demonstrate that Sherry matches state-of-the-art ternary performance while significantly reducing model size. Notably, on an Intel i7-14700HX CPU, our 1B model achieves zero accuracy loss compared to SOTA baselines while providing 25% bit savings and 10% speed up. The code is available at https://github.com/Tencent/AngelSlim .

</details>


### [82] [Revealing the Attention Floating Mechanism in Masked Diffusion Models](https://arxiv.org/abs/2601.07894)
*Xin Dai,Pengcheng Huang,Zhenghao Liu,Shuo Wang,Yukun Yan,Chaojun Xiao,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.LG

TL;DR: 该研究分析了掩码扩散模型中的注意力机制，发现了MDMs存在与自回归模型不同的注意力浮动现象，揭示了其浅层关注结构、深层聚焦内容的模式，解释了MDMs在上下文学习中性能优势的机制原理


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型与自回归模型之间仍然存在性能差距，但其内部注意力机制尚未得到充分探索。研究者希望通过深入分析MDMs的注意力行为，揭示其独特工作机制，以解释模型性能差异的内在原因

Method: 通过观察和分析MDMs在去噪过程中的注意力机制，特别关注了注意力锚点的动态变化模式。研究对比了MDMs与ARMs在注意力机制上的差异，并深入探究了不同深度的注意力层在处理信息时的分工模式

Result: 发现了注意力浮动现象：MDMs的注意力锚点在去噪步骤和层间动态变化，不像ARMs那样收敛到固定汇聚点。揭示了浅层注意力构建全局结构框架，深层注意力聚焦语义内容的分层机制。这种模式使MDMs在知识密集型任务中性能达到ARMs的两倍

Conclusion: MDMs独特的注意力模式为其强大的上下文学习能力提供了机制性解释，这种浅层结构感知、深层内容聚焦的注意力分工是其性能优势的关键所在。这一发现有助于理解扩散模型的工作机制，并为模型改进提供了理论指导

Abstract: Masked diffusion models (MDMs), which leverage bidirectional attention and a denoising process, are narrowing the performance gap with autoregressive models (ARMs). However, their internal attention mechanisms remain under-explored. This paper investigates the attention behaviors in MDMs, revealing the phenomenon of Attention Floating. Unlike ARMs, where attention converges to a fixed sink, MDMs exhibit dynamic, dispersed attention anchors that shift across denoising steps and layers. Further analysis reveals its Shallow Structure-Aware, Deep Content-Focused attention mechanism: shallow layers utilize floating tokens to build a global structural framework, while deeper layers allocate more capability toward capturing semantic content. Empirically, this distinctive attention pattern provides a mechanistic explanation for the strong in-context learning capabilities of MDMs, allowing them to double the performance compared to ARMs in knowledge-intensive tasks. All codes and datasets are available at https://github.com/NEUIR/Attention-Floating.

</details>


### [83] [Large Language Models and Algorithm Execution: Application to an Arithmetic Function](https://arxiv.org/abs/2601.07898)
*Farah Ben Slama,Frédéric Armetta*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) have recently developed new advanced functionalities. Their effectiveness relies on statistical learning and generalization capabilities. However, they face limitations in internalizing the data they process and struggle, for instance, to autonomously execute algorithms. In this paper, we investigate the possibility of extending these models' capabilities to algorithm execution through specialized supervised training focused on reasoning decomposition. We introduce a training model called LLM-DAL (Large Language Model - Decompositional Algorithmic Learning), through which we demonstrate that LLMs' ability to perform complex algorithmic inferences and generalize can be significantly improved when the training method is properly designed to guide the model in its learning process.

</details>


### [84] [Enhancing Large Language Models for Time-Series Forecasting via Vector-Injected In-Context Learning](https://arxiv.org/abs/2601.07903)
*Jianqi Zhang,Jingyao Wang,Wenwen Qiang,Fanjiang Xu,Changwen Zheng*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The World Wide Web needs reliable predictive capabilities to respond to changes in user behavior and usage patterns. Time series forecasting (TSF) is a key means to achieve this goal. In recent years, the large language models (LLMs) for TSF (LLM4TSF) have achieved good performance. However, there is a significant difference between pretraining corpora and time series data, making it hard to guarantee forecasting quality when directly applying LLMs to TSF; fine-tuning LLMs can mitigate this issue, but often incurs substantial computational overhead. Thus, LLM4TSF faces a dual challenge of prediction performance and compute overhead. To address this, we aim to explore a method for improving the forecasting performance of LLM4TSF while freezing all LLM parameters to reduce computational overhead. Inspired by in-context learning (ICL), we propose LVICL. LVICL uses our vector-injected ICL to inject example information into a frozen LLM, eliciting its in-context learning ability and thereby enhancing its performance on the example-related task (i.e., TSF). Specifically, we first use the LLM together with a learnable context vector adapter to extract a context vector from multiple examples adaptively. This vector contains compressed, example-related information. Subsequently, during the forward pass, we inject this vector into every layer of the LLM to improve forecasting performance. Compared with conventional ICL that adds examples into the prompt, our vector-injected ICL does not increase prompt length; moreover, adaptively deriving a context vector from examples suppresses components harmful to forecasting, thereby improving model performance. Extensive experiments demonstrate the effectiveness of our approach.

</details>


### [85] [Towards Specialized Generalists: A Multi-Task MoE-LoRA Framework for Domain-Specific LLM Adaptation](https://arxiv.org/abs/2601.07935)
*Yuxin Yang,Aoxiong Zeng,Xiangquan Yang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rapid evolution of Large Language Models (LLMs) has shifted focus from general-purpose capabilities to domain-specific expertise. However, adapting LLMs to specialized fields such as medicine presents two challenge: (1) the "Stability-Plasticity Dilemma", where the model must acquire complex clinical knowledge without suffering from catastrophic forgetting of general world knowledge; and (2) "Task Interference", where disparate sub-tasks, such as medical diagnosis, report summarization, and drug-drug interaction prediction, compete for limited low-rank parameter space. In this paper, we propose Med-MoE-LoRA, a novel framework that integrates Mixture-of-Experts (MoE) with Low-Rank Adaptation (LoRA) to enable efficient multi-task domain adaptation, especially for medical scenarios. Drawing inspiration from recent advances, our framework employs an asymmetric expert distribution where deeper layers are equipped with a higher density of LoRA experts to capture complex semantic abstractions. We further introduce a "Knowledge-Preservation Plugin", inspired by LoRA MoE, to isolate and protect general-purpose reasoning. By utilizing soft merging with adaptive routing and rank-wise decoupling, Med-MoE-LoRA achieves superior performance in medical benchmarks while reducing interference. Experimental results demonstrate that our approach consistently outperforms standard LoRA and conventional MoE architectures across multiple clinical NLP tasks while retaining the model's general cognitive capabilities.

</details>


### [86] [Coupled Diffusion-Encoder Models for Reconstruction of Flow Fields](https://arxiv.org/abs/2601.07946)
*AmirPouya Hemmasian,Amir Barati Farimani*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Data-driven flow-field reconstruction typically relies on autoencoder architectures that compress high-dimensional states into low-dimensional latent representations. However, classical approaches such as variational autoencoders (VAEs) often struggle to preserve the higher-order statistical structure of fluid flows when subjected to strong compression. We propose DiffCoder, a coupled framework that integrates a probabilistic diffusion model with a conventional convolutional ResNet encoder and trains both components end-to-end. The encoder compresses the flow field into a latent representation, while the diffusion model learns a generative prior over reconstructions conditioned on the compressed state. This design allows DiffCoder to recover distributional and spectral properties that are not strictly required for minimizing pointwise reconstruction loss but are critical for faithfully representing statistical properties of the flow field. We evaluate DiffCoder and VAE baselines across multiple model sizes and compression ratios on a challenging dataset of Kolmogorov flow fields. Under aggressive compression, DiffCoder significantly improves the spectral accuracy while VAEs exhibit substantial degradation. Although both methods show comparable relative L2 reconstruction error, DiffCoder better preserves the underlying distributional structure of the flow. At moderate compression levels, sufficiently large VAEs remain competitive, suggesting that diffusion-based priors provide the greatest benefit when information bottlenecks are severe. These results demonstrate that the generative decoding by diffusion offers a promising path toward compact, statistically consistent representations of complex flow fields.

</details>


### [87] [DataScribe: An AI-Native, Policy-Aligned Web Platform for Multi-Objective Materials Design and Discovery](https://arxiv.org/abs/2601.07966)
*Divyanshu Singh,Doguhan Sarıtürk,Cameron Lea,Md Shafiqul Islam,Raymundo Arroyave,Vahid Attari*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The acceleration of materials discovery requires digital platforms that go beyond data repositories to embed learning, optimization, and decision-making directly into research workflows. We introduce DataScribe, an AI-native, cloud-based materials discovery platform that unifies heterogeneous experimental and computational data through ontology-backed ingestion and machine-actionable knowledge graphs. The platform integrates FAIR-compliant metadata capture, schema and unit harmonization, uncertainty-aware surrogate modeling, and native multi-objective multi-fidelity Bayesian optimization, enabling closed-loop propose-measure-learn workflows across experimental and computational pipelines. DataScribe functions as an application-layer intelligence stack, coupling data governance, optimization, and explainability rather than treating them as downstream add-ons. We validate the platform through case studies in electrochemical materials and high-entropy alloys, demonstrating end-to-end data fusion, real-time optimization, and reproducible exploration of multi-objective trade spaces. By embedding optimization engines, machine learning, and unified access to public and private scientific data directly within the data infrastructure, and by supporting open, free use for academic and non-profit researchers, DataScribe functions as a general-purpose application-layer backbone for laboratories of any scale, including self-driving laboratories and geographically distributed materials acceleration platforms, with built-in support for performance, sustainability, and supply-chain-aware objectives.

</details>


### [88] [Beyond the Next Port: A Multi-Task Transformer for Forecasting Future Voyage Segment Durations](https://arxiv.org/abs/2601.08013)
*Nairui Liu,Fang He,Xindi Tang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate forecasts of segment-level sailing durations are fundamental to enhancing maritime schedule reliability and optimizing long-term port operations. However, conventional estimated time of arrival (ETA) models are primarily designed for the immediate next port of call and rely heavily on real-time automatic identification system (AIS) data, which is inherently unavailable for future voyage segments. To address this gap, the study reformulates future-port ETA prediction as a segment-level time-series forecasting problem. We develop a transformer-based architecture that integrates historical sailing durations, destination port congestion proxies, and static vessel descriptors. The proposed framework employs a causally masked attention mechanism to capture long-range temporal dependencies and a multi-task learning head to jointly predict segment sailing durations and port congestion states, leveraging shared latent signals to mitigate high uncertainty. Evaluation on a real-world global dataset from 2021 demonstrates the proposed model consistently outperforms a comprehensive suite of competitive baselines. The result shows a relative reduction of 4.85% in mean absolute error (MAE) and 4.95% in mean absolute percentage error (MAPE) compared with sequence baseline models. The relative reductions with gradient boosting machines are 9.39% in MAE and 52.97% in MAPE. Case studies for the major destination port further illustrate the model's superior accuracy.

</details>


### [89] [InfGraND: An Influence-Guided GNN-to-MLP Knowledge Distillation](https://arxiv.org/abs/2601.08033)
*Amir Eskandari,Aman Anand,Elyas Rashno,Farhana Zulkernine*

Main category: cs.LG

TL;DR: InfGraND框架通过基于节点结构重要性的知识蒸馏指导MLP学习，结合多跳邻居特征预处理来提升效率。


<details>
  <summary>Details</summary>
Motivation: GNN在低延迟或资源受限场景中面临效率挑战，而MLP虽然计算高效但其直接监督训练性能不足。现有知识蒸馏方法往往忽视节点在图中结构重要性的差异。

Method: 提出InfGraND框架：1）识别图中结构影响力高的节点优先进行知识蒸馏；2）通过一次性多跳邻居特征预计算在MLP输入中嵌入结构意识，避免推理时开销。

Result: 在七个同配性图基准数据集上的传导和归纳设置评估表明，InfGraND持续优于先前从GNN到MLP的知识蒸馏方法。

Conclusion: InfGraND通过图中心的结构重要性指导蒸馏，为众多延迟关键的实际应用提供了实用的高效解决方案。

Abstract: Graph Neural Networks (GNNs) are the go-to model for graph data analysis. However, GNNs rely on two key operations - aggregation and update, which can pose challenges for low-latency inference tasks or resource-constrained scenarios. Simple Multi-Layer Perceptrons (MLPs) offer a computationally efficient alternative. Yet, training an MLP in a supervised setting often leads to suboptimal performance. Knowledge Distillation (KD) from a GNN teacher to an MLP student has emerged to bridge this gap. However, most KD methods either transfer knowledge uniformly across all nodes or rely on graph-agnostic indicators such as prediction uncertainty. We argue this overlooks a more fundamental, graph-centric inquiry: "How important is a node to the structure of the graph?" We introduce a framework, InfGraND, an Influence-guided Graph KNowledge Distillation from GNN to MLP that addresses this by identifying and prioritizing structurally influential nodes to guide the distillation process, ensuring that the MLP learns from the most critical parts of the graph. Additionally, InfGraND embeds structural awareness in MLPs through one-time multi-hop neighborhood feature pre-computation, which enriches the student MLP's input and thus avoids inference-time overhead. Our rigorous evaluation in transductive and inductive settings across seven homophilic graph benchmark datasets shows InfGraND consistently outperforms prior GNN to MLP KD methods, demonstrating its practicality for numerous latency-critical applications in real-world settings.

</details>


### [90] [Riemannian Zeroth-Order Gradient Estimation with Structure-Preserving Metrics for Geodesically Incomplete Manifolds](https://arxiv.org/abs/2601.08039)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper, we study Riemannian zeroth-order optimization in settings where the underlying Riemannian metric $g$ is geodesically incomplete, and the goal is to approximate stationary points with respect to this incomplete metric. To address this challenge, we construct structure-preserving metrics that are geodesically complete while ensuring that every stationary point under the new metric remains stationary under the original one. Building on this foundation, we revisit the classical symmetric two-point zeroth-order estimator and analyze its mean-squared error from a purely intrinsic perspective, depending only on the manifold's geometry rather than any ambient embedding. Leveraging this intrinsic analysis, we establish convergence guarantees for stochastic gradient descent with this intrinsic estimator. Under additional suitable conditions, an $ε$-stationary point under the constructed metric $g'$ also corresponds to an $ε$-stationary point under the original metric $g$, thereby matching the best-known complexity in the geodesically complete setting. Empirical studies on synthetic problems confirm our theoretical findings, and experiments on a practical mesh optimization task demonstrate that our framework maintains stable convergence even in the absence of geodesic completeness.

</details>


### [91] [LUT-Compiled Kolmogorov-Arnold Networks for Lightweight DoS Detection on IoT Edge Devices](https://arxiv.org/abs/2601.08044)
*Oleksandr Kuznetsov*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Denial-of-Service (DoS) attacks pose a critical threat to Internet of Things (IoT) ecosystems, yet deploying effective intrusion detection on resource-constrained edge devices remains challenging. Kolmogorov-Arnold Networks (KANs) offer a compact alternative to Multi-Layer Perceptrons (MLPs) by placing learnable univariate spline functions on edges rather than fixed activations on nodes, achieving competitive accuracy with fewer parameters. However, runtime B-spline evaluation introduces significant computational overhead unsuitable for latency-critical IoT applications. We propose a lookup table (LUT) compilation pipeline that replaces expensive spline computations with precomputed quantized tables and linear interpolation, dramatically reducing inference latency while preserving detection quality. Our lightweight KAN model (50K parameters, 0.19~MB) achieves 99.0\% accuracy on the CICIDS2017 DoS dataset. After LUT compilation with resolution $L=8$, the model maintains 98.96\% accuracy (F1 degradation $<0.0004$) while achieving $\mathbf{68\times}$ speedup at batch size 256 and over $\mathbf{5000\times}$ speedup at batch size 1, with only $2\times$ memory overhead. We provide comprehensive evaluation across LUT resolutions, quantization schemes, and out-of-bounds policies, establishing clear Pareto frontiers for accuracy-latency-memory trade-offs. Our results demonstrate that LUT-compiled KANs enable real-time DoS detection on CPU-only IoT gateways with deterministic inference latency and minimal resource footprint.

</details>


### [92] [Q-realign: Piggybacking Realignment on Quantization for Safe and Efficient LLM Deployment](https://arxiv.org/abs/2601.08089)
*Qitao Tan,Xiaoying Song,Ningxi Cheng,Ninghao Liu,Xiaoming Zhai,Lingzi Hong,Yanzhi Wang,Zhen Xiang,Geng Yuan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Public large language models (LLMs) are typically safety-aligned during pretraining, yet task-specific fine-tuning required for deployment often erodes this alignment and introduces safety risks. Existing defenses either embed safety recovery into fine-tuning or rely on fine-tuning-derived priors for post-hoc correction, leaving safety recovery tightly coupled with training and incurring high computational overhead and a complex workflow. To address these challenges, we propose \texttt{Q-realign}, a post-hoc defense method based on post-training quantization, guided by an analysis of representational structure. By reframing quantization as a dual-objective procedure for compression and safety, \texttt{Q-realign} decouples safety alignment from fine-tuning and naturally piggybacks into modern deployment pipelines. Experiments across multiple models and datasets demonstrate that our method substantially reduces unsafe behaviors while preserving task performance, with significant reductions in memory usage and GPU hours. Notably, our approach can recover the safety alignment of a fine-tuned 7B LLM on a single RTX 4090 within 40 minutes. Overall, our work provides a practical, turnkey solution for safety-aware deployment.

</details>


### [93] [Local-Global Feature Fusion for Subject-Independent EEG Emotion Recognition](https://arxiv.org/abs/2601.08094)
*Zheng Zhou,Isabella McEvoy,Camilo E. Valderrama*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Subject-independent EEG emotion recognition is challenged by pronounced inter-subject variability and the difficulty of learning robust representations from short, noisy recordings. To address this, we propose a fusion framework that integrates (i) local, channel-wise descriptors and (ii) global, trial-level descriptors, improving cross-subject generalization on the SEED-VII dataset. Local representations are formed per channel by concatenating differential entropy with graph-theoretic features, while global representations summarize time-domain, spectral, and complexity characteristics at the trial level. These representations are fused in a dual-branch transformer with attention-based fusion and domain-adversarial regularization, with samples filtered by an intensity threshold. Experiments under a leave-one-subject-out protocol demonstrate that the proposed method consistently outperforms single-view and classical baselines, achieving approximately 40% mean accuracy in 7-class subject-independent emotion recognition. The code has been released at https://github.com/Danielz-z/LGF-EEG-Emotion.

</details>


### [94] [STO-RL: Offline RL under Sparse Rewards via LLM-Guided Subgoal Temporal Order](https://arxiv.org/abs/2601.08107)
*Chengyang Gu,Yuxin Pan,Hui Xiong,Yize Chen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Offline reinforcement learning (RL) enables policy learning from pre-collected datasets, avoiding costly and risky online interactions, but it often struggles with long-horizon tasks involving sparse rewards. Existing goal-conditioned and hierarchical offline RL methods decompose such tasks and generate intermediate rewards to mitigate limitations of traditional offline RL, but usually overlook temporal dependencies among subgoals and rely on imprecise reward shaping, leading to suboptimal policies. To address these issues, we propose STO-RL (Offline RL using LLM-Guided Subgoal Temporal Order), an offline RL framework that leverages large language models (LLMs) to generate temporally ordered subgoal sequences and corresponding state-to-subgoal-stage mappings. Using this temporal structure, STO-RL applies potential-based reward shaping to transform sparse terminal rewards into dense, temporally consistent signals, promoting subgoal progress while avoiding suboptimal solutions. The resulting augmented dataset with shaped rewards enables efficient offline training of high-performing policies. Evaluations on four discrete and continuous sparse-reward benchmarks demonstrate that STO-RL consistently outperforms state-of-the-art offline goal-conditioned and hierarchical RL baselines, achieving faster convergence, higher success rates, and shorter trajectories. Ablation studies further confirm STO-RL's robustness to imperfect or noisy LLM-generated subgoal sequences, demonstrating that LLM-guided subgoal temporal structures combined with theoretically grounded reward shaping provide a practical and scalable solution for long-horizon offline RL.

</details>


### [95] [Intra-tree Column Subsampling Hinders XGBoost Learning of Ratio-like Interactions](https://arxiv.org/abs/2601.08121)
*Mykola Pinchuk*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Many applied problems contain signal that becomes clear only after combining multiple raw measurements. Ratios and rates are common examples. In gradient boosted trees, this combination is not an explicit operation: the model must synthesize it through coordinated splits on the component features. We study whether intra-tree column subsampling in XGBoost makes that synthesis harder. We use two synthetic data generating processes with cancellation-style structure. In both, two primitive features share a strong nuisance factor, while the target depends on a smaller differential factor. A log ratio cancels the nuisance and isolates the signal. We vary colsample_bylevel and colsample_bynode over s in {0.4, 0.6, 0.8, 0.9}, emphasizing mild subsampling (s >= 0.8). A control feature set includes the engineered ratio, removing the need for synthesis. Across both processes, intra-tree column subsampling reduces test PR-AUC in the primitives-only setting. In the main process the relative decrease reaches 54 percent when both parameters are set to 0.4. The effect largely disappears when the engineered ratio is present. A path-based co-usage metric drops in the same cells where performance deteriorates. Practically, if ratio-like structure is plausible, either avoid intra-tree subsampling or include the intended ratio features.

</details>


### [96] [Reverse Flow Matching: A Unified Framework for Online Reinforcement Learning with Diffusion and Flow Policies](https://arxiv.org/abs/2601.08136)
*Zeyang Li,Sunbochen Tang,Navid Azizan*

Main category: cs.LG

TL;DR: 提出了一个统一框架“反向流匹配(RFM)”，通过将训练目标重新定义为给定噪声样本的后验均值估计问题，并结合Langevin Stein算子和控制变量，有效降低重要性采样方差，从而统一了现有的噪声期望和梯度期望方法。


<details>
  <summary>Details</summary>
Motivation: 扩散和流策略在在线强化学习中的表达能力虽强，但训练效率仍然是一个关键挑战。其难点在于缺乏来自目标分布的样本，目标分布是由Q函数定义的未归一化玻尔兹曼分布。噪声期望和梯度期望两种方法之间的关系尚不明确，需要统一的框架来提升训练效率。

Method: 采用反向推断视角，将训练目标形式化为给定中间噪声样本的后验均值估计问题。引入Langevin Stein算子构造零均值控制变量，推导出一类降低重要性采样方差的估计器。

Result: 证明现有噪声期望和梯度期望方法是该类估计器的特例。使用RFM框架训练流策略，在连续控制基准测试中相比扩散策略基线表现出更高的性能和稳定性。

Conclusion: RFM框架统一了扩散和流策略的训练方法，通过结合Q值和Q梯度信息实现最小方差估计，提升了在线强化学习中的训练效率，并将玻尔兹曼分布目标的应用从扩散策略扩展至流策略。

Abstract: Diffusion and flow policies are gaining prominence in online reinforcement learning (RL) due to their expressive power, yet training them efficiently remains a critical challenge. A fundamental difficulty in online RL is the lack of direct samples from the target distribution; instead, the target is an unnormalized Boltzmann distribution defined by the Q-function. To address this, two seemingly distinct families of methods have been proposed for diffusion policies: a noise-expectation family, which utilizes a weighted average of noise as the training target, and a gradient-expectation family, which employs a weighted average of Q-function gradients. Yet, it remains unclear how these objectives relate formally or if they can be synthesized into a more general formulation. In this paper, we propose a unified framework, reverse flow matching (RFM), which rigorously addresses the problem of training diffusion and flow models without direct target samples. By adopting a reverse inferential perspective, we formulate the training target as a posterior mean estimation problem given an intermediate noisy sample. Crucially, we introduce Langevin Stein operators to construct zero-mean control variates, deriving a general class of estimators that effectively reduce importance sampling variance. We show that existing noise-expectation and gradient-expectation methods are two specific instances within this broader class. This unified view yields two key advancements: it extends the capability of targeting Boltzmann distributions from diffusion to flow policies, and enables the principled combination of Q-value and Q-gradient information to derive an optimal, minimum-variance estimator, thereby improving training efficiency and stability. We instantiate RFM to train a flow policy in online RL, and demonstrate improved performance on continuous-control benchmarks compared to diffusion policy baselines.

</details>


### [97] [Dynamic Graph Structure Learning via Resistance Curvature Flow](https://arxiv.org/abs/2601.08149)
*Chaoqun Fei,Huanjiang Liu,Tinglve Zhou,Yangyang Li,Tianyong Hao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Geometric Representation Learning (GRL) aims to approximate the non-Euclidean topology of high-dimensional data through discrete graph structures, grounded in the manifold hypothesis. However, traditional static graph construction methods based on Euclidean distance often fail to capture the intrinsic curvature characteristics of the data manifold. Although Ollivier-Ricci Curvature Flow (OCF) has proven to be a powerful tool for dynamic topological optimization, its core reliance on Optimal Transport (Wasserstein distance) leads to prohibitive computational complexity, severely limiting its application in large-scale datasets and deep learning frameworks. To break this bottleneck, this paper proposes a novel geometric evolution framework: Resistance Curvature Flow (RCF). Leveraging the concept of effective resistance from circuit physics, RCF transforms expensive curvature optimization into efficient matrix operations. This approach achieves over 100x computational acceleration while maintaining geometric optimization capabilities comparable to OCF. We provide an in-depth exploration of the theoretical foundations and dynamical principles of RCF, elucidating how it guides the redistribution of edge weights via curvature gradients to eliminate topological noise and strengthen local cluster structures. Furthermore, we provide a mechanistic explanation of RCF's role in manifold enhancement and noise suppression, as well as its compatibility with deep learning models. We design a graph optimization algorithm, DGSL-RCF, based on this framework. Experimental results across deep metric learning, manifold learning, and graph structure learning demonstrate that DGSL-RCF significantly improves representation quality and downstream task performance.

</details>


### [98] [VBO-MI: A Fully Gradient-Based Bayesian Optimization Framework Using Variational Mutual Information Estimation](https://arxiv.org/abs/2601.08172)
*Farhad Mirkarimi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Many real-world tasks require optimizing expensive black-box functions accessible only through noisy evaluations, a setting commonly addressed with Bayesian optimization (BO). While Bayesian neural networks (BNNs) have recently emerged as scalable alternatives to Gaussian Processes (GPs), traditional BNN-BO frameworks remain burdened by expensive posterior sampling and acquisition function optimization. In this work, we propose {VBO-MI} (Variational Bayesian Optimization with Mutual Information), a fully gradient-based BO framework that leverages recent advances in variational mutual information estimation. To enable end-to-end gradient flow, we employ an actor-critic architecture consisting of an {action-net} to navigate the input space and a {variational critic} to estimate information gain. This formulation effectively eliminates the traditional inner-loop acquisition optimization bottleneck, achieving up to a {$10^2 \times$ reduction in FLOPs} compared to BNN-BO baselines. We evaluate our method on a diverse suite of benchmarks, including high-dimensional synthetic functions and complex real-world tasks such as PDE optimization, the Lunar Lander control problem, and categorical Pest Control. Our experiments demonstrate that VBO-MI consistently provides the same or superior optimization performance and computational scalability over the baselines.

</details>


### [99] [TabPFN Through The Looking Glass: An interpretability study of TabPFN and its internal representations](https://arxiv.org/abs/2601.08181)
*Aviral Gupta,Armaan Sethi,Dhruv Kumar*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Tabular foundational models are pre-trained models designed for a wide range of tabular data tasks. They have shown strong performance across domains, yet their internal representations and learned concepts remain poorly understood. This lack of interpretability makes it important to study how these models process and transform input features. In this work, we analyze the information encoded inside the model's hidden representations and examine how these representations evolve across layers. We run a set of probing experiments that test for the presence of linear regression coefficients, intermediate values from complex expressions, and the final answer in early layers. These experiments allow us to reason about the computations the model performs internally. Our results provide evidence that meaningful and structured information is stored inside the representations of tabular foundational models. We observe clear signals that correspond to both intermediate and final quantities involved in the model's prediction process. This gives insight into how the model refines its inputs and how the final output emerges. Our findings contribute to a deeper understanding of the internal mechanics of tabular foundational models. They show that these models encode concrete and interpretable information, which moves us closer to making their decision processes more transparent and trustworthy.

</details>


### [100] [One-Shot Federated Ridge Regression: Exact Recovery via Sufficient Statistic Aggregation](https://arxiv.org/abs/2601.08216)
*Zahir Alsulaimawi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Federated learning protocols require repeated synchronization between clients and a central server, with convergence rates depending on learning rates, data heterogeneity, and client sampling. This paper asks whether iterative communication is necessary for distributed linear regression. We show it is not. We formulate federated ridge regression as a distributed equilibrium problem where each client computes local sufficient statistics -- the Gram matrix and moment vector -- and transmits them once. The server reconstructs the global solution through a single matrix inversion. We prove exact recovery: under a coverage condition on client feature matrices, one-shot aggregation yields the centralized ridge solution, not an approximation. For heterogeneous distributions violating coverage, we derive non-asymptotic error bounds depending on spectral properties of the aggregated Gram matrix. Communication reduces from $\mathcal{O}(Rd)$ in iterative methods to $\mathcal{O}(d^2)$ total; for high-dimensional settings, we propose and experimentally validate random projection techniques reducing this to $\mathcal{O}(m^2)$ where $m \ll d$. We establish differential privacy guarantees where noise is injected once per client, eliminating the composition penalty that degrades privacy in multi-round protocols. We further address practical considerations including client dropout robustness, federated cross-validation for hyperparameter selection, and comparison with gradient-based alternatives. Comprehensive experiments on synthetic heterogeneous regression demonstrate that one-shot fusion matches FedAvg accuracy while requiring up to $38\times$ less communication. The framework applies to kernel methods and random feature models but not to general nonlinear architectures.

</details>


### [101] [A Preliminary Agentic Framework for Matrix Deflation](https://arxiv.org/abs/2601.08219)
*Paimon Goulart,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Can a small team of agents peel a matrix apart, one rank-1 slice at a time? We propose an agentic approach to matrix deflation in which a solver Large Language Model (LLM) generates rank-1 Singular Value Decomposition (SVD) updates and a Vision Language Model (VLM) accepts or rejects each update and decides when to stop, eliminating fixed norm thresholds. Solver stability is improved through in-context learning (ICL) and types of row/column permutations that expose visually coherent structure. We evaluate on Digits ($8{\times}8$), CIFAR-10 ($32{\times}32$ grayscale), and synthetic ($16{\times}16$) matrices with and without Gaussian noise. In the synthetic noisy case, where the true construction rank $k$ is known, numerical deflation provides the noise target and our best agentic configuration differs by only $1.75$ RMSE of the target. For Digits and CIFAR-10, targets are defined by deflating until the Frobenius norm reaches $10\%$ of the original. Across all settings, our agent achieves competitive results, suggesting that fully agentic, threshold-free deflation is a viable alternative to classical numerical algorithms.

</details>


### [102] [GADPN: Graph Adaptive Denoising and Perturbation Networks via Singular Value Decomposition](https://arxiv.org/abs/2601.08230)
*Hao Deng,Bo Liu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While Graph Neural Networks (GNNs) excel on graph-structured data, their performance is fundamentally limited by the quality of the observed graph, which often contains noise, missing links, or structural properties misaligned with GNNs' underlying assumptions. To address this, graph structure learning aims to infer a more optimal topology. Existing methods, however, often incur high computational costs due to complex generative models and iterative joint optimization, limiting their practical utility. In this paper, we propose GADPN, a simple yet effective graph structure learning framework that adaptively refines graph topology via low-rank denoising and generalized structural perturbation. Our approach makes two key contributions: (1) we introduce Bayesian optimization to adaptively determine the optimal denoising strength, tailoring the process to each graph's homophily level; and (2) we extend the structural perturbation method to arbitrary graphs via Singular Value Decomposition (SVD), overcoming its original limitation to symmetric structures. Extensive experiments on benchmark datasets demonstrate that GADPN achieves state-of-the-art performance while significantly improving efficiency. It shows particularly strong gains on challenging disassortative graphs, validating its ability to robustly learn enhanced graph structures across diverse network types.

</details>


### [103] [Incorporating Cognitive Biases into Reinforcement Learning for Financial Decision-Making](https://arxiv.org/abs/2601.08247)
*Liu He*

Main category: cs.LG

TL;DR: 该研究将认知偏差融入强化学习金融交易框架，探索其对风险调整收益的影响。


<details>
  <summary>Details</summary>
Motivation: 传统RL金融决策模型假设理性智能体，可能忽略心理因素对市场行为的影响。本研究旨在通过整合认知偏差来模拟人类交易行为。

Method: 将过度自信和损失厌恶等偏差引入奖励结构和决策过程，在模拟和真实交易环境中评估性能。

Result: 结果尚不明确或为负面，但揭示了将人类偏差融入RL的挑战。

Conclusion: 虽然直接效果有限，但为开发稳健的金融AI系统提供了宝贵经验和教训。

Abstract: Financial markets are influenced by human behavior that deviates from rationality due to cognitive biases. Traditional reinforcement learning (RL) models for financial decision-making assume rational agents, potentially overlooking the impact of psychological factors. This study integrates cognitive biases into RL frameworks for financial trading, hypothesizing that such models can exhibit human-like trading behavior and achieve better risk-adjusted returns than standard RL agents. We introduce biases, such as overconfidence and loss aversion, into reward structures and decision-making processes and evaluate their performance in simulated and real-world trading environments. Despite its inconclusive or negative results, this study provides insights into the challenges of incorporating human-like biases into RL, offering valuable lessons for developing robust financial AI systems.

</details>


### [104] [Hyperbolic Heterogeneous Graph Transformer](https://arxiv.org/abs/2601.08251)
*Jongmin Park,Seunghoon Han,Hyewon Lee,Won-Yong Shin,Sungsu Lim*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In heterogeneous graphs, we can observe complex structures such as tree-like or hierarchical structures. Recently, the hyperbolic space has been widely adopted in many studies to effectively learn these complex structures. Although these methods have demonstrated the advantages of the hyperbolic space in learning heterogeneous graphs, most existing methods still have several challenges. They rely heavily on tangent-space operations, which often lead to mapping distortions during frequent transitions. Moreover, their message-passing architectures mainly focus on local neighborhood information, making it difficult to capture global hierarchical structures and long-range dependencies between different types of nodes. To address these limitations, we propose Hyperbolic Heterogeneous Graph Transformer (HypHGT), which effectively and efficiently learns heterogeneous graph representations entirely within the hyperbolic space. Unlike previous message-passing based hyperbolic heterogeneous GNNs, HypHGT naturally captures both local and global dependencies through transformer-based architecture. Furthermore, the proposed relation-specific hyperbolic attention mechanism in HypHGT, which operates with linear time complexity, enables efficient computation while preserving the heterogeneous information across different relation types. This design allows HypHGT to effectively capture the complex structural properties and semantic information inherent in heterogeneous graphs. We conduct comprehensive experiments to evaluate the effectiveness and efficiency of HypHGT, and the results demonstrate that it consistently outperforms state-of-the-art methods in node classification task, with significantly reduced training time and memory usage.

</details>


### [105] [On Evaluation of Unsupervised Feature Selection for Pattern Classification](https://arxiv.org/abs/2601.08257)
*Gyu-Il Kim,Dae-Won Kim,Jaesung Lee*

Main category: cs.LG

TL;DR: 本文重新评估了无监督特征选择方法的评价范式，通过采用多标签分类框架发现，基于单标签准确率的评估存在不稳定性和局限性，提出了更公平可靠的多标签评价设置。


<details>
  <summary>Details</summary>
Motivation: 传统无监督特征选择方法主要在单标签数据集上评估性能，但实际应用中特征选择应独立于任何特定标签。通过从多标签数据中随机选择一个标签进行单标签评估，会导致方法优劣比较结果随所选标签而变化，这种评估方式不合理且不稳定。本文旨在建立更公平可靠的评价框架来准确评估无监督特征选择方法的真实判别能力。

Method: 本研究采用多标签分类框架来重新评估无监督特征选择方法。实验在21个多标签数据集上进行，使用多种代表性无监督特征选择方法，通过多标签分类性能（而不是单标签准确率）来比较不同方法的效果。

Result: 实验结果表明，在多标签评价设置下，不同无监督特征选择方法的性能排名与在单标签设置下报告的排名存在显著差异。这意味着在单标签数据集上表现优异的方法在多标签环境中不一定保持优势，证实了传统单标签评估范式的不稳定性。

Conclusion: 基于单标签准确率的评估不适合用于衡量无监督特征选择方法的真实判别能力。使用多标签评估设置能够提供更公平、更可靠的比较基准，有助于更准确地评估不同方法的优劣。研究者应采用多标签分类框架来构建更合理的无监督特征选择方法评价体系。

Abstract: Unsupervised feature selection aims to identify a compact subset of features that captures the intrinsic structure of data without supervised label. Most existing studies evaluate the performance of methods using the single-label dataset that can be instantiated by selecting a label from multi-label data while maintaining the original features. Because the chosen label can vary arbitrarily depending on the experimental setting, the superiority among compared methods can be changed with regard to which label happens to be selected. Thus, evaluating unsupervised feature selection methods based solely on single-label accuracy is unreasonable for assessing their true discriminative ability. This study revisits this evaluation paradigm by adopting a multi-label classification framework. Experiments on 21 multi-label datasets using several representative methods demonstrate that performance rankings differ markedly from those reported under single-label settings, suggesting the possibility of multi-label evaluation settings for fair and reliable comparison of unsupervised feature selection methods.

</details>


### [106] [A Usable GAN-Based Tool for Synthetic ECG Generation in Cardiac Amyloidosis Research](https://arxiv.org/abs/2601.08260)
*Francesco Speziale,Ugo Lomoio,Fabiola Boccuto,Pierangelo Veltri,Pietro Hiram Guzzi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cardiac amyloidosis (CA) is a rare and underdiagnosed infiltrative cardiomyopathy, and available datasets for machine-learning models are typically small, imbalanced and heterogeneous. This paper presents a Generative Adversarial Network (GAN) and a graphical command-line interface for generating realistic synthetic electrocardiogram (ECG) beats to support early diagnosis and patient stratification in CA. The tool is designed for usability, allowing clinical researchers to train class-specific generators once and then interactively produce large volumes of labelled synthetic beats that preserve the distribution of minority classes.

</details>


### [107] [Demystifying the Slash Pattern in Attention: The Role of RoPE](https://arxiv.org/abs/2601.08297)
*Yuan Cheng,Fengzhuo Zhang,Yunlong Hou,Cunxiao Du,Chao Du,Tianyu Pang,Aixin Sun,Zhuoran Yang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) often exhibit slash attention patterns, where attention scores concentrate along the $Δ$-th sub-diagonal for some offset $Δ$. These patterns play a key role in passing information across tokens. But why do they emerge? In this paper, we demystify the emergence of these Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives. First, by analyzing open-source LLMs, we find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. To explain the intrinsic emergence, we analyze the queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. Our empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by medium- and high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between medium- and high-frequency components of RoPE give rise to SDHs. Beyond empirical evidence, we theoretically show that these conditions are sufficient to ensure the emergence of SDHs by formalizing them as our modeling assumptions. Particularly, we analyze the training dynamics of a shallow Transformer equipped with RoPE under these conditions, and prove that models trained via gradient descent exhibit SDHs. The SDHs generalize to out-of-distribution prompts.

</details>


### [108] [ORBIT: On-policy Exploration-Exploitation for Controllable Multi-Budget Reasoning](https://arxiv.org/abs/2601.08310)
*Kun Liang,Clive Bai,Xin Xu,Chenming Tang,Sanwoo Lee,Weijie Liu,Saiyong Yang,Yunfang Wu*

Main category: cs.LG

TL;DR: ORBIT是一种可控的多预算推理框架，通过多阶段强化学习和策略蒸馏实现适应不同计算成本需求的推理模式切换。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型采用过长的链式思维推理会导致计算成本高且不必要，先前工作通过估计推理预算的方法在极端情况下不可靠且缺乏部署灵活性。

Method: 提出ORBIT框架：1）使用多阶段强化学习发现每个计算预算下的帕累托最优推理行为；2）通过同策略蒸馏将这些行为融合到单一统一模型中，保持模式分离性。

Result: ORBIT实现了：1）多模式可控推理行为；2）各模式下具有竞争力的推理密度；3）将前沿策略集成到单一学生模型中，同时保持清晰的模式分离和每个模式的高性能。

Conclusion: ORBIT通过可控多预算推理框架解决了现有方法在推理成本控制和部署灵活性方面的不足，实现了计算效率与推理质量的平衡。

Abstract: Recent Large Reasoning Models (LRMs) achieve strong performance by leveraging long-form Chain-of-Thought (CoT) reasoning, but uniformly applying overlong reasoning at inference time incurs substantial and often unnecessary computational cost. To address this, prior work explores various strategies to infer an appropriate reasoning budget from the input. However, such approaches are unreliable in the worst case, as estimating the minimal required reasoning effort is fundamentally difficult, and they implicitly fix the trade-off between reasoning cost and accuracy during training, limiting flexibility under varying deployment scenarios. Motivated by these limitations, we propose ORBIT, a controllable multi-budget reasoning framework with well-separated reasoning modes triggered by input. ORBIT employs multi-stage reinforcement learning to discover Pareto-optimal reasoning behaviors at each effort, followed by on-policy distillation to fuse these behaviors into a single unified model. Experiments show that ORBIT achieves (1) controllable reasoning behavior over multiple modes, (2) competitive reasoning density within each mode, and (3) integration of these frontier policies into a single unified student model while preserving clear mode separation and high per-mode performance.

</details>


### [109] [Deep Exploration of Epoch-wise Double Descent in Noisy Data: Signal Separation, Large Activation, and Benign Overfitting](https://arxiv.org/abs/2601.08316)
*Tomoki Kubo,Ryuken Uda,Yusuke Iida*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep double descent is one of the key phenomena underlying the generalization capability of deep learning models. In this study, epoch-wise double descent, which is delayed generalization following overfitting, was empirically investigated by focusing on the evolution of internal structures. Fully connected neural networks of three different sizes were trained on the CIFAR-10 dataset with 30% label noise. By decomposing the loss curves into signal contributions from clean and noisy training data, the epoch-wise evolutions of internal signals were analyzed separately. Three main findings were obtained from this analysis. First, the model achieved strong re-generalization on test data even after perfectly fitting noisy training data during the double descent phase, corresponding to a "benign overfitting" state. Second, noisy data were learned after clean data, and as learning progressed, their corresponding internal activations became increasingly separated in outer layers; this enabled the model to overfit only noisy data. Third, a single, very large activation emerged in the shallow layer across all models; this phenomenon is referred as "outliers," "massive activa-tions," and "super activations" in recent large language models and evolves with re-generalization. The magnitude of large activation correlated with input patterns but not with output patterns. These empirical findings directly link the recent key phenomena of "deep double descent," "benign overfitting," and "large activation", and support the proposal of a novel scenario for understanding deep double descent.

</details>


### [110] [Automated Machine Learning in Radiomics: A Comparative Evaluation of Performance, Efficiency and Accessibility](https://arxiv.org/abs/2601.08334)
*Jose Lozano-Montoya,Emilio Soria-Olivas,Almudena Fuster-Matanzo,Angel Alberich-Bayarri,Ana Jimenez-Pastor*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Automated machine learning (AutoML) frameworks can lower technical barriers for predictive and prognostic model development in radiomics by enabling researchers without programming expertise to build models. However, their effectiveness in addressing radiomics-specific challenges remains unclear. This study evaluates the performance, efficiency, and accessibility of general-purpose and radiomics-specific AutoML frameworks on diverse radiomics classification tasks, thereby highlighting development needs for radiomics. Ten public/private radiomics datasets with varied imaging modalities (CT/MRI), sizes, anatomies and endpoints were used. Six general-purpose and five radiomics-specific frameworks were tested with predefined parameters using standardized cross-validation. Evaluation metrics included AUC, runtime, together with qualitative aspects related to software status, accessibility, and interpretability. Simplatab, a radiomics-specific tool with a no-code interface, achieved the highest average test AUC (81.81%) with a moderate runtime (~1 hour). LightAutoML, a general-purpose framework, showed the fastest execution with competitive performance (78.74% mean AUC in six minutes). Most radiomics-specific frameworks were excluded from the performance analysis due to obsolescence, extensive programming requirements, or computational inefficiency. Conversely, general-purpose frameworks demonstrated higher accessibility and ease of implementation. Simplatab provides an effective balance of performance, efficiency, and accessibility for radiomics classification problems. However, significant gaps remain, including the lack of accessible survival analysis support and the limited integration of feature reproducibility and harmonization within current AutoML frameworks. Future research should focus on adapting AutoML solutions to better address these radiomics-specific challenges.

</details>


### [111] [Decodable but not structured: linear probing enables Underwater Acoustic Target Recognition with pretrained audio embeddings](https://arxiv.org/abs/2601.08358)
*Hilde I. Hummel,Sandjai Bhulai,Rob D. van der Mei,Burooj Ghani*

Main category: cs.LG

TL;DR: 首次对水下声学目标识别(UATR)进行迁移学习的实证比较研究，评估了多种预训练音频模型，发现嵌入空间主要受录音特定特征主导，但线性探测能有效抑制这些信息并提取船型特征，从而以低成本实现自动UATR。


<details>
  <summary>Details</summary>
Motivation: 船舶产生的人为噪声增加导致水下声污染，对海洋生态系统构成威胁，需要监测来量化其影响。被动声学监测系统产生大量数据，手动分析不切实际，而监督学习方法受限于标记数据稀缺，迁移学习为克服这一限制提供了有前景的替代方案。

Method: 采用多种来自不同音频领域的预训练音频模型（冻结权重），通过嵌入分析进行表征，使用分类、聚类和相似性评估方法。重点是使用线性探测器从嵌入中分离船型特征并抑制录音特定特征。

Result: 分析显示嵌入空间的几何结构主要受录音特定特征主导，但简单的线性探测器能有效抑制这些录音特定信息，从这些嵌入中分离出船型特征，从而以低计算成本实现有效的自动UATR。

Conclusion: 线性探测方法能够利用预训练音频模型进行有效的自动水下声学目标识别，显著减少对大量高质量标记船舶录音的需求，为UATR提供了一种实用且高效的解决方案。

Abstract: Increasing levels of anthropogenic noise from ships contribute significantly to underwater sound pollution, posing risks to marine ecosystems. This makes monitoring crucial to understand and quantify the impact of the ship radiated noise. Passive Acoustic Monitoring (PAM) systems are widely deployed for this purpose, generating years of underwater recordings across diverse soundscapes. Manual analysis of such large-scale data is impractical, motivating the need for automated approaches based on machine learning. Recent advances in automatic Underwater Acoustic Target Recognition (UATR) have largely relied on supervised learning, which is constrained by the scarcity of labeled data. Transfer Learning (TL) offers a promising alternative to mitigate this limitation. In this work, we conduct the first empirical comparative study of transfer learning for UATR, evaluating multiple pretrained audio models originating from diverse audio domains. The pretrained model weights are frozen, and the resulting embeddings are analyzed through classification, clustering, and similarity-based evaluations. The analysis shows that the geometrical structure of the embedding space is largely dominated by recording-specific characteristics. However, a simple linear probe can effectively suppress this recording-specific information and isolate ship-type features from these embeddings. As a result, linear probing enables effective automatic UATR using pretrained audio models at low computational cost, significantly reducing the need for a large amounts of high-quality labeled ship recordings.

</details>


### [112] [Controlled LLM Training on Spectral Sphere](https://arxiv.org/abs/2601.08393)
*Tian Xie,Haoming Luo,Haoyu Tang,Yiwen Hu,Jason Klein Liu,Qingnan Ren,Yang Wang,Wayne Xin Zhao,Rui Yan,Bing Su,Chong Luo,Baining Guo*

Main category: cs.LG

TL;DR: 提出SSO优化器，通过施加模块级谱约束实现完全μP对齐，在大规模预训练中优于AdamW和Muon并提升稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有优化器（如Muon）仅部分满足μP约束（控制更新但允许权重漂移），缺乏对权重和更新的严格谱约束，导致未完全实现宽度不变的激活控制。需要一种完全μP对齐的优化方法。

Method: 提出Spectral Sphere Optimizer（SSO）：推导谱球上的最陡下降方向，对每个模块的权重及其更新施加严格的谱约束，实现完全μP对齐的优化过程。在Megatron中实现高效并行算法以支持大规模训练。

Result: 在多种架构（Dense 1.7B, MoE 8B-A1B, 200层DeepNet）上预训练，SSO均优于AdamW和Muon。观察到显著稳定性优势：改进MoE路由器负载均衡、抑制异常值、严格有界的激活。

Conclusion: SSO通过完全μP对齐实现了稳定高效的大模型优化，为扩展大型模型提供了理论和实践基础。

Abstract: Scaling large models requires optimization strategies that ensure rapid convergence grounded in stability. Maximal Update Parametrization ($\boldsymbolμ$P) provides a theoretical safeguard for width-invariant $Θ(1)$ activation control, whereas emerging optimizers like Muon are only ``half-aligned'' with these constraints: they control updates but allow weights to drift. To address this limitation, we introduce the \textbf{Spectral Sphere Optimizer (SSO)}, which enforces strict module-wise spectral constraints on both weights and their updates. By deriving the steepest descent direction on the spectral sphere, SSO realizes a fully $\boldsymbolμ$P-aligned optimization process. To enable large-scale training, we implement SSO as an efficient parallel algorithm within Megatron. Through extensive pretraining on diverse architectures, including Dense 1.7B, MoE 8B-A1B, and 200-layer DeepNet models, SSO consistently outperforms AdamW and Muon. Furthermore, we observe significant practical stability benefits, including improved MoE router load balancing, suppressed outliers, and strictly bounded activations.

</details>


### [113] [Out-of-distribution generalization of deep-learning surrogates for 2D PDE-generated dynamics in the small-data regime](https://arxiv.org/abs/2601.08404)
*Binh Duong Nguyen,Stefan Sandfeld*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Partial differential equations (PDEs) are a central tool for modeling the dynamics of physical, engineering, and materials systems, but high-fidelity simulations are often computationally expensive. At the same time, many scientific applications can be viewed as the evolution of spatially distributed fields, making data-driven forecasting of such fields a core task in scientific machine learning. In this work we study autoregressive deep-learning surrogates for two-dimensional PDE dynamics on periodic domains, focusing on generalization to out-of-distribution initial conditions within a fixed PDE and parameter regime and on strict small-data settings with at most $\mathcal{O}(10^2)$ simulated trajectories per system. We introduce a multi-channel U-Net [...], evaluate it on five qualitatively different PDE families and compare it to ViT, AFNO, PDE-Transformer, and KAN-UNet under a common training setup. Across all datasets, me-UNet matches or outperforms these more complex architectures in terms of field-space error, spectral similarity, and physics-based metrics for in-distribution rollouts, while requiring substantially less training time. It also generalizes qualitatively to unseen initial conditions with as few as $\approx 20$ training simulations. A data-efficiency study and Grad-CAM analysis further suggest that, in small-data periodic 2D PDE settings, convolutional architectures with inductive biases aligned to locality and periodic boundary conditions remain strong contenders for accurate and moderately out-of-distribution-robust surrogate modeling.

</details>


### [114] [Taxon: Hierarchical Tax Code Prediction with Semantically Aligned LLM Expert Guidance](https://arxiv.org/abs/2601.08418)
*Jihang Li,Qing Liu,Zulong Chen,Jing Wang,Wei Wang,Chuanfei Xu,Zeyi Wen*

Main category: cs.LG

TL;DR: Taxon框架：基于特征门控专家混合架构和语义一致性模型的多模态层次税收编码预测系统


<details>
  <summary>Details</summary>
Motivation: 解决大规模电商平台自动化开票和合规管理中税收编码预测的关键但未充分研究的问题，避免因编码错误导致的财务不一致性和监管风险

Method: Taxon框架包含：(1)特征门控专家混合架构，自适应地在税收分类层次的不同级别间路由多模态特征；(2)基于大语言模型提炼的语义一致性模型，验证产品标题与官方税收定义的语义对齐；以及多源训练管线，结合策划的税收数据库、发票验证日志和商家注册数据

Result: 在专有TaxCode数据集和公共基准测试中实现最先进性能；完整层次路径重构显著提升结构一致性，获得最高F1分数；已在阿里巴巴税务服务系统部署，日均处理超50万次税收编码查询，峰值超500万次，准确性、可解释性和鲁棒性均有改进

Conclusion: Taxon框架在层次税收编码预测任务中展现出卓越性能，通过语义对齐和专家指导的方法有效解决了该领域的关键挑战，并在实际生产环境中证明了其实用价值

Abstract: Tax code prediction is a crucial yet underexplored task in automating invoicing and compliance management for large-scale e-commerce platforms. Each product must be accurately mapped to a node within a multi-level taxonomic hierarchy defined by national standards, where errors lead to financial inconsistencies and regulatory risks. This paper presents Taxon, a semantically aligned and expert-guided framework for hierarchical tax code prediction. Taxon integrates (i) a feature-gating mixture-of-experts architecture that adaptively routes multi-modal features across taxonomy levels, and (ii) a semantic consistency model distilled from large language models acting as domain experts to verify alignment between product titles and official tax definitions. To address noisy supervision in real business records, we design a multi-source training pipeline that combines curated tax databases, invoice validation logs, and merchant registration data to provide both structural and semantic supervision. Extensive experiments on the proprietary TaxCode dataset and public benchmarks demonstrate that Taxon achieves state-of-the-art performance, outperforming strong baselines. Further, an additional full hierarchical paths reconstruction procedure significantly improves structural consistency, yielding the highest overall F1 scores. Taxon has been deployed in production within Alibaba's tax service system, handling an average of over 500,000 tax code queries per day and reaching peak volumes above five million requests during business event with improved accuracy, interpretability, and robustness.

</details>


### [115] [Coverage Improvement and Fast Convergence of On-policy Preference Learning](https://arxiv.org/abs/2601.08421)
*Juno Kim,Jihun Yun,Jason D. Lee,Kwang-Sung Jun*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Online on-policy preference learning algorithms for language model alignment such as online direct policy optimization (DPO) can significantly outperform their offline counterparts. We provide a theoretical explanation for this phenomenon by analyzing how the sampling policy's coverage evolves throughout on-policy training. We propose and rigorously justify the \emph{coverage improvement principle}: with sufficient batch size, each update moves into a region around the target where coverage is uniformly better, making subsequent data increasingly informative and enabling rapid convergence. In the contextual bandit setting with Bradley-Terry preferences and linear softmax policy class, we show that on-policy DPO converges exponentially in the number of iterations for batch size exceeding a generalized coverage threshold. In contrast, any learner restricted to offline samples from the initial policy suffers a slower minimax rate, leading to a sharp separation in total sample complexity. Motivated by this analysis, we further propose a simple hybrid sampler based on a novel \emph{preferential} G-optimal design, which removes dependence on coverage and guarantees convergence in just two rounds. Finally, we develop principled on-policy schemes for reward distillation in the general function class setting, and show faster noiseless rates under an alternative deviation-based notion of coverage. Experimentally, we confirm that on-policy DPO and our proposed reward distillation algorithms outperform their off-policy counterparts and enjoy stable, monotonic performance gains across iterations.

</details>


### [116] [DiffMM: Efficient Method for Accurate Noisy and Sparse Trajectory Map Matching via One Step Diffusion](https://arxiv.org/abs/2601.08482)
*Chenxu Han,Sean Bin Yang,Jilin Hu*

Main category: cs.LG

TL;DR: DiffMM是一个基于编码器-扩散的稀疏轨迹地图匹配框架，通过单步扩散过程实现高效准确的地图匹配，显著提升对稀疏采样和噪声GPS轨迹的处理性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于HMM或编码器-解码器的地图匹配方法在处理噪声或稀疏采样的GPS轨迹时面临显著挑战，需要更有效的解决方案来提升交通调度和流量分析等应用的准确性。

Method: 提出了DiffMM框架：1) 路网分段感知的轨迹编码器，通过注意力机制将输入轨迹及其周围的候选路段共同嵌入到共享隐空间；2) 单步扩散方法，利用轨迹和候选路段的联合嵌入作为条件上下文，通过捷径模型实现地图匹配。

Result: 在大规模轨迹数据集上的广泛实验表明，该方法在准确性和效率方面均优于现有最先进的地图匹配方法，特别是在处理稀疏轨迹和复杂路网拓扑时表现更为突出。

Conclusion: DiffMM通过创新的编码器-扩散框架成功解决了稀疏轨迹地图匹配的挑战，为轨迹相关应用提供了更准确高效的地图匹配解决方案。

Abstract: Map matching for sparse trajectories is a fundamental problem for many trajectory-based applications, e.g., traffic scheduling and traffic flow analysis. Existing methods for map matching are generally based on Hidden Markov Model (HMM) or encoder-decoder framework. However, these methods continue to face significant challenges when handling noisy or sparsely sampled GPS trajectories. To address these limitations, we propose DiffMM, an encoder-diffusion-based map matching framework that produces effective yet efficient matching results through a one-step diffusion process. We first introduce a road segment-aware trajectory encoder that jointly embeds the input trajectory and its surrounding candidate road segments into a shared latent space through an attention mechanism. Next, we propose a one step diffusion method to realize map matching through a shortcut model by leveraging the joint embedding of the trajectory and candidate road segments as conditioning context. We conduct extensive experiments on large-scale trajectory datasets, demonstrating that our approach consistently outperforms state-of-the-art map matching methods in terms of both accuracy and efficiency, particularly for sparse trajectories and complex road network topologies.

</details>


### [117] [Temporal Fusion Nexus: A task-agnostic multi-modal embedding model for clinical narratives and irregular time series in post-kidney transplant care](https://arxiv.org/abs/2601.08503)
*Aditya Kumar,Simon Rauch,Mario Cypko,Marcel Naik,Matthieu-P Schapranow,Aadil Rashid,Fabian Halleck,Bilgin Osmanodja,Roland Roller,Lars Pape,Klemens Budde,Mario Schiffer,Oliver Amft*

Main category: cs.LG

TL;DR: TFN是一种多模态、任务无关的嵌入模型，用于整合不规则时间序列和非结构化临床叙述数据，在肾移植后护理的三种关键结局预测中表现出优异的性能。


<details>
  <summary>Details</summary>
Motivation: 临床环境中存在多种异构数据源（如不规则时间序列和丰富叙述文档），需要开发能够整合这些多模态数据的模型来提升临床预测任务的性能。

Method: 提出Temporal Fusion Nexus（TFN）模型，这是一个多模态、任务无关的嵌入框架，能够整合不规则时间序列和非结构化临床叙述数据，并在包含3382名患者的肾移植后护理回顾性队列中进行评估。

Result: TFN在肾移植后护理的三个关键结局预测中表现出色：移植物丢失（AUC 0.96 vs. 0.94）、移植物排斥（AUC 0.84 vs. 0.74）和死亡率（AUC 0.86）。与单模态基线相比有明显提升（时间序列基线AUC提升约10%，加入静态患者数据后提升约5%），整合临床文本在所有任务中都改善了性能。

Conclusion: TFN在整合异构数据源、不规则纵向数据和丰富叙述文档方面表现出强大潜力，其解缠度量和SHAP归因分析证实了嵌入空间的稳健性和可解释性，有望扩展到肾移植护理之外的其他临床任务中。

Abstract: We introduce Temporal Fusion Nexus (TFN), a multi-modal and task-agnostic embedding model to integrate irregular time series and unstructured clinical narratives. We analysed TFN in post-kidney transplant (KTx) care, with a retrospective cohort of 3382 patients, on three key outcomes: graft loss, graft rejection, and mortality. Compared to state-of-the-art model in post KTx care, TFN achieved higher performance for graft loss (AUC 0.96 vs. 0.94) and graft rejection (AUC 0.84 vs. 0.74). In mortality prediction, TFN yielded an AUC of 0.86. TFN outperformed unimodal baselines (approx 10% AUC improvement over time series only baseline, approx 5% AUC improvement over time series with static patient data). Integrating clinical text improved performance across all tasks. Disentanglement metrics confirmed robust and interpretable latent factors in the embedding space, and SHAP-based attributions confirmed alignment with clinical reasoning. TFN has potential application in clinical tasks beyond KTx, where heterogeneous data sources, irregular longitudinal data, and rich narrative documentation are available.

</details>


### [118] [Your Group-Relative Advantage Is Biased](https://arxiv.org/abs/2601.08521)
*Fengkai Yang,Zherui Chen,Xiaohan Wang,Xiaodong Lu,Jiajun Chai,Guojun Yin,Wei Lin,Shuai Ma,Fuzhen Zhuang,Deqing Wang,Yaodong Yang,Jianxin Li,Yikun Ban*

Main category: cs.LG

TL;DR: 提出了HA-DW方法解决GRPO等分组RL方法中相对优势估计的固有偏差问题


<details>
  <summary>Details</summary>
Motivation: 发现分组强化学习（如GRPO等）中的组相对优势估计器相对于真实的期望优势存在固有偏差：对困难问题会系统性地低估优势，对简单问题则会高估优势，这导致了探索与利用的不平衡。需要理解这一理论特性并找到解决方案。

Method: 提出了历史感知自适应难度加权（HA-DW）方法：这是一种自适应重加权方案，根据演化中的难度锚点和训练动态来调整优势估计。该方法旨在纠正相对优势估计的偏差。通过理论分析和在五个数学推理基准上的实验验证了该方法的有效性。

Result: 实验证明，当HA-DW集成到GRPO及其变体中时，能够一致地提升性能。这表明纠正优势估计的偏差对于实现鲁棒高效的RLVR训练至关重要。

Conclusion: 纠正分组RL方法中固有的优势估计偏差是解决问题的关键。提出的HA-DW方法通过自适应难度加权有效改善了GRPO等方法的性能，为RLVR训练的改进提供了重要方向。

Abstract: Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.
  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.

</details>


### [119] [Contrastive and Multi-Task Learning on Noisy Brain Signals with Nonlinear Dynamical Signatures](https://arxiv.org/abs/2601.08549)
*Sucheta Ghosh,Zahra Monfared,Felix Dietrich*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce a two-stage multitask learning framework for analyzing Electroencephalography (EEG) signals that integrates denoising, dynamical modeling, and representation learning. In the first stage, a denoising autoencoder is trained to suppress artifacts and stabilize temporal dynamics, providing robust signal representations. In the second stage, a multitask architecture processes these denoised signals to achieve three objectives: motor imagery classification, chaotic versus non-chaotic regime discrimination using Lyapunov exponent-based labels, and self-supervised contrastive representation learning with NT-Xent loss. A convolutional backbone combined with a Transformer encoder captures spatial-temporal structure, while the dynamical task encourages sensitivity to nonlinear brain dynamics. This staged design mitigates interference between reconstruction and discriminative goals, improves stability across datasets, and supports reproducible training by clearly separating noise reduction from higher-level feature learning. Empirical studies show that our framework not only enhances robustness and generalization but also surpasses strong baselines and recent state-of-the-art methods in EEG decoding, highlighting the effectiveness of combining denoising, dynamical features, and self-supervised learning.

</details>


### [120] [EviNAM: Intelligibility and Uncertainty via Evidential Neural Additive Models](https://arxiv.org/abs/2601.08556)
*Sören Schleibaum,Anton Frederik Thielmann,Julian Teusch,Benjamin Säfken,Jörg P. Müller*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Intelligibility and accurate uncertainty estimation are crucial for reliable decision-making. In this paper, we propose EviNAM, an extension of evidential learning that integrates the interpretability of Neural Additive Models (NAMs) with principled uncertainty estimation. Unlike standard Bayesian neural networks and previous evidential methods, EviNAM enables, in a single pass, both the estimation of the aleatoric and epistemic uncertainty as well as explicit feature contributions. Experiments on synthetic and real data demonstrate that EviNAM matches state-of-the-art predictive performance. While we focus on regression, our method extends naturally to classification and generalized additive models, offering a path toward more intelligible and trustworthy predictions.

</details>


### [121] [M$^2$FMoE: Multi-Resolution Multi-View Frequency Mixture-of-Experts for Extreme-Adaptive Time Series Forecasting](https://arxiv.org/abs/2601.08631)
*Yaohui Huang,Runmin Zou,Yun Wang,Laeeq Aslam,Ruipeng Dong*

Main category: cs.LG

TL;DR: M²FMoE模型通过在傅里叶和小波域进行多视图频率混合专家分配，并结合多分辨率自适应融合和时态门控集成，有效提升了极端事件的时间序列预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法虽能处理常规模式，但在极端事件（高方差、不规则动态、稀疏但高影响）场景下性能显著下降，构成实际应用中的主要误差来源。现有方法即使引入辅助信号仍难以捕捉极端事件的复杂时态动态。

Method: 提出M²FMoE极端自适应预测模型，包含三个模块：1）多视图频率混合专家模块：在傅里叶和小波域为不同频段分配专家，通过跨视图共享频带分割对齐频率分区，实现专家协作捕捉主导和罕见波动；2）多分辨率自适应融合模块：从粗到细层次聚合频率特征，增强对短期变化和突变的敏感性；3）时态门控集成模块：动态平衡长期趋势和短期频率感知特征，提升对常规和极端时态模式的适应性。

Result: 在具有极端模式的真实世界水文数据集上的实验表明，M²FMoE在不需极端事件标签的情况下，超越了现有最先进的基线方法。

Conclusion: M²FMoE通过多分辨率多视图频率建模学习常规和极端模式，有效解决了极端事件时间序列预测的挑战，为实际应用提供了更可靠的预测工具。

Abstract: Forecasting time series with extreme events is critical yet challenging due to their high variance, irregular dynamics, and sparse but high-impact nature. While existing methods excel in modeling dominant regular patterns, their performance degrades significantly during extreme events, constituting the primary source of forecasting errors in real-world applications. Although some approaches incorporate auxiliary signals to improve performance, they still fail to capture extreme events' complex temporal dynamics. To address these limitations, we propose M$^2$FMoE, an extreme-adaptive forecasting model that learns both regular and extreme patterns through multi-resolution and multi-view frequency modeling. It comprises three modules: (1) a multi-view frequency mixture-of-experts module assigns experts to distinct spectral bands in Fourier and Wavelet domains, with cross-view shared band splitter aligning frequency partitions and enabling inter-expert collaboration to capture both dominant and rare fluctuations; (2) a multi-resolution adaptive fusion module that hierarchically aggregates frequency features from coarse to fine resolutions, enhancing sensitivity to both short-term variations and sudden changes; (3) a temporal gating integration module that dynamically balances long-term trends and short-term frequency-aware features, improving adaptability to both regular and extreme temporal patterns. Experiments on real-world hydrological datasets with extreme patterns demonstrate that M$^2$FMoE outperforms state-of-the-art baselines without requiring extreme-event labels.

</details>


### [122] [Provably Safe Reinforcement Learning using Entropy Regularizer](https://arxiv.org/abs/2601.08646)
*Abhijit Mazumdar,Rafal Wisniewski,Manuela L. Bujorianu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We consider the problem of learning the optimal policy for Markov decision processes with safety constraints. We formulate the problem in a reach-avoid setup. Our goal is to design online reinforcement learning algorithms that ensure safety constraints with arbitrarily high probability during the learning phase. To this end, we first propose an algorithm based on the optimism in the face of uncertainty (OFU) principle. Based on the first algorithm, we propose our main algorithm, which utilizes entropy regularization. We investigate the finite-sample analysis of both algorithms and derive their regret bounds. We demonstrate that the inclusion of entropy regularization improves the regret and drastically controls the episode-to-episode variability that is inherent in OFU-based safe RL algorithms.

</details>


### [123] [TRACE: Reconstruction-Based Anomaly Detection in Ensemble and Time-Dependent Simulations](https://arxiv.org/abs/2601.08659)
*Hamid Gadirov,Martijn Westra,Steffen Frey*

Main category: cs.LG

TL;DR: 本文研究基于重构的异常检测方法，比较了2D与3D卷积自编码器在处理参数化卡门涡街模拟数据中的性能。


<details>
  <summary>Details</summary>
Motivation: 高维时间相关模拟数据中的异常检测面临复杂时空动态性的挑战，需要有效处理空间和时间维度信息的检测方法。

Method: 使用卷积自编码器作为重构模型，比较2D自编码器（处理单帧图像）和3D自编码器（处理短时序堆栈）在卡门涡街模拟集合数据上的表现。

Result: 2D模型能识别单时间步的局部空间不规则性，3D模型利用时空上下文检测异常运动模式并减少跨时间冗余检测；体积时间相关数据的重构误差受质量空间分布强烈影响，高度集中区域误差更大。

Conclusion: 研究强调了时空上下文对于动态模拟中稳健异常检测的重要性。

Abstract: Detecting anomalies in high-dimensional, time-dependent simulation data is challenging due to complex spatial and temporal dynamics. We study reconstruction-based anomaly detection for ensemble data from parameterized Kármán vortex street simulations using convolutional autoencoders. We compare a 2D autoencoder operating on individual frames with a 3D autoencoder that processes short temporal stacks. The 2D model identifies localized spatial irregularities in single time steps, while the 3D model exploits spatio-temporal context to detect anomalous motion patterns and reduces redundant detections across time. We further evaluate volumetric time-dependent data and find that reconstruction errors are strongly influenced by the spatial distribution of mass, with highly concentrated regions yielding larger errors than dispersed configurations. Our results highlight the importance of temporal context for robust anomaly detection in dynamic simulations.

</details>


### [124] [Soft Partition-based KAPI-ELM for Multi-Scale PDEs](https://arxiv.org/abs/2601.08719)
*Vikas Dwivedi,Monica Sigovan,Bruno Sixou*

Main category: cs.LG

TL;DR: 本文提出了一种软分区核自适应物理信息极端学习机（KAPI-ELM）用于求解多尺度偏微分方程，通过平滑分区长度联合控制配置中心和核宽，避免傅里叶特征和随机采样，在单次线性求解中达到或超越现有方法的精度。


<details>
  <summary>Details</summary>
Motivation: 现有物理信息机器学习方法在处理高振荡、多尺度或奇异扰动PDE时面临谱偏差、反向传播成本高、手工调整参数等问题，需要一种更高效、自适应的求解框架。

Method: 提出KAPI-ELM方法，采用确定性低维参数化，通过平滑分区长度联合优化高斯核的配置中心和宽度，实现连续粗到细分辨率；引入基于符号距离的加权机制稳定不规则几何上的最小二乘学习。

Result: 在八个基准测试（包括振荡ODE、高频泊松方程、不规则区域和刚性奇异扰动对流扩散问题）中，该方法仅需单次线性求解即可匹配或超越最先进PINN和TFC变体的精度。

Conclusion: 软分区核自适应为多尺度PDE提供了一种快速、无架构依赖的求解框架，在稳态线性PDE上验证有效，具有广阔的物理信息建模扩展潜力。

Abstract: Physics-informed machine learning holds great promise for solving differential equations, yet existing methods struggle with highly oscillatory, multiscale, or singularly perturbed PDEs due to spectral bias, costly backpropagation, and manually tuned kernel or Fourier frequencies. This work introduces a soft partition--based Kernel-Adaptive Physics-Informed Extreme Learning Machine (KAPI-ELM), a deterministic low-dimensional parameterization in which smooth partition lengths jointly control collocation centers and Gaussian kernel widths, enabling continuous coarse-to-fine resolution without Fourier features, random sampling, or hard domain interfaces. A signed-distance-based weighting further stabilizes least-squares learning on irregular geometries. Across eight benchmarks--including oscillatory ODEs, high-frequency Poisson equations, irregular-shaped domains, and stiff singularly perturbed convection-diffusion problems-the proposed method matches or exceeds the accuracy of state-of-the-art Physics-Informed Neural Network (PINN) and Theory of Functional Connections (TFC) variants while using only a single linear solve. Although demonstrated on steady linear PDEs, the results show that soft-partition kernel adaptation provides a fast, architecture-free approach for multiscale PDEs with broad potential for future physics-informed modeling. For reproducibility, the reference codes are available at https://github.com/vikas-dwivedi-2022/soft_kapi

</details>


### [125] [A Novel Approach to Explainable AI with Quantized Active Ingredients in Decision Making](https://arxiv.org/abs/2601.08733)
*A. M. A. S. D. Alagiyawanna,Asoka Karunananda,Thushari Silva,A. Mahasinghe*

Main category: cs.LG

TL;DR: 针对AI系统在高风险领域缺乏可解释性的问题，本文提出了基于量子玻尔兹曼机（QBM）与经典玻尔兹曼机（CBM）对比研究的可解释AI框架，通过在二值化降维MNIST数据集上的实验，证明量子-经典混合模型在分类准确率（83.5% vs 54%）和特征归因集中度（熵值1.27 vs 1.39）上均优于经典模型，为构建更可信、可解释的AI系统提供了新路径。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统虽然在分类任务上表现良好，但在高风险领域（如医疗健康、金融）中，其决策过程缺乏可解释性成为一个重要挑战。为了构建更值得信赖和透明的AI系统，需要开发既能保持高准确性又能提供清晰决策解释的模型。

Method: 采用二值化并经过主成分分析（PCA）降维的MNIST数据集，对比训练量子玻尔兹曼机（QBM）和经典玻尔兹曼机（CBM）。QBM采用具有强纠缠层的混合量子-经典电路，CBM作为经典基线使用对比散度算法。为评估可解释性，在QBM中使用基于梯度的显著性图，在CBM中使用SHAP进行特征归因分析。

Result: QBM在分类准确率上显著优于CBM（83.5% vs 54%）。同时，通过熵值衡量的特征归因集中度表明，QBM的特征分布更加集中（熵值1.27 vs 1.39），这意味着QBM能更清晰地识别模型预测中的“活性成分”或最重要特征。

Conclusion: 量子-经典混合模型在准确性和可解释性方面均显示出优势，为开发更可信赖、可解释的AI系统提供了有前景的方向，表明量子计算原理在增强经典机器学习模型的透明度和性能方面具有潜力。

Abstract: Artificial Intelligence (AI) systems have shown good success at classifying. However, the lack of explainability is a true and significant challenge, especially in high-stakes domains, such as health and finance, where understanding is paramount. We propose a new solution to this challenge: an explainable AI framework based on our comparative study with Quantum Boltzmann Machines (QBMs) and Classical Boltzmann Machines (CBMs). We leverage principles of quantum computing within classical machine learning to provide substantive transparency around decision-making. The design involves training both models on a binarised and dimensionally reduced MNIST dataset, where Principal Component Analysis (PCA) is applied for preprocessing. For interpretability, we employ gradient-based saliency maps in QBMs and SHAP (SHapley Additive exPlanations) in CBMs to evaluate feature attributions.QBMs deploy hybrid quantum-classical circuits with strongly entangling layers, allowing for richer latent representations, whereas CBMs serve as a classical baseline that utilises contrastive divergence. Along the way, we found that QBMs outperformed CBMs on classification accuracy (83.5% vs. 54%) and had more concentrated distributions in feature attributions as quantified by entropy (1.27 vs. 1.39). In other words, QBMs not only produced better predictive performance than CBMs, but they also provided clearer identification of "active ingredient" or the most important features behind model predictions. To conclude, our results illustrate that quantum-classical hybrid models can display improvements in both accuracy and interpretability, which leads us toward more trustworthy and explainable AI systems.

</details>


### [126] [Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs](https://arxiv.org/abs/2601.08763)
*Zhiyuan Hu,Yucheng Wang,Yufei He,Jiaying Wu,Yilun Zhao,See-Kiong Ng,Cynthia Breazeal,Anh Tuan Luu,Hae Won Park,Bryan Hooi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@$k$ across large sampling budgets and increases the area under the pass@$k$ curve (AUC@$K$) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.

</details>


### [127] [Asymptotic Universal Alignment: A New Alignment Framework via Test-Time Scaling](https://arxiv.org/abs/2601.08777)
*Yang Cai,Weiqiang Zheng*

Main category: cs.LG

TL;DR: 该论文形式化了通过测试时规模调整实现大语言模型普遍对齐的理想概念，提出(k,f(k))-鲁棒对齐和渐近全局对齐的定义。


<details>
  <summary>Details</summary>
Motivation: 对齐大型语言模型以适应具有异质性和潜在冲突偏好的用户，是个性化和可信AI的核心挑战。现有对齐方法如NLHF可能在输出多样性上不足，无法充分利用测试时规模调整的优势。

Method: 引入(k,f(k))-鲁棒对齐和渐近全局对齐；提出对称多玩家对齐博弈框架，证明(k+1)-玩家博弈的任何对称纳什均衡策略可达到最优(k,k/(k+1))-鲁棒对齐。

Result: 存在单输出策略族，其k样本乘积策略以速率f(k)=k/(k+1)实现全局对齐，且这是最优收敛率；NLHF等方法无法保证赢率超过1/2（除任意微小松弛）。

Conclusion: 论文提出的对称多玩家对齐博弈框架能保持输出多样性并达到最优测试时规模调整速率，为具有多样偏好的用户群体实现有效对齐提供了理论保证和实用方法。

Abstract: Aligning large language models (LLMs) to serve users with heterogeneous and potentially conflicting preferences is a central challenge for personalized and trustworthy AI. We formalize an ideal notion of universal alignment through test-time scaling: for each prompt, the model produces $k\ge 1$ candidate responses and a user selects their preferred one. We introduce $(k,f(k))$-robust alignment, which requires the $k$-output model to have win rate $f(k)$ against any other single-output model, and asymptotic universal alignment (U-alignment), which requires $f(k)\to 1$ as $k\to\infty$. Our main result characterizes the optimal convergence rate: there exists a family of single-output policies whose $k$-sample product policies achieve U-alignment at rate $f(k)=\frac{k}{k+1}$, and no method can achieve a faster rate in general.
  We show that popular post-training methods, including Nash learning from human feedback (NLHF), can fundamentally underutilize the benefits of test-time scaling. Even though NLHF is optimal for $k=1$, sampling from the resulting (often deterministic) policy cannot guarantee win rates above $\tfrac{1}{2}$ except for an arbitrarily small slack. This stems from a lack of output diversity: existing alignment methods can collapse to a single majority-preferred response, making additional samples redundant. In contrast, our approach preserves output diversity and achieves the optimal test-time scaling rate. In particular, we propose a family of symmetric multi-player alignment games and prove that any symmetric Nash equilibrium policy of the $(k+1)$-player alignment game achieves the optimal $(k,\frac{k}{k+1})$-robust alignment. Finally, we provide theoretical convergence guarantees for self-play learning dynamics in these games and extend the framework to opponents that also generate multiple responses.

</details>


### [128] [Fast and explainable clustering in the Manhattan and Tanimoto distance](https://arxiv.org/abs/2601.08781)
*Stefan Güttel,Kaustubh Roy*

Main category: cs.LG

TL;DR: CLASSIX算法扩展至多种距离度量（曼哈顿、Tanimoto等），通过向量范数排序和三角形不等式终止搜索实现高效聚类，在化学指纹基准测试中性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 原始CLASSIX算法仅支持欧氏距离，限制了其在其他距离度量场景的应用。本文旨在扩展CLASSIX算法框架，使其支持曼哈顿距离、Tanimoto距离等多种距离度量，提升算法通用性和在特定领域（如化学信息学）的实用性。

Method: 1. 放弃原始算法的第一主成分排序，改用数据向量的合适范数作为排序准则；2. 结合三角形不等式实现搜索终止；3. 针对Tanimoto距离，使用更严格的交集不等式进一步提升性能；4. 算法保持原有的快速解释性特点。

Result: 1. 在真实化学指纹基准测试中，CLASSIX Tanimoto版本比Taylor-Butina算法快约30倍；2. 比DBSCAN快约80倍；3. 在两种比较中都计算出了更高质量的聚类结果；4. 成功实现了对多种距离度量的扩展。

Conclusion: CLASSIX算法通过引入范数排序和三角形不等式，成功扩展至多种距离度量，显著提升了在化学信息学等领域的性能表现，为快速、可解释的聚类提供了更通用的解决方案。

Abstract: The CLASSIX algorithm is a fast and explainable approach to data clustering. In its original form, this algorithm exploits the sorting of the data points by their first principal component to truncate the search for nearby data points, with nearness being defined in terms of the Euclidean distance. Here we extend CLASSIX to other distance metrics, including the Manhattan distance and the Tanimoto distance. Instead of principal components, we use an appropriate norm of the data vectors as the sorting criterion, combined with the triangle inequality for search termination. In the case of Tanimoto distance, a provably sharper intersection inequality is used to further boost the performance of the new algorithm. On a real-world chemical fingerprint benchmark, CLASSIX Tanimoto is about 30 times faster than the Taylor--Butina algorithm, and about 80 times faster than DBSCAN, while computing higher-quality clusters in both cases.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [129] [Emergent Coordination in Multi-Agent Systems via Pressure Fields and Temporal Decay](https://arxiv.org/abs/2601.08129)
*Roland Rodriguez*

Main category: cs.MA

TL;DR: 压力场协调匹配层次控制且显著优于顺序、随机和对话式多智能体方法；研究发现，基于共享压力梯度的隐式协调可以达到与显式层次控制相当的效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类组织结构的显式协调框架存在扩展性差的问题，作者受到自然协调机制启发，提出通过共享压力梯度实现隐式协调的新范式来解决这一问题。

Method: 提出压力场协调范式——智能体基于可测量的质量信号压力梯度和防止过早收敛的时间衰减机制，局部作用于共享工件上，并将其形式化为压力景观优化问题。

Result: 在拉丁方约束满足问题上，压力场协调与层次控制表现相当（38.2% vs 38.8%），显著优于顺序、随机和对话式方法；时间衰减机制至关重要（禁用时压力增加49倍）；在简单问题上达到87%解决率；系统在2-32个智能体规模上保持稳定性能。

Conclusion: 基于共享压力梯度的隐式协调能够在实现与显式层次控制相同效果的同时，大幅超越显式对话式协调，表明约束驱动的涌现机制可以为多智能体AI提供更简单但同样有效的基础。

Abstract: Current multi-agent LLM frameworks rely on explicit orchestration patterns borrowed from human organizational structures: planners delegate to executors, managers coordinate workers, and hierarchical control flow governs agent interactions. These approaches suffer from coordination overhead that scales poorly with agent count and task complexity. We propose a fundamentally different paradigm inspired by natural coordination mechanisms: agents operate locally on a shared artifact, guided only by pressure gradients derived from measurable quality signals, with temporal decay preventing premature convergence. We formalize this as optimization over a pressure landscape and prove convergence guarantees under mild conditions.
  Empirically, on Latin Square constraint satisfaction across 1,078 trials, pressure-field coordination matches hierarchical control (38.2% vs 38.8% aggregate solve rate, p=0.94, indicating statistical equivalence). Both significantly outperform sequential (23.3%), random (11.7%), and conversation-based multi-agent dialogue (8.6%, p<0.00001). Temporal decay is essential: disabling it increases final pressure 49-fold (d=4.15). On easy problems, pressure-field achieves 87% solve rate. The approach maintains consistent performance from 2 to 32 agents. Our key finding: implicit coordination through shared pressure gradients achieves parity with explicit hierarchical control while dramatically outperforming explicit dialogue-based coordination. This suggests that constraint-driven emergence offers a simpler, equally effective foundation for multi-agent AI.

</details>


### [130] [Agent Contracts: A Formal Framework for Resource-Bounded Autonomous AI Systems](https://arxiv.org/abs/2601.08815)
*Qing Ye,Jing Tan*

Main category: cs.MA

TL;DR: 提出了Agent Contracts框架，将合同从任务分配扩展为具有资源边界约束的执行治理机制，统一规范输入输出、多维资源限制、时间边界和成功标准，并建立预算守恒定律支持分层委托，实证验证中实现90%令牌减少和525倍方差降低。


<details>
  <summary>Details</summary>
Motivation: 现有的智能体协议虽然标准化了连接性和互操作性，但缺乏正式的、基于资源治理规范的机制来限制智能体可以消耗多少资源或运行多长时间。当前多智能体系统需要可预测、可审计且资源受限的自主AI部署形式化基础。

Method: 提出Agent Contracts形式化框架，将合同从任务分配扩展到资源受限执行。该框架统一：输入/输出规范、多维资源约束、时间边界和成功标准，并建立守恒定律确保委托预算遵守父级约束，支持通过合同委托进行分层协调。

Result: 在四个实验中进行实证验证：在迭代工作流中实现90%令牌减少和525倍方差降低；在多智能体委托中实现零守恒违规；通过合同模式实现可测量的质量-资源权衡。

Conclusion: Agent Contracts为可预测、可审计和资源受限的自主AI部署提供了形式化基础，扩展了经典协议到资源治理领域，确保了层次结构中预算的守恒性和可审计性。

Abstract: The Contract Net Protocol (1980) introduced coordination through contracts in multi-agent systems. Modern agent protocols standardize connectivity and interoperability; yet, none provide formal, resource governance-normative mechanisms to bound how much agents may consume or how long they may operate. We introduce Agent Contracts, a formal framework that extends the contract metaphor from task allocation to resource-bounded execution. An Agent Contract unifies input/output specifications, multi-dimensional resource constraints, temporal boundaries, and success criteria into a coherent governance mechanism with explicit lifecycle semantics. For multi-agent coordination, we establish conservation laws ensuring delegated budgets respect parent constraints, enabling hierarchical coordination through contract delegation. Empirical validation across four experiments demonstrates 90% token reduction with 525x lower variance in iterative workflows, zero conservation violations in multi-agent delegation, and measurable quality-resource tradeoffs through contract modes. Agent Contracts provide formal foundations for predictable, auditable, and resource-bounded autonomous AI deployment.

</details>
